{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Python for Data Science <p>A comprehensive and dynamic course designed to equip you with the skills to thrive in the world of data science</p> Explore Courses About the Author"},{"location":"#courses","title":"What You Will LearnWhy Choose This Course?About the Author","text":"Python Programming <p>Start with the basics of Python, a versatile and powerful programming language. This course lays the foundation for your data science journey.</p>          Get Started         arrow_forward Python Data Analysis <p>Explore data analysis using libraries like Pandas, NumPy, and Matplotlib. Learn to transform raw data into actionable insights.</p>          Get Started         arrow_forward Machine Learning (Python) <p>Discover the principles of machine learning and gain hands-on experience in building and optimizing models.</p>          Get Started         arrow_forward code Hands-On Learning <p>Each module is designed with practical exercises and real-world projects to ensure you can apply what you've learned.</p> route Flexible Learning Path <p>Choose to follow the entire course or focus on specific modules that meet your individual learning goals.</p> psychology Expert Guidance <p>Gain insights from industry professionals who are passionate about data science and dedicated to your success.</p> trending_up Career-Ready Skills <p>By the end of this course, you'll be ready to tackle data science challenges, whether you're transitioning careers or enhancing your current role.</p> notifications Don't Miss Any Updates! <p>       To be among the first to hear about future updates of the course materials, simply enter your email below, follow us on        open_in_new Twitter,        or subscribe to our play_circle YouTube channel.     </p> <p>Hi, My name is Juma Shafara. I am a Data Scientist at Raising The Village and Instructor at DATAIDEA. I have taught hundreds of people Programming, Data Analysis, and Machine Learning.</p> <p>I enjoy developing innovative algorithms and models that can drive insights and value. I regularly share content that I find useful throughout my work/learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies, and reading mystery books.</p> language code email <p> Last Updated: November 5, 2024 | Author: Juma Shafara </p>"},{"location":"Data%20Collection%20and%20Visulization/21_weather_data/","title":"Data Cleaning","text":""},{"location":"Data%20Collection%20and%20Visulization/21_weather_data/#cleaning-the-weather-dataset","title":"Cleaning the weather dataset","text":"<p>In this notebook, we'll be using numpy and pandas, to explore some techniques we can use to manipulate and clean a dataset. We'll be using the <code>weather dataset</code> which I developed specifically for the purpose of this lesson.</p>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool whereas Numpy is the fundamental package for scientific computing with Python.</p> <p>To continue with this notebook, you must have python, pandas and numpy installed.</p> <pre><code>## Uncomment and run this cell to install pandas and numpy\n#!pip install pandas numpy\n</code></pre> <pre><code># import the libraries\nimport pandas as pd\nimport numpy as np\nfrom dataidea.datasets import loadDataset\n</code></pre> <p>Let's check the versions of python, numpy and pandas we'll be using for this notebook</p> <pre><code># checking python version\nprint('Python Version: ',)\n!python --version\n</code></pre> <pre><code>Python Version: \nPython 3.10.12\n</code></pre> <pre><code># Checking numpy and pandas versions\nprint('Pandas Version: ', pd.__version__)\nprint('Numpy Version: ', np.__version__)\n</code></pre> <pre><code>Pandas Version:  2.2.2\nNumpy Version:  1.26.4\n</code></pre> <p>Let's load the dataset. We'll be using a weather dataset that imagined for learning purposes.</p> <pre><code># load the dataset\nweather_data = loadDataset('weather')\n</code></pre> <p>We can sample out random rows from the dataset using the <code>sample()</code> method, we can use the <code>n</code> parameter to specify the number of rows to sample</p> <pre><code># sample out random values from the dataset\nweather_data.sample(n=5)\n</code></pre> day temperature windspead event 4 07/01/2017 32.0 NaN Rain 1 04/01/2017 NaN 9.0 Sunny 7 10/01/2017 34.0 8.0 Cloudy 6 09/01/2017 NaN NaN NaN 3 06/01/2017 NaN 7.0 NaN <p>From our quick our sample, we can already observe some probles with the data that will need fixing</p> <p>Display some info about the dataset eg number of entries, count of non-null values and variable datatypes using the <code>info()</code> method</p> <pre><code># get quick dataframe info\nweather_data.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9 entries, 0 to 8\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   day          9 non-null      object \n 1   temperature  5 non-null      float64\n 2   windspead    5 non-null      float64\n 3   event        7 non-null      object \ndtypes: float64(2), object(2)\nmemory usage: 416.0+ bytes\n</code></pre> <p>We can count all missing values in each column in our dataframe by using <code>dataframe.isna().sum()</code>, eg</p> <pre><code># count missing values in each column\nweather_data.isna().sum()\n</code></pre> <pre><code>day            0\ntemperature    4\nwindspead      4\nevent          2\ndtype: int64\n</code></pre> <p>We can use a boolean-indexing like technique to find all rows in a dataset with missing values in a specific column.</p> <pre><code># get rows with missing data in temperature\nweather_data[weather_data.temperature.isna()]\n</code></pre> day temperature windspead event 1 04/01/2017 NaN 9.0 Sunny 3 06/01/2017 NaN 7.0 NaN 5 08/01/2017 NaN NaN Sunny 6 09/01/2017 NaN NaN NaN <pre><code># get rows with missing data in event column\nweather_data[weather_data.event.isna()]\n</code></pre> day temperature windspead event 3 06/01/2017 NaN 7.0 NaN 6 09/01/2017 NaN NaN NaN <p>For the next part, we would like to demonstrate forward fill (<code>ffill()</code>) and  backward fill (<code>bfill</code>), we first create two copies of the dataframe to avoid modifying our original copy in memory. - <code>ffill()</code> fills the missing values with the previous valid value in the column - <code>bfill()</code> fills the missing values with the next valid value in the column</p> <p>Let's create 2 copies of our dataframe and test out each of these concepts on either of the copies</p> <p>For the first copy, let's fill NaN values in the <code>event</code> column with <code>ffill()</code></p> <pre><code># fill with the previous valid value\nweather_data['event'] = weather_data.event.ffill()\nweather_data\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 NaN 9.0 Sunny 2 05/01/2017 28.0 NaN Snow 3 06/01/2017 NaN 7.0 Snow 4 07/01/2017 32.0 NaN Rain 5 08/01/2017 NaN NaN Sunny 6 09/01/2017 NaN NaN Sunny 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <p>From the returned dataframe we can observe that the <code>NaN</code> values in <code>event</code> have been replaced with their corresponding non-null values orccurring earlier than them ie Snow at row 3 and Sunny at row 6 </p> <p>Exercise: Demonstrate how to replace missing values with the <code>bfill()</code> method</p>"},{"location":"Data%20Collection%20and%20Visulization/21_weather_data/#choosing-between-ffill-and-bfill","title":"Choosing Between <code>ffill</code> and <code>bfill</code>","text":"<ul> <li>Context: Choose based on the context and the logical assumption that fits the nature of your data. If the past influences the present, use <code>ffill</code>. If the future influences the present, use <code>bfill</code>.</li> <li>Data Patterns: Consider the patterns in your data and what makes sense for your specific analysis or model. Ensure that the method you choose maintains the integrity and meaning of your data. </li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/21_weather_data/#fill-with-a-specific-value","title":"Fill with a specific value","text":"<p>We can modify (or fill) a specific value in the dataframe by using the <code>loc[]</code> method. This picks the value by its row (index) and column names. Assigning it a new value modifies it in the dataframe as illustrated below</p> <pre><code># modify a specific value in the dataframe\nweather_data.loc[1, 'temperature'] = 29\nweather_data\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 29.0 9.0 Sunny 2 05/01/2017 28.0 NaN Snow 3 06/01/2017 NaN 7.0 Snow 4 07/01/2017 32.0 NaN Rain 5 08/01/2017 NaN NaN Sunny 6 09/01/2017 NaN NaN Sunny 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <p>Observe that the missing value in row 1 and <code>temperature</code> has been replaced with 29</p> <p>We can use the <code>fillna()</code> method to replace all missing values in a column with a specific value as demostrated value</p> <pre><code># replace missing values in temperature column with mean\nweather_data['temperature'] = weather_data.temperature.fillna(\n    value=weather_data.temperature.mean()\n)\nweather_data\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 29.0 9.0 Sunny 2 05/01/2017 28.0 NaN Snow 3 06/01/2017 32.5 7.0 Snow 4 07/01/2017 32.0 NaN Rain 5 08/01/2017 32.5 NaN Sunny 6 09/01/2017 32.5 NaN Sunny 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <p>Exercise: Demonstrate some technniques to replace missing data for numeric, and categorical data using the <code>fillna()</code></p> <p>We can also use the <code>fillna()</code> method to fill missing values in multiple columns by passing in the dictionary of key/value pairs of column-name and value to replace. To demonstrate this, let's first reload a fresh dataframe with missing data</p> <pre><code>weather_data = loadDataset('weather')\nweather_data\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 NaN 9.0 Sunny 2 05/01/2017 28.0 NaN Snow 3 06/01/2017 NaN 7.0 NaN 4 07/01/2017 32.0 NaN Rain 5 08/01/2017 NaN NaN Sunny 6 09/01/2017 NaN NaN NaN 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <pre><code># Replace missing values in temperature, column and event\nweather_data.fillna(value={\n    'temperature': weather_data.temperature.mean(), \n    'windspead': weather_data.windspead.max(), \n    'event': weather_data.event.bfill()\n    }, inplace=True)\n\n# Now let's look at our data\nweather_data\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 29.0 9.0 Sunny 2 05/01/2017 28.0 12.0 Snow 3 06/01/2017 32.5 7.0 Snow 4 07/01/2017 32.0 12.0 Rain 5 08/01/2017 32.5 12.0 Sunny 6 09/01/2017 32.5 12.0 Sunny 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <p>We can optionally drop all rows with missing values using the <code>dropna()</code> method. To demonstrate this, let's first reload a fresh dataframe with missing data</p> <pre><code>weather_data = loadDataset('weather')\nweather_data\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 NaN 9.0 Sunny 2 05/01/2017 28.0 NaN Snow 3 06/01/2017 NaN 7.0 NaN 4 07/01/2017 32.0 NaN Rain 5 08/01/2017 NaN NaN Sunny 6 09/01/2017 NaN NaN NaN 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <pre><code># Drop all rows with missing values\nweather_data.dropna()\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <p>In the next chapter we'll look at some data visualization tools in Python</p>"},{"location":"Data%20Collection%20and%20Visulization/21_weather_data/#congratulations","title":"Congratulations!What's on your mind? Put it in the comments!","text":"<p>Congratulations on finishing this lesson. In this lesson, you have learned various methods of handling missing data including:</p> <ul> <li> Finding missing values</li> <li> bfill and ffill</li> <li> filling with a specific value</li> <li> min, max and mean values</li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/31_matplotlib_refined/","title":"Matplotlib Crash Course","text":""},{"location":"Data%20Collection%20and%20Visulization/31_matplotlib_refined/#what-is-matploblib","title":"What is Matploblib","text":"<p>Matplotlib is a powerful plotting library in Python commonly used for data visualization. </p> <p>When working with datasets, you can use Matplotlib to create various plots to explore and visualize the data. </p> <p>Here are some major plots you can create using Matplotlib with the Titanic dataset:</p>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code># # Uncomment and run this cell to install the libraries\n# !pip install pandas matplotlib dataidea\n</code></pre> <pre><code># import the libraries, packages and modules\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom dataidea.datasets import loadDataset\n</code></pre> <p>Let's demonstrate each of the plots using the Titanic dataset.  We'll first load the dataset and then create each plot using Matplotlib.</p> <pre><code># Load the Titanic dataset\ntitanic_df = loadDataset('titanic')\n</code></pre> <p>We can load this dataset like this because it is inbuilt in the dataidea package</p> <pre><code>titanic_df.head(n=5)\n</code></pre> pclass survived name sex age sibsp parch ticket fare cabin embarked boat body home.dest 0 1.0 1.0 Allen, Miss. Elisabeth Walton female 29.0000 0.0 0.0 24160 211.3375 B5 S 2 NaN St Louis, MO 1 1.0 1.0 Allison, Master. Hudson Trevor male 0.9167 1.0 2.0 113781 151.5500 C22 C26 S 11 NaN Montreal, PQ / Chesterville, ON 2 1.0 0.0 Allison, Miss. Helen Loraine female 2.0000 1.0 2.0 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON 3 1.0 0.0 Allison, Mr. Hudson Joshua Creighton male 30.0000 1.0 2.0 113781 151.5500 C22 C26 S NaN 135.0 Montreal, PQ / Chesterville, ON 4 1.0 0.0 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) female 25.0000 1.0 2.0 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON <ol> <li>Bar Plot: You can create a bar plot to visualize categorical data such as the number of passengers in each class (first class, second class, third class), the number of survivors vs. non-survivors, or the number of passengers embarked from each port (Cherbourg, Queenstown, Southampton).</li> </ol> <pre><code># 1. Bar Plot - Number of passengers in each class\nclass_counts = titanic_df.pclass.value_counts()\nclasses = class_counts.index\ncounts = class_counts.values\n\nplt.bar(x=classes, height=counts, color='#dd8604')\nplt.title('Number of Passengers Per Passenger Class')\nplt.xlabel('Passenger Class')\nplt.ylabel('Number of Passengers')\n\nplt.show()\n</code></pre> <p></p> <p>It's easy to see from the graph that the 3rd class had the largest number of passengers, followed by the 1st class and 2nd class comes last</p> <ol> <li>Histogram: Histograms are useful for visualizing the distribution of continuous variables such as age or fare. You can create histograms to see the age distribution of passengers or the fare distribution.</li> </ol> <pre><code># 2. Histogram - Age distribution of passengers\nages = titanic_df.age\nplt.hist(x=ages, bins=20, color='#dd8604', \n         edgecolor='#66FDEE')\nplt.title('Age Distribution of Passengers')\nplt.ylabel('Frequency')\nplt.xlabel('Age')\nplt.show()\n</code></pre> <p></p> <p>From the histogram we can observe that:</p> <ul> <li> The majority of the people we of ages between 15 and 35</li> <li> Fewer older people(above 60 years) boarded the titanic (below 20)t</li> </ul> <ol> <li>Box Plot: A box plot can be used to show the distribution of a continuous variable across different categories. For example, you can create a box plot to visualize the distribution of age or fare across different passenger classes.</li> </ol> <p>3.1. Age distribution boxplot</p> <pre><code># 3.1 Age distribution boxplot\nages = titanic_df.age.dropna()\nplt.boxplot(x=ages, vert=False,)\nplt.title('Age Distribution of Passengers')\nplt.xlabel('Age')\nplt.show()\n</code></pre> <p></p> <p>Features of a box plot:</p> <p> Box: The box in a boxplot represents the interquartile range (IQR), which contains the middle 50% of the data. The top and bottom edges of the box are the third quartile (Q3) and the first quartile (Q1), respectively.</p> <p> Median Line: A line inside the box indicates the median (Q2) of the data, which is the middle value of the dataset.</p> <p> Whiskers: The whiskers extend from the edges of the box to the smallest and largest values within 1.5 times the IQR from Q1 and Q3. They represent the range of the bulk of the data.</p> <p> Outliers: Data points that fall outside the whiskers are considered outliers. They are typically plotted as individual points. Outliers can be indicative of variability or errors in the data.</p> <p> Minimum and Maximum: The ends of the whiskers show the minimum and maximum values within the range of 1.5 times the IQR from the first and third quartiles.</p> <p>Meaning: A boxplot provides a visual summary of several important aspects of a dataset:</p> <ul> <li>Central Tendency: The median line shows the central point of the data.</li> <li>Spread: The IQR (the length of the box) shows the spread of the middle 50% of the data.</li> <li>Symmetry and Skewness: The relative position of the median within the box and the length of the whiskers can indicate whether the data is symmetric or skewed.</li> <li>Outliers: Individual points outside the whiskers highlight potential outliers.</li> </ul> <p>Boxplots are particularly useful for comparing distributions between several groups or datasets and identifying outliers and potential anomalies.</p> <p>3.2 Age Distribution Across Passenger Classes</p> <pre><code># 3. Box Plot - Distribution of age across passenger classes\nplt.boxplot([titanic_df[titanic_df['pclass'] == 1]['age'].dropna(),\n             titanic_df[titanic_df['pclass'] == 2]['age'].dropna(),\n             titanic_df[titanic_df['pclass'] == 3]['age'].dropna()],\n            labels=['1st Class', '2nd Class', '3rd Class'])\nplt.xlabel('Passenger Class')\nplt.ylabel('Age')\nplt.title('Distribution of Age Across Passenger Classes')\nplt.show()\n</code></pre> <pre><code>/tmp/ipykernel_16695/4289029800.py:2: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n  plt.boxplot([titanic_df[titanic_df['pclass'] == 1]['age'].dropna(),\n</code></pre> <p></p> <ol> <li>Scatter Plot: Scatter plots are helpful for visualizing the relationship between two continuous variables. You can create scatter plots to explore relationships such as age vs. fare. Read more about the scatter plot from the Matplotlib documentation</li> </ol> <pre><code># 4. Scatter Plot - Age vs. Fare\nplt.scatter(\n    x=titanic_df['age'], \n    y=titanic_df['fare'], \n    alpha=.5, \n    c=titanic_df['survived'], \n    cmap=ListedColormap(['#dd8604', '#000000'])\n)\nplt.xlabel('Age')\nplt.ylabel('Fare')\nplt.title('Age vs. Fare')\nplt.colorbar(label='Survived')  \nplt.show()\n</code></pre> <p></p> <p>I don't about you but for me I don't see a linear relationship between the age and fare of the titanic passengers</p> <ol> <li>Pie Chart: Pie charts can be used to visualize the proportion of different categories within a dataset. For example, you can create a pie chart to show the proportion of male vs. female passengers or the proportion of survivors vs. non-survivors.</li> </ol> <pre><code># 5. Pie Chart - Proportion of male vs. female passengers\ngender_counts = titanic_df['sex'].value_counts()\nplt.pie(x=gender_counts, labels=gender_counts.index, \n        autopct='%1.1f%%', startangle=90, \n        colors=['#dd8604', '#66FDEE'])\nplt.title('Proportion of Male vs. Female Passengers')\nplt.legend(loc='lower right')\nplt.show()\n</code></pre> <p></p> <ol> <li>Stacked Bar Plot: Stacked bar plots can be used to compare the composition of different categories across groups. For example, you can create a stacked bar plot to compare the proportion of survivors and non-survivors within each passenger class.</li> </ol> <pre><code># 6. Stacked Bar Plot - Survival status within each passenger class\nsurvival_counts = titanic_df.groupby(['pclass', 'survived']).size().unstack()\nsurvival_counts.plot(kind='bar', stacked=True,  \n                     color=['#dd8604', '#66FDEE'])\nplt.xlabel('Passenger Class')\nplt.ylabel('Number of Passengers')\nplt.title('Survival Status Within Each Passenger Class')\nplt.legend(['Did not survive', 'Survived'])\nplt.show()\n</code></pre> <p></p> <pre><code>titanic_df.groupby(['pclass', 'survived']).size().unstack()\n</code></pre> survived 0.0 1.0 pclass 1.0 123 200 2.0 158 119 3.0 528 181 <p>We observe that:</p> <ul> <li> More passengers in class 1 survived than those that did not survive (200 vs 123)</li> <li> Most of the passengers in class 3 did not survive (528 vs 181)</li> <li> Slightly more passengers did not survive as compared to those that survived in class 2 (152 vs 119)</li> </ul> <ol> <li>Line Plot: Line plots can be useful for visualizing trends over time or continuous variables. While the Titanic dataset may not have explicit time data, you can still use line plots to visualize trends such as the change in survival rate with increasing age or fare.</li> </ol> <pre><code># 7. Line Plot - Mean age of passengers by passenger class\nmean_age_by_class = titanic_df.groupby('pclass')['age'].mean()\nplt.plot(mean_age_by_class.index, mean_age_by_class.values, \n         marker='*', color='#dd8604')\nplt.xlabel('Passenger Class')\nplt.ylabel('Mean Age')\nplt.title('Mean Age of Passengers by Passenger Class')\nplt.show()\n</code></pre> <p></p> <p>We can quickly see the average ages for each passenger class, ie:</p> <ul> <li> Around 39 for first class</li> <li> Around 30 for second class</li> <li> Around 25 for third class</li> </ul> <p>These are some of the major plots you can create using Matplotlib. Each plot serves a different purpose and can help you gain insights into the data and explore relationships between variables.</p> <pre><code>air_passengers_data = loadDataset('air_passengers')\nair_passengers_data.head()\n</code></pre> Month Passengers 0 1949-01 112 1 1949-02 118 2 1949-03 132 3 1949-04 129 4 1949-05 121 <pre><code>air_passengers_data['Month'] = pd.to_datetime(air_passengers_data.Month)\nplt.plot('Month', 'Passengers', data=air_passengers_data, color='#dd8604')\nplt.xlabel('Years')\nplt.ylabel('Number of Passengers')\nplt.show()\n</code></pre> <p></p> <p>We can observe that the number of passengers seems to increase with time</p>"},{"location":"Data%20Collection%20and%20Visulization/31_matplotlib_refined/#review","title":"ReviewWhat's on your mind? Put it in the comments!","text":"<p>Congratulations on reaching the end of this tutorial. In this tutorial, we have learned the basic graphs and how to interprete them. ie</p> <ul> <li> Bar chart</li> <li> Histogram</li> <li> Scatter plot</li> <li> Line plot</li> <li> Box plot</li> <li> Pie chart</li> <li> Stacked bar chart</li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/32_data_exploration_and_cleaning_exercise/","title":"Data Exploration and Cleaning Exercise","text":"Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <ol> <li>Load demo.xlsx dataset</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Rename the columns as suggested below</li> </ol> Old name New name Age age Gender gender Marital Status marital_status Address address Income income Income Category income_category Job Category job_category <pre><code># your solution\n</code></pre> <ol> <li>Display all the columns in the dataset</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Display some basic statistics about the numeric variables in the dataset</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Display some basic statistics about the categorical variables in the dataset</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>What are the unique observations under gender?</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Can you fix any problems observed under the gender, give brief explanations why and how</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>How many observations have 'no answer' for marital status?</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write some piece of code to return only numeric variables from the dataset</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Are there any missing values in the dataset?</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Are there any outliers in the income variable?</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Investigate the relationship between age and income</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>How many people earn more than 300 units?</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>What data type is the marital status?</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Create dummy variables for gender</li> </ol> <pre><code># your solution\n</code></pre> <p>END</p> What's on your mind? Put it in the comments!"},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/","title":"Handling Missing Data","text":""},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/#introduction","title":"Introduction:","text":"<p>Missing data is a common hurdle in data analysis, impacting the reliability of insights drawn from datasets. Python offers a range of solutions to address this issue, some of which we discussed in the earlier weeks. In this notebook, we look into the top four missing data imputation methods:</p> <ul> <li>SimpleImputer</li> <li>KNNImputer</li> <li>IterativeImputer </li> <li>Datawig</li> </ul> <p>We'll explore these essential techniques, using sklearn and the weather dataset.</p>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code># install the libraries for this demonstration\n# ! pip install -U dataidea\n</code></pre> <pre><code>import pandas as pd\nimport dataidea as di\n</code></pre> <p><code>loadDataset</code> allows us to load datasets inbuilt in the dataidea library</p> <pre><code>weather = di.loadDataset('weather') \nweather\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 NaN 9.0 Sunny 2 05/01/2017 28.0 NaN Snow 3 06/01/2017 NaN 7.0 NaN 4 07/01/2017 32.0 NaN Rain 5 08/01/2017 NaN NaN Sunny 6 09/01/2017 NaN NaN NaN 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <pre><code>weather.isna().sum()\n</code></pre> <pre><code>day            0\ntemperature    4\nwindspead      4\nevent          2\ndtype: int64\n</code></pre> <p>Let's demonstrate how to use the top three missing data imputation methods\u2014SimpleImputer, KNNImputer, and IterativeImputer\u2014using the simple weather dataset.</p> <pre><code># select age from the data\ntemp_wind = weather[['temperature', 'windspead']].copy()\n</code></pre> <pre><code>temp_wind_imputed = temp_wind.copy()\n</code></pre> <p></p>"},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/#simpleimputer-from-scikit-learn","title":"SimpleImputer from scikit-learn:","text":"<ul> <li>Usage: SimpleImputer is a straightforward method for imputing missing values by replacing them with a constant, mean, median, or most frequent value along each column.</li> <li>Pros:<ul> <li>Easy to use and understand.</li> <li>Can handle both numerical and categorical data.</li> <li>Offers flexibility with different imputation strategies.</li> </ul> </li> <li>Cons:<ul> <li>It doesn't consider relationships between features.</li> <li>May not be the best choice for datasets with complex patterns of missingness.</li> </ul> </li> <li>Example:</li> </ul> <pre><code>from sklearn.impute import SimpleImputer\n\nsimple_imputer = SimpleImputer(strategy='mean')\ntemp_wind_simple_imputed = simple_imputer.fit_transform(temp_wind)\n\ntemp_wind_simple_imputed_df = pd.DataFrame(temp_wind_simple_imputed, columns=temp_wind.columns)\n</code></pre> <p>Let's have a look at the outcome</p> <pre><code>temp_wind_simple_imputed_df\n</code></pre> temperature windspead 0 32.0 6.0 1 33.2 9.0 2 28.0 8.4 3 33.2 7.0 4 32.0 8.4 5 33.2 8.4 6 33.2 8.4 7 34.0 8.0 8 40.0 12.0"},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/#exercise","title":"Exercise:","text":"<ol> <li>Try out the SimpleImputer with different imputation strategies like mode, constant </li> <li>Choose and try some imputation techniques on categorical data</li> </ol>"},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/#knnimputer-from-scikit-learn","title":"KNNImputer from scikit-learn:","text":"<ul> <li>Usage: <ul> <li>KNNImputer imputes missing values using k-nearest neighbors, replacing them with the mean value of the nearest neighbors.</li> <li>You can read more about the KNNImputer from the sklearn official docs site</li> </ul> </li> <li>Pros:<ul> <li>Considers relationships between features, making it suitable for datasets with complex patterns of missingness.</li> <li>Can handle both numerical and categorical data.</li> </ul> </li> <li>Cons:<ul> <li>Computationally expensive for large datasets.</li> <li>Requires careful selection of the number of neighbors (k).</li> </ul> </li> </ul> Note!<p>By default, the KNNImputer uses 'nan' values as missing data and the 'nan_euclidean' metric to calculate the distances between values.</p> <ul> <li>Example:</li> </ul> <pre><code>from sklearn.impute import KNNImputer\n\nknn_imputer = KNNImputer(n_neighbors=2)\ntemp_wind_knn_imputed = knn_imputer.fit_transform(temp_wind)\n\ntemp_wind_knn_imputed_df = pd.DataFrame(temp_wind_knn_imputed, columns=temp_wind.columns)\n</code></pre> <p>If we take a look at the outcome</p> <pre><code>weather\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 NaN 9.0 Sunny 2 05/01/2017 28.0 NaN Snow 3 06/01/2017 NaN 7.0 NaN 4 07/01/2017 32.0 NaN Rain 5 08/01/2017 NaN NaN Sunny 6 09/01/2017 NaN NaN NaN 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <p></p>"},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/#filling-a-single-column-independently-using-the-knnimputer","title":"Filling a single column independently using the <code>KNNImputer</code>","text":"<p>To use the KNNImputer for a single independ column, you can use the index as the other column instead, this will result into equal euclidean distances resulting into the use of the physical neighbors in the data table.</p> <pre><code>from sklearn.impute import KNNImputer\n\nknn_imputer = KNNImputer(n_neighbors=2)\nwindspead_imputed = knn_imputer.fit_transform(weather[['windspead']].reset_index())\n\nwindspead_imputed\n</code></pre> <pre><code>array([[ 0. ,  6. ],\n       [ 1. ,  9. ],\n       [ 2. ,  8. ],\n       [ 3. ,  7. ],\n       [ 4. ,  8. ],\n       [ 5. ,  7.5],\n       [ 6. , 10. ],\n       [ 7. ,  8. ],\n       [ 8. , 12. ]])\n</code></pre> <pre><code># we can fill it back in the weather data\nweather['windspead'] = windspead_imputed[:, 1]\n\n# now looking at the data\nweather\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 NaN 9.0 Sunny 2 05/01/2017 28.0 8.0 Snow 3 06/01/2017 NaN 7.0 NaN 4 07/01/2017 32.0 8.0 Rain 5 08/01/2017 NaN 7.5 Sunny 6 09/01/2017 NaN 10.0 NaN 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny"},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/#exercise_1","title":"Exercise","text":"<ul> <li>Try out the KNNImputer with different numbers of neighbors and compare the results</li> <li>Findo out how to use KNNImputer to fill categorical data</li> </ul>"},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/#iterativeimputer-from-scikit-learn","title":"IterativeImputer from scikit-learn:","text":"<ul> <li>Usage: IterativeImputer models each feature with missing values as a function of other features and uses that estimate for imputation. It iteratively estimates the missing values.</li> <li>Pros:<ul> <li>Takes into account relationships between features, making it suitable for datasets with complex missing patterns.</li> <li>More robust than SimpleImputer for handling missing data.</li> </ul> </li> <li>Cons:<ul> <li>Can be computationally intensive and slower than SimpleImputer.</li> <li>Requires careful tuning of model parameters.</li> </ul> </li> <li>Example:</li> </ul> <pre><code>from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\niterative_imputer = IterativeImputer()\ntemp_wind_iterative_imputed = iterative_imputer.fit_transform(temp_wind)\n\ntemp_wind_iterative_imputed_df = pd.DataFrame(temp_wind_iterative_imputed, columns=temp_wind.columns)\n\ntemp_wind_iterative_imputed_df\n</code></pre> temperature windspead 0 32.000000 6.0 1 33.967053 9.0 2 28.000000 8.0 3 31.410210 7.0 4 32.000000 8.0 5 32.049421 7.5 6 35.245474 10.0 7 34.000000 8.0 8 40.000000 12.0 <p>You can also choose an estimator of your choice, let's try a <code>Linear Regression</code> model</p> <pre><code>from sklearn.linear_model import LinearRegression\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# set estimator to an instance of a model\niterative_imputer = IterativeImputer(estimator=LinearRegression())\ntemp_wind_iterative_imputed = iterative_imputer.fit_transform(temp_wind)\n\ntemp_wind_iterative_imputed_df = pd.DataFrame(temp_wind_iterative_imputed, columns=temp_wind.columns)\n\ntemp_wind_iterative_imputed_df\n</code></pre> temperature windspead 0 32.000000 6.0 1 34.125000 9.0 2 28.000000 8.0 3 31.041667 7.0 4 32.000000 8.0 5 31.812500 7.5 6 35.666667 10.0 7 34.000000 8.0 8 40.000000 12.0 <p></p>"},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/#datawig","title":"Datawig:","text":"<p>Datawig is a library specifically designed for imputing missing values in tabular data using deep learning models.</p> <pre><code># import datawig\n\n# # Impute missing values\n# df_imputed = datawig.SimpleImputer.complete(weather)\n</code></pre> <p>These top imputation methods offer different trade-offs in terms of computational complexity, handling of missing data patterns, and ease of use. The choice between them depends on the specific characteristics of the dataset and the requirements of the analysis.</p>"},{"location":"Data%20Collection%20and%20Visulization/76_handling_missing_data/#homework","title":"HomeworkWhat's on your mind? Put it in the comments!","text":"<ul> <li>Try out these techniques for categorical data</li> </ul> <p> Don't miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it's easy and safe. </p>"},{"location":"Deep%20Learning/helper_code/","title":"Helper code","text":"<p>The class for plot the diagram</p> In\u00a0[\u00a0]: Copied! <pre>class plot_error_surfaces(object):\n    \n    # Constructor\n    def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True):\n        W = np.linspace(-w_range, w_range, n_samples)\n        B = np.linspace(-b_range, b_range, n_samples)\n        w, b = np.meshgrid(W, B)    \n        Z = np.zeros((30, 30))\n        count1 = 0\n        self.y = Y.numpy()\n        self.x = X.numpy()\n        for w1, b1 in zip(w, b):\n            count2 = 0\n            for w2, b2 in zip(w1, b1):\n                Z[count1, count2] = np.mean((self.y - w2 * self.x + b2) ** 2)\n                count2 += 1\n            count1 += 1\n        self.Z = Z\n        self.w = w\n        self.b = b\n        self.W = []\n        self.B = []\n        self.LOSS = []\n        self.n = 0\n        if go == True:\n            plt.figure()\n            plt.figure(figsize = (7.5, 5))\n            plt.axes(projection = '3d').plot_surface(self.w, self.b, self.Z, rstride = 1, cstride = 1,cmap = 'viridis', edgecolor = 'none')\n            plt.title('Loss Surface')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.show()\n            plt.figure()\n            plt.title('Loss Surface Contour')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.contour(self.w, self.b, self.Z)\n            plt.show()\n    \n    # Setter\n    def set_para_loss(self, W, B, loss):\n        self.n = self.n + 1\n        self.W.append(W)\n        self.B.append(B)\n        self.LOSS.append(loss)\n    \n    # Plot diagram\n    def final_plot(self): \n        ax = plt.axes(projection = '3d')\n        ax.plot_wireframe(self.w, self.b, self.Z)\n        ax.scatter(self.W, self.B, self.LOSS, c = 'r', marker = 'x', s = 200, alpha = 1)\n        plt.figure()\n        plt.contour(self.w, self.b, self.Z)\n        plt.scatter(self.W, self.B, c = 'r', marker = 'x')\n        plt.xlabel('w')\n        plt.ylabel('b')\n        plt.show()\n    \n    # Plot diagram\n    def plot_ps(self):\n        plt.subplot(121)\n        plt.ylim\n        plt.plot(self.x, self.y, 'ro', label = \"training points\")\n        plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label = \"estimated line\")\n        plt.xlabel('x')\n        plt.ylabel('y')\n        plt.ylim((-10, 15))\n        plt.title('Data Space Iteration: ' + str(self.n))\n        plt.subplot(122)\n        plt.contour(self.w, self.b, self.Z)\n        plt.scatter(self.W, self.B, c = 'r', marker = 'x')\n        plt.title('Loss Surface Contour Iteration' + str(self.n))\n        plt.xlabel('w')\n        plt.ylabel('b')\n        plt.show()\n</pre> class plot_error_surfaces(object):          # Constructor     def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True):         W = np.linspace(-w_range, w_range, n_samples)         B = np.linspace(-b_range, b_range, n_samples)         w, b = np.meshgrid(W, B)             Z = np.zeros((30, 30))         count1 = 0         self.y = Y.numpy()         self.x = X.numpy()         for w1, b1 in zip(w, b):             count2 = 0             for w2, b2 in zip(w1, b1):                 Z[count1, count2] = np.mean((self.y - w2 * self.x + b2) ** 2)                 count2 += 1             count1 += 1         self.Z = Z         self.w = w         self.b = b         self.W = []         self.B = []         self.LOSS = []         self.n = 0         if go == True:             plt.figure()             plt.figure(figsize = (7.5, 5))             plt.axes(projection = '3d').plot_surface(self.w, self.b, self.Z, rstride = 1, cstride = 1,cmap = 'viridis', edgecolor = 'none')             plt.title('Loss Surface')             plt.xlabel('w')             plt.ylabel('b')             plt.show()             plt.figure()             plt.title('Loss Surface Contour')             plt.xlabel('w')             plt.ylabel('b')             plt.contour(self.w, self.b, self.Z)             plt.show()          # Setter     def set_para_loss(self, W, B, loss):         self.n = self.n + 1         self.W.append(W)         self.B.append(B)         self.LOSS.append(loss)          # Plot diagram     def final_plot(self):          ax = plt.axes(projection = '3d')         ax.plot_wireframe(self.w, self.b, self.Z)         ax.scatter(self.W, self.B, self.LOSS, c = 'r', marker = 'x', s = 200, alpha = 1)         plt.figure()         plt.contour(self.w, self.b, self.Z)         plt.scatter(self.W, self.B, c = 'r', marker = 'x')         plt.xlabel('w')         plt.ylabel('b')         plt.show()          # Plot diagram     def plot_ps(self):         plt.subplot(121)         plt.ylim         plt.plot(self.x, self.y, 'ro', label = \"training points\")         plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label = \"estimated line\")         plt.xlabel('x')         plt.ylabel('y')         plt.ylim((-10, 15))         plt.title('Data Space Iteration: ' + str(self.n))         plt.subplot(122)         plt.contour(self.w, self.b, self.Z)         plt.scatter(self.W, self.B, c = 'r', marker = 'x')         plt.title('Loss Surface Contour Iteration' + str(self.n))         plt.xlabel('w')         plt.ylabel('b')         plt.show()"},{"location":"Deep%20Learning/outline/","title":"Pytorch Deep Learning Outline","text":"Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Deep%20Learning/outline/#week-1-tensors-and-datasets","title":"Week 1: Tensors and Datasets","text":"<ul> <li>1 Dimension Tensors</li> <li>Two Dimension Tensors</li> <li>Derivatives and Graphs in PyTorch</li> <li>Simple Dataset</li> <li>Pre Built Datasets</li> </ul> <p>Get Started </p>"},{"location":"Deep%20Learning/outline/#week-2-linear-regression","title":"Week 2: Linear Regression","text":"<ul> <li>Linear Regression 1 Dimension</li> <li>Linear Regression with 1 Parameter</li> <li>Training Slope and Bias</li> </ul> <p>Get Started </p>"},{"location":"Deep%20Learning/outline/#week-3-linear-regression-in-pytorch","title":"Week 3: Linear Regression in PyTorch","text":"<ul> <li>Stochastic Gradient Descent</li> <li>Mini-Batch Gradient Descent</li> <li>PyTorch Build-in Functions</li> <li>Training and Validation Sets</li> </ul> <p>Get Started </p>"},{"location":"Deep%20Learning/outline/#week-4-multiple-input-linear-regression","title":"Week 4: Multiple Input Linear Regression","text":"<ul> <li>Making Predictions in Multiple Linear Regression</li> <li>Training a Multiple Linear Regression Models</li> <li>Multi-Target Linear Regression</li> <li>Training Multiple Output Linear Regression Models</li> </ul> <p>Get Started </p>"},{"location":"Deep%20Learning/outline/#week-5-logistic-regression","title":"Week 5: Logistic Regression","text":"<ul> <li>Making Predictions in Multiple Linear Regression</li> <li>Logistic Regression and Bad Initialization Values</li> <li>Cross Entropy Loss Function</li> <li>Sofmax Activation in 1 Dimension</li> </ul> <p>Get Started </p>"},{"location":"Deep%20Learning/outline/#week-6-practice","title":"Week 6: Practice","text":"<ul> <li>Practice</li> </ul>"},{"location":"Deep%20Learning/outline/#week-7-shallow-neural-networks","title":"Week 7: Shallow Neural Networks","text":"<ul> <li>Simple One Hidden Layer</li> <li>Multiple Neurons</li> <li>Noisy XO</li> <li>One Layer Neural Network</li> <li>Activation Functions</li> <li>Test Activation Functions</li> </ul> <p>Get Started </p>"},{"location":"Deep%20Learning/outline/#week-8-deep-neural-networks","title":"Week 8: Deep Neural Networks","text":"<ul> <li>Multiple Linear Regression</li> <li>Deeper Neural Networks with nn.ModuleList()</li> <li>Using Dropout for Classification</li> <li>Neural Networks with Momentum</li> </ul> <p>Get Started </p>"},{"location":"Deep%20Learning/outline/#week-9-convolution-neural-networks","title":"Week 9: Convolution Neural Networks","text":"<ul> <li>What is Convolution</li> <li>Activatioin Function and Max Pooling</li> <li>Multiple Channel Convolutional Neural Network</li> <li>Convolutional Neural Network with Batch Normalization Get Started </li> </ul>"},{"location":"Deep%20Learning/outline/#week-x-capstone-project-and-review","title":"Week X: Capstone Project and Review","text":"<ul> <li>Applying learned concepts to a real-world dataset</li> </ul>"},{"location":"Deep%20Learning/oxdeep-learning-intro/","title":"Oxdeep learning intro","text":"About the Author: <p>Hi, My name is Juma Shafara. Am a Data Scientist at Raising The Village and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning. I enjoy developing innovative algorithms and models that can drive insights and value. I regularly share some content that I find useful throughout my work/learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org. Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#title-deep-learning-intro-author-juma-shafara-date-2025-25-02-keywords-data-science-data-analysis-programming-dataidea-description-python-for-data-science-is-a-subject-weve-designed-to-explore-the-various-programming-components-of-data-science","title":"title: Deep Learning Intro author: Juma Shafara date: \"2025-25-02\" keywords: [data science, data analysis, programming, dataidea] description: Python for Data Science is a subject we\u2019ve designed to explore the various programming components of data science.\u00b6","text":""},{"location":"Deep%20Learning/oxdeep-learning-intro/#welcome-to-deep-learning","title":"Welcome to Deep Learning!\u00b6","text":"<p>by Juma Shafara</p> <p>Welcome to DATAIDEA's Introduction to Deep Learning You're about to learn all you need to get started building your own deep neural networks. You'll learn how to:</p> <ul> <li>create a fully-connected neural network architecture</li> <li>apply neural nets to two classic ML problems: regression and classification</li> <li>train neural nets with stochastic gradient descent, and</li> <li>improve performance with dropout, batch normalization, and other techniques</li> </ul> <p>Let's get started!</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#what-is-deep-learning","title":"What is Deep Learning?\u00b6","text":"<p>Some of the most impressive advances in artificial intelligence in recent years have been in the field of deep learning. Natural language translation, image recognition, and game playing are all tasks where deep learning models have neared or even exceeded human-level performance.</p> <p>So what is deep learning? Deep learning is an approach to machine learning characterized by deep stacks of computations. This depth of computation is what has enabled deep learning models to disentangle the kinds of complex and hierarchical patterns found in the most challenging real-world datasets.</p> <p>Through their power and scalability neural networks have become the defining model of deep learning.  Neural networks are composed of neurons, where each neuron individually performs only a simple computation. The power of a neural network comes instead from the complexity of the connections these neurons can form.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#the-linear-unit","title":"The Linear Unit\u00b6","text":"<p>So let's begin with the fundamental component of a neural network: the individual neuron. As a diagram, a neuron (or unit) with one input looks like:</p> The Linear Unit: $y = w x + b$  <p>The input is <code>x</code>. Its connection to the neuron has a weight which is <code>w</code>. Whenever a value flows through a connection, you multiply the value by the connection's weight. For the input <code>x</code>, what reaches the neuron is <code>w * x</code>. A neural network \"learns\" by modifying its weights.</p> <p>The <code>b</code> is a special kind of weight we call the bias. The bias doesn't have any input data associated with it; instead, we put a <code>1</code> in the diagram so that the value that reaches the neuron is just <code>b</code> (since <code>1 * b = b</code>). The bias enables the neuron to modify the output independently of its inputs.</p> <p>The <code>y</code> is the value the neuron ultimately outputs. To get the output, the neuron sums up all the values it receives through its connections. This neuron's activation is <code>y = w * x + b</code>, or as a formula $y = w x + b$.</p> Does the formula $y=w x + b$ look familiar? It's an equation of a line! It's the slope-intercept equation, where $w$ is the slope and $b$ is the y-intercept."},{"location":"Deep%20Learning/oxdeep-learning-intro/#example-the-linear-unit-as-a-model","title":"Example - The Linear Unit as a Model\u00b6","text":"<p>Though individual neurons will usually only function as part of a larger network, it's often useful to start with a single neuron model as a baseline. Single neuron models are linear models.</p> <p>Let's think about how this might work on a dataset like 80 Cereals. Training a model with <code>'sugars'</code> (grams of sugars per serving) as input and <code>'calories'</code> (calories per serving) as output, we might find the bias is <code>b=90</code> and the weight is <code>w=2.5</code>. We could estimate the calorie content of a cereal with 5 grams of sugar per serving like this:</p> Computing with the linear unit.  <p>And, checking against our formula, we have $calories = 2.5 \\times 5 + 90 = 102.5$, just like we expect.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#multiple-inputs","title":"Multiple Inputs\u00b6","text":"<p>The 80 Cereals dataset has many more features than just <code>'sugars'</code>. What if we wanted to expand our model to include things like fiber or protein content? That's easy enough. We can just add more input connections to the neuron, one for each additional feature. To find the output, we would multiply each input to its connection weight and then add them all together.</p> A linear unit with three inputs.  <p>The formula for this neuron would be $y = w_0 x_0 + w_1 x_1 + w_2 x_2 + b$. A linear unit with two inputs will fit a plane, and a unit with more inputs than that will fit a hyperplane.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#deep-neural-networks-introduction","title":"Deep Neural Networks Introduction\u00b6","text":"<p>In this lesson we're going to see how we can build neural networks capable of learning the complex kinds of relationships deep neural nets are famous for.</p> <p>The key idea here is modularity, building up a complex network from simpler functional units. We've seen how a linear unit computes a linear function -- now we'll see how to combine and modify these single units to model more complex relationships.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#layers","title":"Layers\u00b6","text":"<p>Neural networks typically organize their neurons into layers. When we collect together linear units having a common set of inputs we get a dense layer.</p> A dense layer of two linear units receiving two inputs and a bias.  <p>You could think of each layer in a neural network as performing some kind of relatively simple transformation. Through a deep stack of layers, a neural network can transform its inputs in more and more complex ways. In a well-trained neural network, each layer is a transformation getting us a little bit closer to a solution.</p> Many Kinds of Layers A \"layer\" in Keras is a very general kind of thing. A layer can be, essentially, any kind of data transformation. Many layers, like the convolutional and recurrent layers, transform data through use of neurons and differ primarily in the pattern of connections they form. Others though are used for feature engineering or just simple arithmetic. There's a whole world of layers to discover -- check them out!"},{"location":"Deep%20Learning/oxdeep-learning-intro/#the-activation-function","title":"The Activation Function\u00b6","text":"<p>It turns out, however, that two dense layers with nothing in between are no better than a single dense layer by itself. Dense layers by themselves can never move us out of the world of lines and planes. What we need is something nonlinear. What we need are activation functions.</p> Without activation functions, neural networks can only learn linear relationships. In order to fit curves, we'll need to use activation functions.   <p>An activation function is simply some function we apply to each of a layer's outputs (its activations). The most common is the rectifier function $max(0, x)$.</p> <p>The rectifier function has a graph that's a line with the negative part \"rectified\" to zero. Applying the function to the outputs of a neuron will put a bend in the data, moving us away from simple lines.</p> <p>When we attach the rectifier to a linear unit, we get a rectified linear unit or ReLU. (For this reason, it's common to call the rectifier function the \"ReLU function\".)  Applying a ReLU activation to a linear unit means the output becomes <code>max(0, w * x + b)</code>, which we might draw in a diagram like:</p> A rectified linear unit."},{"location":"Deep%20Learning/oxdeep-learning-intro/#stacking-dense-layers","title":"Stacking Dense Layers\u00b6","text":"<p>Now that we have some nonlinearity, let's see how we can stack layers to get complex data transformations.</p> A stack of dense layers makes a \"fully-connected\" network.  <p>The layers before the output layer are sometimes called hidden since we never see their outputs directly.</p> <p>Now, notice that the final (output) layer is a linear unit (meaning, no activation function). That makes this network appropriate to a regression task, where we are trying to predict some arbitrary numeric value. Other tasks (like classification) might require an activation function on the output.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#building-sequential-models","title":"Building Sequential Models\u00b6","text":"<p>The <code>Sequential</code> model we've been using will connect together a list of layers in order from first to last: the first layer gets the input, the last layer produces the output. This creates the model in the figure above:</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#stochastic-gradient-descent","title":"Stochastic Gradient Descent\u00b6","text":"<p>In the first two lessons, we learned how to build fully-connected networks out of stacks of dense layers. When first created, all of the network's weights are set randomly -- the network doesn't \"know\" anything yet. In this lesson we're going to see how to train a neural network; we're going to see how neural networks learn.</p> <p>As with all machine learning tasks, we begin with a set of training data. Each example in the training data consists of some features (the inputs) together with an expected target (the output). Training the network means adjusting its weights in such a way that it can transform the features into the target. In the 80 Cereals dataset, for instance, we want a network that can take each cereal's <code>'sugar'</code>, <code>'fiber'</code>, and <code>'protein'</code> content and produce a prediction for that cereal's <code>'calories'</code>. If we can successfully train a network to do that, its weights must represent in some way the relationship between those features and that target as expressed in the training data.</p> <p>In addition to the training data, we need two more things:</p> <ul> <li>A \"loss function\" that measures how good the network's predictions are.</li> <li>An \"optimizer\" that can tell the network how to change its weights.</li> </ul>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#the-loss-function","title":"The Loss Function\u00b6","text":"<p>We've seen how to design an architecture for a network, but we haven't seen how to tell a network what problem to solve. This is the job of the loss function.</p> <p>The loss function measures the disparity between the the target's true value and the value the model predicts.</p> <p>Different problems call for different loss functions. We have been looking at regression problems, where the task is to predict some numerical value -- calories in 80 Cereals, rating in Red Wine Quality. Other regression tasks might be predicting the price of a house or the fuel efficiency of a car.</p> <p>A common loss function for regression problems is the mean absolute error or MAE. For each prediction <code>y_pred</code>, MAE measures the disparity from the true target <code>y_true</code> by an absolute difference <code>abs(y_true - y_pred)</code>.</p> <p>The total MAE loss on a dataset is the mean of all these absolute differences.</p> The mean absolute error is the average length between the fitted curve and the data points.  <p>Besides MAE, other loss functions you might see for regression problems are the mean-squared error (MSE) or the Huber loss (both available in Keras).</p> <p>During training, the model will use the loss function as a guide for finding the correct values of its weights (lower loss is better). In other words, the loss function tells the network its objective.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#the-optimizer-stochastic-gradient-descent","title":"The Optimizer - Stochastic Gradient Descent\u00b6","text":"<p>We've described the problem we want the network to solve, but now we need to say how to solve it. This is the job of the optimizer. The optimizer is an algorithm that adjusts the weights to minimize the loss.</p> <p>Virtually all of the optimization algorithms used in deep learning belong to a family called stochastic gradient descent. They are iterative algorithms that train a network in steps. One step of training goes like this:</p> <ol> <li>Sample some training data and run it through the network to make predictions.</li> <li>Measure the loss between the predictions and the true values.</li> <li>Finally, adjust the weights in a direction that makes the loss smaller.</li> </ol> <p>Then just do this over and over until the loss is as small as you like (or until it won't decrease any further.)</p> Training a neural network with Stochastic Gradient Descent.  <p>Each iteration's sample of training data is called a minibatch (or often just \"batch\"), while a complete round of the training data is called an epoch. The number of epochs you train for is how many times the network will see each training example.</p> <p>The animation shows the linear model from Lesson 1 being trained with SGD. The pale red dots depict the entire training set, while the solid red dots are the minibatches. Every time SGD sees a new minibatch, it will shift the weights (<code>w</code> the slope and <code>b</code> the y-intercept) toward their correct values on that batch. Batch after batch, the line eventually converges to its best fit. You can see that the loss gets smaller as the weights get closer to their true values.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#learning-rate-and-batch-size","title":"Learning Rate and Batch Size\u00b6","text":"<p>Notice that the line only makes a small shift in the direction of each batch (instead of moving all the way). The size of these shifts is determined by the learning rate. A smaller learning rate means the network needs to see more minibatches before its weights converge to their best values.</p> <p>The learning rate and the size of the minibatches are the two parameters that have the largest effect on how the SGD training proceeds. Their interaction is often subtle and the right choice for these parameters isn't always obvious. (We'll explore these effects in the exercise.)</p> <p>Fortunately, for most work it won't be necessary to do an extensive hyperparameter search to get satisfactory results. Adam is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning (it is \"self tuning\", in a sense). Adam is a great general-purpose optimizer.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#adding-the-loss-and-optimizer","title":"Adding the Loss and Optimizer\u00b6","text":"<p>After defining a model, you can add a loss function and optimizer with the model's <code>compile</code> method:</p> <pre><code>model.compile(\n    optimizer=\"adam\",\n    loss=\"mae\",\n)\n</code></pre> <p>Notice that we are able to specify the loss and optimizer with just a string. You can also access these directly through the Keras API -- if you wanted to tune parameters, for instance -- but for us, the defaults will work fine.</p> What's In a Name? The gradient is a vector that tells us in what direction the weights need to go. More precisely, it tells us how to change the weights to make the loss change fastest. We call our process gradient descent because it uses the gradient to descend the loss curve towards a minimum. Stochastic means \"determined by chance.\" Our training is stochastic because the minibatches are random samples from the dataset. And that's why it's called SGD!"},{"location":"Deep%20Learning/oxdeep-learning-intro/#overfitting-and-underfitting","title":"Overfitting and underfitting\u00b6","text":"<p>Recall from the example in the previous lesson that Keras will keep a history of the training and validation loss over the epochs that it is training the model. In this lesson, we're going to learn how to interpret these learning curves and how we can use them to guide model development. In particular, we'll examine at the learning curves for evidence of underfitting and overfitting and look at a couple of strategies for correcting it.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#interpreting-the-learning-curves","title":"Interpreting the Learning Curves\u00b6","text":"<p>You might think about the information in the training data as being of two kinds: signal and noise. The signal is the part that generalizes, the part that can help our model make predictions from new data. The noise is that part that is only true of the training data; the noise is all of the random fluctuation that comes from data in the real-world or all of the incidental, non-informative patterns that can't actually help the model make predictions. The noise is the part might look useful but really isn't.</p> <p>We train a model by choosing weights or parameters that minimize the loss on a training set. You might know, however, that to accurately assess a model's performance, we need to evaluate it on a new set of data, the validation data. (You could see our lesson on model validation in Introduction to Machine Learning for a review.)</p> <p>When we train a model we've been plotting the loss on the training set epoch by epoch. To this we'll add a plot the validation data too. These plots we call the learning curves. To train deep learning models effectively, we need to be able to interpret them.</p> The validation loss gives an estimate of the expected error on unseen data.  <p>Now, the training loss will go down either when the model learns signal or when it learns noise. But the validation loss will go down only when the model learns signal. (Whatever noise the model learned from the training set won't generalize to new data.) So, when a model learns signal both curves go down, but when it learns noise a gap is created in the curves. The size of the gap tells you how much noise the model has learned.</p> <p>Ideally, we would create models that learn all of the signal and none of the noise. This will practically never happen. Instead we make a trade. We can get the model to learn more signal at the cost of learning more noise. So long as the trade is in our favor, the validation loss will continue to decrease. After a certain point, however, the trade can turn against us, the cost exceeds the benefit, and the validation loss begins to rise.</p> Underfitting and overfitting.  <p>This trade-off indicates that there can be two problems that occur when training a model: not enough signal or too much noise. Underfitting the training set is when the loss is not as low as it could be because the model hasn't learned enough signal. Overfitting the training set is when the loss is not as low as it could be because the model learned too much noise. The trick to training deep learning models is finding the best balance between the two.</p> <p>We'll look at a couple ways of getting more signal out of the training data while reducing the amount of noise.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#capacity","title":"Capacity\u00b6","text":"<p>A model's capacity refers to the size and complexity of the patterns it is able to learn. For neural networks, this will largely be determined by how many neurons it has and how they are connected together. If it appears that your network is underfitting the data, you should try increasing its capacity.</p> <p>You can increase the capacity of a network either by making it wider (more units to existing layers) or by making it deeper (adding more layers). Wider networks have an easier time learning more linear relationships, while deeper networks prefer more nonlinear ones. Which is better just depends on the dataset.</p> <p>You'll explore how the capacity of a network can affect its performance in the exercise.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#early-stopping","title":"Early Stopping\u00b6","text":"<p>We mentioned that when a model is too eagerly learning noise, the validation loss may start to increase during training. To prevent this, we can simply stop the training whenever it seems the validation loss isn't decreasing anymore. Interrupting the training this way is called early stopping.</p> We keep the model where the validation loss is at a minimum.  <p>Once we detect that the validation loss is starting to rise again, we can reset the weights back to where the minimum occured. This ensures that the model won't continue to learn noise and overfit the data.</p> <p>Training with early stopping also means we're in less danger of stopping the training too early, before the network has finished learning signal. So besides preventing overfitting from training too long, early stopping can also prevent underfitting from not training long enough. Just set your training epochs to some large number (more than you'll need), and early stopping will take care of the rest.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#adding-early-stopping","title":"Adding Early Stopping\u00b6","text":"<p>We can include early stopping in our training through a callback. A callback is just a function you want run every so often while the network trains. The early stopping callback will run after every epoch. (Keras has a variety of useful callbacks pre-defined, but you can define your own, too.)</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#drop-out-and-batch-normalization","title":"Drop out and batch normalization\u00b6","text":"<p>There's more to the world of deep learning than just dense layers. There are dozens of kinds of layers you might add to a model. (Try browsing through the Keras docs for a sample!) Some are like dense layers and define connections between neurons, and others can do preprocessing or transformations of other sorts.</p> <p>In this lesson, we'll learn about a two kinds of special layers, not containing any neurons themselves, but that add some functionality that can sometimes benefit a model in various ways. Both are commonly used in modern architectures.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#dropout","title":"Dropout\u00b6","text":"<p>The first of these is the \"dropout layer\", which can help correct overfitting.</p> <p>In the last lesson we talked about how overfitting is caused by the network learning spurious patterns in the training data. To recognize these spurious patterns a network will often rely on very a specific combinations of weight, a kind of \"conspiracy\" of weights. Being so specific, they tend to be fragile: remove one and the conspiracy falls apart.</p> <p>This is the idea behind dropout. To break up these conspiracies, we randomly drop out some fraction of a layer's input units every step of training, making it much harder for the network to learn those spurious patterns in the training data. Instead, it has to search for broad, general patterns, whose weight patterns tend to be more robust.</p> Here, 50% dropout has been added between the two hidden layers. <p>You could also think about dropout as creating a kind of ensemble of networks. The predictions will no longer be made by one big network, but instead by a committee of smaller networks. Individuals in the committee tend to make different kinds of mistakes, but be right at the same time, making the committee as a whole better than any individual. (If you're familiar with random forests as an ensemble of decision trees, it's the same idea.)</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#batch-normalization","title":"Batch Normalization\u00b6","text":"<p>The next special layer we'll look at performs \"batch normalization\" (or \"batchnorm\"), which can help correct training that is slow or unstable.</p> <p>With neural networks, it's generally a good idea to put all of your data on a common scale, perhaps with something like scikit-learn's StandardScaler or MinMaxScaler. The reason is that SGD will shift the network weights in proportion to how large an activation the data produces. Features that tend to produce activations of very different sizes can make for unstable training behavior.</p> <p>Now, if it's good to normalize the data before it goes into the network, maybe also normalizing inside the network would be better! In fact, we have a special kind of layer that can do this, the batch normalization layer. A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also putting the data on a new scale with two trainable rescaling parameters. Batchnorm, in effect, performs a kind of coordinated rescaling of its inputs.</p> <p>Most often, batchnorm is added as an aid to the optimization process (though it can sometimes also help prediction performance). Models with batchnorm tend to need fewer epochs to complete training. Moreover, batchnorm can also fix various problems that can cause the training to get \"stuck\". Consider adding batch normalization to your models, especially if you're having trouble during training.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#binary-classification","title":"Binary Classification\u00b6","text":"<p>So far in this course, we've learned about how neural networks can solve regression problems. Now we're going to apply neural networks to another common machine learning problem: classification. Most everything we've learned up until now still applies. The main difference is in the loss function we use and in what kind of outputs we want the final layer to produce.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#binary-classification","title":"Binary Classification\u00b6","text":"<p>Classification into one of two classes is a common machine learning problem. You might want to predict whether or not a customer is likely to make a purchase, whether or not a credit card transaction was fraudulent, whether deep space signals show evidence of a new planet, or a medical test evidence of a disease. These are all binary classification problems.</p> <p>In your raw data, the classes might be represented by strings like <code>\"Yes\"</code> and <code>\"No\"</code>, or <code>\"Dog\"</code> and <code>\"Cat\"</code>. Before using this data we'll assign a class label: one class will be <code>0</code> and the other will be <code>1</code>. Assigning numeric labels puts the data in a form a neural network can use.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#accuracy-and-cross-entropy","title":"Accuracy and Cross-Entropy\u00b6","text":"<p>Accuracy is one of the many metrics in use for measuring success on a classification problem. Accuracy is the ratio of correct predictions to total predictions: <code>accuracy = number_correct / total</code>. A model that always predicted correctly would have an accuracy score of <code>1.0</code>. All else being equal, accuracy is a reasonable metric to use whenever the classes in the dataset occur with about the same frequency.</p> <p>The problem with accuracy (and most other classification metrics) is that it can't be used as a loss function. SGD needs a loss function that changes smoothly, but accuracy, being a ratio of counts, changes in \"jumps\". So, we have to choose a substitute to act as the loss function. This substitute is the cross-entropy function.</p> <p>Now, recall that the loss function defines the objective of the network during training. With regression, our goal was to minimize the distance between the expected outcome and the predicted outcome. We chose MAE to measure this distance.</p> <p>For classification, what we want instead is a distance between probabilities, and this is what cross-entropy provides. Cross-entropy is a sort of measure for the distance from one probability distribution to another.</p> Cross-entropy penalizes incorrect probability predictions. <p>The idea is that we want our network to predict the correct class with probability <code>1.0</code>. The further away the predicted probability is from <code>1.0</code>, the greater will be the cross-entropy loss.</p> <p>The technical reasons we use cross-entropy are a bit subtle, but the main thing to take away from this section is just this: use cross-entropy for a classification loss; other metrics you might care about (like accuracy) will tend to improve along with it.</p>"},{"location":"Deep%20Learning/oxdeep-learning-intro/#making-probabilities-with-the-sigmoid-function","title":"Making Probabilities with the Sigmoid Function\u00b6","text":"<p>The cross-entropy and accuracy functions both require probabilities as inputs, meaning, numbers from 0 to 1. To covert the real-valued outputs produced by a dense layer into probabilities, we attach a new kind of activation function, the sigmoid activation.</p> The sigmoid function maps real numbers into the interval $[0, 1]$. <p>To get the final class prediction, we define a threshold probability. Typically this will be 0.5, so that rounding will give us the correct class: below 0.5 means the class with label 0 and 0.5 or above means the class with label 1. A 0.5 threshold is what Keras uses by default with its accuracy metric.</p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/","title":"Torch Tensors in 1D","text":"Objective <ul><li> How tensor operations work in pytorch.</li></ul> Table of Contents <p>In this lab, you will learn the basics of tensor operations. Tensors are an essential part of PyTorch; there are complex mathematical objects in and of themselves. Fortunately, most of the intricacies are not necessary. In this section, you will compare them to vectors and numpy arrays.</p> <ul> <li>Types and Shape</li> <li>Indexing and Slicing</li> <li>Tensor Functions</li> <li>Tensor Operations</li> <li>Device_Op Operations</li> </ul> <p>Estimated Time Needed: 25 min</p>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> Preparation <p>Import the following libraries that you'll use for this lab:</p> <pre><code># These are the libraries will be used for this lab.\n\nimport torch \nimport numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline  \n</code></pre> <p>Check PyTorch version:</p> <pre><code>torch.__version__\n</code></pre> <pre><code>'2.4.0+cpu'\n</code></pre> <p>This is the function for plotting diagrams. You will use this function to plot the vectors in Coordinate system.</p> <pre><code># Plot vecotrs, please keep the parameters in the same length\n# @param: Vectors = [{\"vector\": vector variable, \"name\": name of vector, \"color\": color of the vector on diagram}]\n\ndef plotVec(vectors):\n    ax = plt.axes()\n\n    # For loop to draw the vectors\n    for vec in vectors:\n        ax.arrow(0, 0, *vec[\"vector\"], head_width = 0.05,color = vec[\"color\"], head_length = 0.1)\n        plt.text(*(vec[\"vector\"] + 0.1), vec[\"name\"])\n\n    plt.ylim(-2,2)\n    plt.xlim(-2,2)\n</code></pre>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#Types_Shape","title":"Types and Shape","text":"<p>You can find the type of the following list of integers [0, 1, 2, 3, 4] by applying the constructor <code>torch.tensor()</code>:</p> <pre><code># Convert a integer list with length 5 to a tensor\n\nints_to_tensor = torch.tensor([0, 1, 2, 3, 4])\nprint(\"The dtype of tensor object after converting it to tensor: \", ints_to_tensor.dtype)\nprint(\"The type of tensor object after converting it to tensor: \", ints_to_tensor.type())\n</code></pre> <pre><code>The dtype of tensor object after converting it to tensor:  torch.int64\nThe type of tensor object after converting it to tensor:  torch.LongTensor\n</code></pre> <p>As a result, the integer list has been converted to a long tensor.</p> <p>The Python type is still <code>torch.Tensor</code>:</p> <pre><code>type(ints_to_tensor)\n</code></pre> <pre><code>torch.Tensor\n</code></pre> <p>You can find the type of this float list [0.0, 1.0, 2.0, 3.0, 4.0] by applying the method <code>torch.tensor()</code>:</p> <pre><code># Convert a float list with length 5 to a tensor\n\nfloats_to_tensor = torch.tensor([0.0, 1.0, 2.0, 3.0, 4.0])\nprint(\"The dtype of tensor object after converting it to tensor: \", floats_to_tensor.dtype)\nprint(\"The type of tensor object after converting it to tensor: \", floats_to_tensor.type())\n</code></pre> <pre><code>The dtype of tensor object after converting it to tensor:  torch.float32\nThe type of tensor object after converting it to tensor:  torch.FloatTensor\n</code></pre> <p>The float list is converted to a float tensor.</p> <pre><code>list_floats=[0.0, 1.0, 2.0, 3.0, 4.0]\n\nfloats_int_tensor=torch.tensor(list_floats,dtype=torch.int64)\n</code></pre> <pre><code>print(\"The dtype of tensor object is: \", floats_int_tensor.dtype)\nprint(\"The type of tensor object is: \", floats_int_tensor.type())\n</code></pre> <pre><code>The dtype of tensor object is:  torch.int64\nThe type of tensor object is:  torch.LongTensor\n</code></pre>  Note!<p>The elements in the list that will be converted to tensor must have the same type.</p> <p>From the previous examples, you see that <code>torch.tensor()</code> converts the list to the tensor type, which is similar to the original list type. However, what if you want to convert the list to a certain tensor type? <code>torch</code> contains the methods required to do this conversion. The following code  converts an integer list to float tensor:</p> <pre><code># Convert a integer list with length 5 to float tensor\n\nnew_float_tensor = torch.FloatTensor([0, 1, 2, 3, 4])\nnew_float_tensor.type()\nprint(\"The type of the new_float_tensor:\", new_float_tensor.type())\n</code></pre> <pre><code>The type of the new_float_tensor: torch.FloatTensor\n</code></pre> <pre><code>new_float_tensor = torch.FloatTensor([0, 1, 2, 3, 4])\n</code></pre> <p>You can also convert an existing tensor object (<code>tensor_obj</code>) to another tensor type. Convert the integer tensor to a float tensor:</p> <pre><code># Another method to convert the integer list to float tensor\n\nold_int_tensor = torch.tensor([0, 1, 2, 3, 4])\nnew_float_tensor = old_int_tensor.type(torch.FloatTensor)\nprint(\"The type of the new_float_tensor:\", new_float_tensor.type())\n</code></pre> <pre><code>The type of the new_float_tensor: torch.FloatTensor\n</code></pre> <p>The <code>tensor_obj.size()</code> helps you to find out the size of the <code>tensor_obj</code>. The <code>tensor_obj.ndimension()</code> shows the dimension of the tensor object.</p> <pre><code># Introduce the tensor_obj.size() &amp; tensor_ndimension.size() methods\n\nprint(\"The size of the new_float_tensor: \", new_float_tensor.size())\nprint(\"The dimension of the new_float_tensor: \",new_float_tensor.ndimension())\n</code></pre> <pre><code>The size of the new_float_tensor:  torch.Size([5])\nThe dimension of the new_float_tensor:  1\n</code></pre> <p>The <code>tensor_obj.view(row, column)</code> is used for reshaping a tensor object.</p> <p>What if you have a tensor object with <code>torch.Size([5])</code> as a <code>new_float_tensor</code> as shown in the previous example? After you execute <code>new_float_tensor.view(5, 1)</code>, the size of <code>new_float_tensor</code> will be <code>torch.Size([5, 1])</code>. This means that the tensor object <code>new_float_tensor</code> has been reshaped from a one-dimensional  tensor object with 5 elements to a two-dimensional tensor object with 5 rows and 1 column.</p> <pre><code># Introduce the tensor_obj.view(row, column) method\n\ntwoD_float_tensor = new_float_tensor.view(5, 1)\nprint(\"Original Size: \", new_float_tensor)\nprint(\"Size after view method\", twoD_float_tensor)\n</code></pre> <pre><code>Original Size:  tensor([0., 1., 2., 3., 4.])\nSize after view method tensor([[0.],\n        [1.],\n        [2.],\n        [3.],\n        [4.]])\n</code></pre> <p>Note that the original size is 5. The tensor after reshaping becomes a 5X1 tensor analog to a column vector.</p>  Note!<p>The number of elements in a tensor must remain constant after applying view.</p> <p>What if you have a tensor with dynamic size but you want to reshape it? You can use -1 to do just that.</p> <pre><code># Introduce the use of -1 in tensor_obj.view(row, column) method\n\ntwoD_float_tensor = new_float_tensor.view(-1, 1)\nprint(\"Original Size: \", new_float_tensor)\nprint(\"Size after view method\", twoD_float_tensor)\n</code></pre> <pre><code>Original Size:  tensor([0., 1., 2., 3., 4.])\nSize after view method tensor([[0.],\n        [1.],\n        [2.],\n        [3.],\n        [4.]])\n</code></pre> <p>You get the same result as the previous example. The -1 can represent any size. However, be careful because you can set only one argument as -1.</p> <p>You can also convert a numpy array to a tensor, for example: </p> <pre><code># Convert a numpy array to a tensor\n\nnumpy_array = np.array([0.0, 1.0, 2.0, 3.0, 4.0])\nnew_tensor = torch.from_numpy(numpy_array)\n\nprint(\"The dtype of new tensor: \", new_tensor.dtype)\nprint(\"The type of new tensor: \", new_tensor.type())\n</code></pre> <pre><code>The dtype of new tensor:  torch.float64\nThe type of new tensor:  torch.DoubleTensor\n</code></pre> <p>Converting a tensor to a numpy is also supported in PyTorch. The syntax is shown below:</p> <pre><code># Convert a tensor to a numpy array\n\nback_to_numpy = new_tensor.numpy()\nprint(\"The numpy array from tensor: \", back_to_numpy)\nprint(\"The dtype of numpy array: \", back_to_numpy.dtype)\n</code></pre> <pre><code>The numpy array from tensor:  [0. 1. 2. 3. 4.]\nThe dtype of numpy array:  float64\n</code></pre> <p><code>back_to_numpy</code> and <code>new_tensor</code> still point to <code>numpy_array</code>. As a result if we change <code>numpy_array</code> both <code>back_to_numpy</code> and <code>new_tensor</code> will change. For example if we set all the elements in <code>numpy_array</code> to zeros, <code>back_to_numpy</code> and <code> new_tensor</code> will follow suit.</p> <pre><code># Set all elements in numpy array to zero \nnumpy_array[:] = 0\nprint(\"The new tensor points to numpy_array : \", new_tensor)\nprint(\"and back to numpy array points to the tensor: \", back_to_numpy)\n</code></pre> <pre><code>The new tensor points to numpy_array :  tensor([0., 0., 0., 0., 0.], dtype=torch.float64)\nand back to numpy array points to the tensor:  [0. 0. 0. 0. 0.]\n</code></pre> <p>Pandas Series can also be converted by using the numpy array that is stored in <code>pandas_series.values</code>. Note that <code>pandas_series</code> can be any pandas_series object. </p> <pre><code># Convert a panda series to a tensor\n\npandas_series=pd.Series([0.1, 2, 0.3, 10.1])\nnew_tensor=torch.from_numpy(pandas_series.values)\nprint(\"The new tensor from numpy array: \", new_tensor)\nprint(\"The dtype of new tensor: \", new_tensor.dtype)\nprint(\"The type of new tensor: \", new_tensor.type())\n</code></pre> <pre><code>The new tensor from numpy array:  tensor([ 0.1000,  2.0000,  0.3000, 10.1000], dtype=torch.float64)\nThe dtype of new tensor:  torch.float64\nThe type of new tensor:  torch.DoubleTensor\n</code></pre> <p>consider the following tensor </p> <pre><code>this_tensor=torch.tensor([0,1, 2,3]) \n</code></pre> <p>The method <code>item()</code> returns the value of this tensor as a standard Python number. This only works for one element. </p> <pre><code>this_tensor=torch.tensor([0,1, 2,3]) \n\nprint(\"the first item is given by\",this_tensor[0].item(),\"the first tensor value is given by \",this_tensor[0])\nprint(\"the second item is given by\",this_tensor[1].item(),\"the second tensor value is given by \",this_tensor[1])\nprint(\"the third  item is given by\",this_tensor[2].item(),\"the third tensor value is given by \",this_tensor[2])\n</code></pre> <pre><code>the first item is given by 0 the first tensor value is given by  tensor(0)\nthe second item is given by 1 the second tensor value is given by  tensor(1)\nthe third  item is given by 2 the third tensor value is given by  tensor(2)\n</code></pre> <p>we can use the method <code> tolist()</code> to return a list </p> <pre><code>torch_to_list=this_tensor.tolist()\n\nprint('tensor:', this_tensor,\"\\nlist:\",torch_to_list)\n</code></pre> <pre><code>tensor: tensor([0, 1, 2, 3]) \nlist: [0, 1, 2, 3]\n</code></pre> Practice <p>Try to convert <code>your_tensor</code> to a 1X5 tensor.</p> <pre><code># Practice: convert the following tensor to a tensor object with 1 row and 5 columns\n\nyour_tensor = torch.tensor([1, 2, 3, 4, 5])\n</code></pre>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#Index_Slice","title":"Indexing and Slicing","text":"<p>In Python, the index starts with 0. Therefore, the last index will always be 1 less than the length of the tensor object. You can access the value on a certain index by using the square bracket, for example:</p> <pre><code># A tensor for showing how the indexs work on tensors\n\nindex_tensor = torch.tensor([0, 1, 2, 3, 4])\nprint(\"The value on index 0:\",index_tensor[0])\nprint(\"The value on index 1:\",index_tensor[1])\nprint(\"The value on index 2:\",index_tensor[2])\nprint(\"The value on index 3:\",index_tensor[3])\nprint(\"The value on index 4:\",index_tensor[4])\n</code></pre> <pre><code>The value on index 0: tensor(0)\nThe value on index 1: tensor(1)\nThe value on index 2: tensor(2)\nThe value on index 3: tensor(3)\nThe value on index 4: tensor(4)\n</code></pre> <p>Note that the <code>index_tensor[5]</code> will create an error.</p> <p>The index is shown in the following figure: </p> <p></p> <p>Now, you'll see how to change the values on certain indexes.</p> <p>Suppose you have a tensor as shown here: </p> <pre><code># A tensor for showing how to change value according to the index\n\ntensor_sample = torch.tensor([20, 1, 2, 3, 4])\n</code></pre> <p>Assign the value on index 0 as 100:</p> <pre><code># Change the value on the index 0 to 100\n\nprint(\"Inital value on index 0:\", tensor_sample[0])\ntensor_sample[0] = 100\nprint(\"Modified tensor:\", tensor_sample)\n</code></pre> <pre><code>Inital value on index 0: tensor(20)\nModified tensor: tensor([100,   1,   2,   3,   4])\n</code></pre> <p>As you can see, the value on index 0 changes. Change the value on index 4 to 0:</p> <pre><code># Change the value on the index 4 to 0\n\nprint(\"Inital value on index 4:\", tensor_sample[4])\ntensor_sample[4] = 0\nprint(\"Modified tensor:\", tensor_sample)\n</code></pre> <pre><code>Inital value on index 4: tensor(4)\nModified tensor: tensor([100,   1,   2,   3,   0])\n</code></pre> <p>The value on index 4 turns to 0.</p> <p>If you are familiar with Python, you know that there is a feature called slicing on a list. Tensors support the same feature. </p> <p>Get the subset of <code>tensor_sample</code>. The subset should contain the values in <code>tensor_sample</code> from index 1 to index 3.</p> <pre><code># Slice tensor_sample\n\nsubset_tensor_sample = tensor_sample[1:4]\nprint(\"Original tensor sample: \", tensor_sample)\nprint(\"The subset of tensor sample:\", subset_tensor_sample)\n</code></pre> <pre><code>Original tensor sample:  tensor([100,   1,   2,   3,   0])\nThe subset of tensor sample: tensor([1, 2, 3])\n</code></pre> <p>As a result, the <code>subset_tensor_sample</code> returned only the values on index 1, index 2, and index 3. Then, it stored them in a <code>subset_tensor_sample</code>.</p>  Note!<p>The number on the left side of the colon represents the index of the first value. The number on the right side of the colon is always 1 larger than the index of the last value. For example, tensor_sample[1:4]means you get values from the index 1 to index 3 (4-1).</p> <p>As for assigning values to the certain index, you can also assign the value to the slices:</p> <p>Change the value of <code>tensor_sample</code> from index 3 to index 4:</p> <pre><code># Change the values on index 3 and index 4\n\nprint(\"Inital value on index 3 and index 4:\", tensor_sample[3:5])\ntensor_sample[3:5] = torch.tensor([300.0, 400.0])\nprint(\"Modified tensor:\", tensor_sample)\n</code></pre> <pre><code>Inital value on index 3 and index 4: tensor([3, 0])\nModified tensor: tensor([100,   1,   2, 300, 400])\n</code></pre> <p>The values on both index 3 and index 4 were changed. The values on other indexes remain the same.</p> <p>You can also use a variable to contain the selected indexes and pass that variable to a tensor slice operation as a parameter, for example:  </p> <pre><code># Using variable to contain the selected index, and pass it to slice operation\n\nselected_indexes = [3, 4]\nsubset_tensor_sample = tensor_sample[selected_indexes]\nprint(\"The inital tensor_sample\", tensor_sample)\nprint(\"The subset of tensor_sample with the values on index 3 and 4: \", subset_tensor_sample)\n</code></pre> <pre><code>The inital tensor_sample tensor([100,   1,   2, 300, 400])\nThe subset of tensor_sample with the values on index 3 and 4:  tensor([300, 400])\n</code></pre> <p>You can also assign one value to the selected indexes by using the variable. For example, assign 100,000 to all the <code>selected_indexes</code>:</p> <pre><code>#Using variable to assign the value to the selected indexes\n\nprint(\"The inital tensor_sample\", tensor_sample)\nselected_indexes = [1, 3]\ntensor_sample[selected_indexes] = 100000\nprint(\"Modified tensor with one value: \", tensor_sample)\n</code></pre> <pre><code>The inital tensor_sample tensor([100,   1,   2, 300, 400])\nModified tensor with one value:  tensor([   100, 100000,      2, 100000,    400])\n</code></pre> <p>The values on index 1 and index 3 were changed to 100,000. Others remain the same.</p>  Note!<p>You can use only one value for the assignment.</p> Practice <p>Try to change the values on index 3, 4, 7 of the following tensor to 0.</p> <pre><code># Practice: Change the values on index 3, 4, 7 to 0\n\npractice_tensor = torch.tensor([2, 7, 3, 4, 6, 2, 3, 1, 2])\n</code></pre>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#Tensor_Func","title":"Tensor Functions","text":"<p>For this section, you'll work with some methods that you can apply to tensor objects.</p> Mean and Standard Deviation <p>You'll review the mean and standard deviation methods first. They are two basic statistical methods.</p> <p>Create a tensor with values [1.0, -1, 1, -1]:</p> <pre><code># Sample tensor for mathmatic calculation methods on tensor\n\nmath_tensor = torch.tensor([1.0, -1.0, 1, -1])\nprint(\"Tensor example: \", math_tensor)\n</code></pre> <pre><code>Tensor example:  tensor([ 1., -1.,  1., -1.])\n</code></pre> <p>Here is the mean method:  </p> <pre><code>#Calculate the mean for math_tensor\n\nmean = math_tensor.mean()\nprint(\"The mean of math_tensor: \", mean)\n</code></pre> <pre><code>The mean of math_tensor:  tensor(0.)\n</code></pre> <p>The standard deviation can also be calculated by using <code>tensor_obj.std()</code>:</p> <pre><code>#Calculate the standard deviation for math_tensor\n\nstandard_deviation = math_tensor.std()\nprint(\"The standard deviation of math_tensor: \", standard_deviation)\n</code></pre> <pre><code>The standard deviation of math_tensor:  tensor(1.1547)\n</code></pre> Max and Min <p>Now, you'll review another two useful methods: <code>tensor_obj.max()</code> and <code>tensor_obj.min()</code>. These two methods are used for finding the maximum value and the minimum value in the tensor.</p> <p>Create a <code>max_min_tensor</code>: </p> <pre><code># Sample for introducing max and min methods\n\nmax_min_tensor = torch.tensor([1, 1, 3, 5, 5])\nprint(\"Tensor example: \", max_min_tensor)\n</code></pre> <pre><code>Tensor example:  tensor([1, 1, 3, 5, 5])\n</code></pre>  Note!<p>There are two minimum numbers as 1 and two maximum numbers as 5 in the tensor. Can you guess how PyTorch is going to deal with the duplicates?</p> <p>Apply <code>tensor_obj.max()</code> on <code>max_min_tensor</code>:</p> <pre><code># Method for finding the maximum value in the tensor\n\nmax_val = max_min_tensor.max()\nprint(\"Maximum number in the tensor: \", max_val)\n</code></pre> <pre><code>Maximum number in the tensor:  tensor(5)\n</code></pre> <p>The answer is <code>tensor(5)</code>. Therefore, the method <code>tensor_obj.max()</code> is grabbing the maximum value but not the elements that contain the maximum value in the tensor.</p> <pre><code> max_min_tensor.max()\n</code></pre> <pre><code>tensor(5)\n</code></pre> <p>Use <code>tensor_obj.min()</code> on <code>max_min_tensor</code>:</p> <pre><code># Method for finding the minimum value in the tensor\n\nmin_val = max_min_tensor.min()\nprint(\"Minimum number in the tensor: \", min_val)\n</code></pre> <pre><code>Minimum number in the tensor:  tensor(1)\n</code></pre> <p>The answer is <code>tensor(1)</code>. Therefore, the method <code>tensor_obj.min()</code> is grabbing the minimum value but not the elements that contain the minimum value in the tensor.</p> Sin <p>Sin is the trigonometric function of an angle. Again, you will not be introducedvto any mathematic functions. You'll focus on Python.</p> <p>Create a tensor with 0, \u03c0/2 and \u03c0. Then, apply the sin function on the tensor. Notice here that the <code>sin()</code> is not a method of tensor object but is a function of torch:</p> <pre><code># Method for calculating the sin result of each element in the tensor\n\npi_tensor = torch.tensor([0, np.pi/2, np.pi])\nsin = (torch.sin(pi_tensor))\nprint(\"The sin result of pi_tensor: \", sin)\n</code></pre> <pre><code>The sin result of pi_tensor:  tensor([ 0.0000e+00,  1.0000e+00, -8.7423e-08])\n</code></pre> <p>The resultant tensor <code>sin</code> contains the result of the <code>sin</code> function applied to each element in the <code>pi_tensor</code>. This is different from the previous methods. For <code>tensor_obj.mean()</code>, <code>tensor_obj.std()</code>, <code>tensor_obj.max()</code>, and <code>tensor_obj.min()</code>, the result is a tensor with only one number because these are aggregate methods. However, the <code>torch.sin()</code> is not. Therefore, the resultant tensors have the same length as the input tensor.</p> Create Tensor by <code>torch.linspace()</code> <p>A useful function for plotting mathematical functions is <code>torch.linspace()</code>. <code>torch.linspace()</code> returns evenly spaced numbers over a specified interval. You specify the starting point of the sequence and the ending point of the sequence. The parameter <code>steps</code> indicates the number of samples to generate. Now, you'll work with <code>steps = 5</code>.</p> <pre><code># First try on using linspace to create tensor\n\nlen_5_tensor = torch.linspace(-2, 2, steps = 5)\nprint (\"First Try on linspace\", len_5_tensor)\n</code></pre> <pre><code>First Try on linspace tensor([-2., -1.,  0.,  1.,  2.])\n</code></pre> <p>Assign <code>steps</code> with 9:</p> <pre><code># Second try on using linspace to create tensor\n\nlen_9_tensor = torch.linspace(-2, 2, steps = 9)\nprint (\"Second Try on linspace\", len_9_tensor)\n</code></pre> <pre><code>Second Try on linspace tensor([-2.0000, -1.5000, -1.0000, -0.5000,  0.0000,  0.5000,  1.0000,  1.5000,\n         2.0000])\n</code></pre> <p>Use both <code>torch.linspace()</code> and <code>torch.sin()</code> to construct a tensor that contains the 100 sin result in range from 0 (0 degree) to 2\u03c0 (360 degree): </p> <pre><code># Construct the tensor within 0 to 360 degree\n\npi_tensor = torch.linspace(0, 2*np.pi, 100)\nsin_result = torch.sin(pi_tensor)\n</code></pre> <p>Plot the result to get a clearer picture. You must cast the tensor to a numpy array before plotting it.</p> <pre><code># Plot sin_result\n\nplt.plot(pi_tensor.numpy(), sin_result.numpy())\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x787287da7020&gt;]\n</code></pre> <p></p> <p>If you know the trigonometric function, you will notice this is the diagram of the sin result in the range 0 to 360 degrees.</p> Practice <p>Construct a tensor with 25 steps in the range 0 to \u03c0/2. Print out the Maximum and Minimum number. Also, plot  a graph showing the diagram that shows the result.</p> <pre><code># Practice: Create your tensor, print max and min number, plot the sin result diagram\n\npi_tensor = torch.linspace(0, np.pi/2, steps = 25)\nprint(\"Maximum: \", pi_tensor.max())\nprint(\"Minimum: \", pi_tensor.min())\nsin_result = torch.sin(pi_tensor)\nplt.plot(pi_tensor.numpy(), sin_result.numpy())\n# Type your code here\n</code></pre> <pre><code>Maximum:  tensor(1.5708)\nMinimum:  tensor(0.)\n\n\n\n\n\n[&lt;matplotlib.lines.Line2D at 0x787287dec5c0&gt;]\n</code></pre> <p></p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#Tensor_Op","title":"Tensor Operations","text":"<p>In the following section, you'll work with operations that you can apply to a tensor.</p> Tensor Addition <p>You can perform addition between two tensors.</p> <p>Create a tensor <code>u</code> with 1 dimension and 2 elements. Then, create another tensor <code>v</code> with the same number of dimensions and the same number of elements:</p> <pre><code># Create two sample tensors\n\nu = torch.tensor([1, 0])\nv = torch.tensor([0, 1])\n</code></pre> <p>Add <code>u</code> and <code>v</code> together:</p> <pre><code># Add u and v\n\nw = u + v\nprint(\"The result tensor: \", w)\n</code></pre> <pre><code>The result tensor:  tensor([1, 1])\n</code></pre> <p>The result is <code>tensor([1, 1])</code>. The behavior is [1 + 0, 0 + 1].</p> <p>Plot the result to to get a clearer picture.</p> <pre><code># Plot u, v, w\n\nplotVec([\n    {\"vector\": u.numpy(), \"name\": 'u', \"color\": 'r'},\n    {\"vector\": v.numpy(), \"name\": 'v', \"color\": 'b'},\n    {\"vector\": w.numpy(), \"name\": 'w', \"color\": 'g'}\n])\n</code></pre> <p></p> Try <p>Implement the tensor subtraction with <code>u</code> and <code>v</code> as u-v.</p> <pre><code># Try by yourself to get a result of u-v\n\nu = torch.tensor([1, 0])\nv = torch.tensor([0, 1])\nw = u - v\nprint(w)\n</code></pre> <pre><code>tensor([ 1, -1])\n</code></pre> <p>Tensors must be of the same data type to perform addition as well as other operations.If you uncomment the  following code and try to run it you will get an error as the two tensors are of two different data types. NOTE This lab was created on a older PyTorch version so in the current version we are using this is possible and will produce a float64 tensor.</p> <pre><code>torch.tensor([1,2,3],dtype=torch.int64)+torch.tensor([1,2,3],dtype=torch.float64)\n</code></pre> <pre><code>tensor([2., 4., 6.], dtype=torch.float64)\n</code></pre> <p>You can add a scalar to the tensor. Use <code>u</code> as the sample tensor:</p> <pre><code># tensor + scalar\n\nu = torch.tensor([1, 2, 3, -1])\nv = u + 1\nprint (\"Addition Result: \", v)\n</code></pre> <pre><code>Addition Result:  tensor([2, 3, 4, 0])\n</code></pre> <p>The result is simply adding 1 to each element in tensor <code>u</code> as shown in the following image:</p> <p></p> Tensor Multiplication  <p>Now, you'll review the multiplication between a tensor and a scalar.</p> <p>Create a tensor with value <code>[1, 2]</code> and then multiply it by 2:</p> <pre><code># tensor * scalar\n\nu = torch.tensor([1, 2])\nv = 2 * u\nprint(\"The result of 2 * u: \", v)\n</code></pre> <pre><code>The result of 2 * u:  tensor([2, 4])\n</code></pre> <p>The result is <code>tensor([2, 4])</code>, so the code <code>2 * u</code> multiplies each element in the tensor by 2. This is how you get the product between a vector or matrix and a scalar in linear algebra.</p> <p>You can use multiplication between two tensors.</p> <p>Create two tensors <code>u</code> and <code>v</code> and then multiply them together:</p> <pre><code># tensor * tensor\n\nu = torch.tensor([1, 2])\nv = torch.tensor([3, 2])\nw = u * v\nprint (\"The result of u * v\", w)\n</code></pre> <pre><code>The result of u * v tensor([3, 4])\n</code></pre> <p>The result is simply <code>tensor([3, 4])</code>. This result is achieved by multiplying every element in <code>u</code> with the corresponding element in the same position <code>v</code>, which is similar to [1 * 3, 2 * 2].</p> Dot Product <p>The dot product is a special operation for a vector that you can use in Torch.</p> <p>Here is the dot product of the two tensors <code>u</code> and <code>v</code>:</p> <pre><code># Calculate dot product of u, v\n\nu = torch.tensor([1, 2])\nv = torch.tensor([3, 2])\n\nprint(\"Dot Product of u, v:\", torch.dot(u,v))\n</code></pre> <pre><code>Dot Product of u, v: tensor(7)\n</code></pre> <p>The result is <code>tensor(7)</code>. The function is 1 x 3 + 2 x 2 = 7.</p> Practice <p>Convert the list [-1, 1] and [1, 1] to tensors <code>u</code> and <code>v</code>. Then, plot the tensor <code>u</code> and <code>v</code> as a vector by using the function <code>plotVec</code> and find the dot product:</p> <pre><code># Practice: calculate the dot product of u and v, and plot out two vectors\nu = torch.tensor([-1, 1])\nv = torch.tensor([1, 1])\n\nplotVec([\n    {\"vector\": u.numpy(), \"name\": 'u', \"color\": 'r'},\n    {\"vector\": v.numpy(), \"name\": 'v', \"color\": 'b'},\n    {\"vector\": (u / v).numpy(), \"name\": w, \"color\": 'g'}\n    ])\n\n# Type your code here\n</code></pre> <p></p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_1Dtensors_v2/#about-the-author","title":"About the Author","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning. I also enjoy developing innovative algorithms and models that can drive insights and value. I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics. Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/","title":"Torch Tensors in 2D","text":"Two-Dimensional Tensors Objective <ul><li> How to perform tensor operations on 2D tensors.</li></ul> Table of Contents <p>In this lab, you will learn the basics of tensor operations on 2D tensors.</p> <ul> <li>Types and Shape </li> <li>Indexing and Slicing</li> <li>Tensor Operations</li> </ul> <p>Estimated Time Needed: 10 min</p> Preparation <p>The following are the libraries we are going to use for this lab.</p> <pre><code># These are the libraries will be used for this lab.\n\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport torch\nimport pandas as pd\n</code></pre>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/#Types_Shape","title":"Types and Shape","text":"<p>The methods and types for 2D tensors is similar to the methods and types for 1D tensors which has been introduced in Previous Lab.</p> <p>Let us see how to convert a 2D list to a 2D tensor. First, let us create a 3X3 2D tensor. Then let us try to use <code>torch.tensor()</code> which we used for converting a 1D list to 1D tensor. Is it going to work?</p> <pre><code># Convert 2D List to 2D Tensor\n\ntwoD_list = [[11, 12, 13], [21, 22, 23], [31, 32, 33]]\ntwoD_tensor = torch.tensor(twoD_list)\nprint(\"The New 2D Tensor: \", twoD_tensor)\n</code></pre> <p>Bravo! The method <code>torch.tensor()</code> works perfectly.Now, let us try other functions we studied in the Previous Lab.</p> <p>Let us try <code>tensor_obj.ndimension()</code> (<code>tensor_obj</code>: This can be any tensor object), <code>tensor_obj.shape</code>, and <code>tensor_obj.size()</code></p> <pre><code># Try tensor_obj.ndimension(), tensor_obj.shape, tensor_obj.size()\n\nprint(\"The dimension of twoD_tensor: \", twoD_tensor.ndimension())\nprint(\"The shape of twoD_tensor: \", twoD_tensor.shape)\nprint(\"The shape of twoD_tensor: \", twoD_tensor.size())\nprint(\"The number of elements in twoD_tensor: \", twoD_tensor.numel())\n</code></pre> <p>Because it is a 2D 3X3 tensor,  the outputs are correct.</p> <p>Now, let us try converting the tensor to a numpy array and convert the numpy array back to a tensor.</p> <pre><code># Convert tensor to numpy array; Convert numpy array to tensor\n\ntwoD_numpy = twoD_tensor.numpy()\nprint(\"Tensor -&gt; Numpy Array:\")\nprint(\"The numpy array after converting: \", twoD_numpy)\nprint(\"Type after converting: \", twoD_numpy.dtype)\n\nprint(\"================================================\")\n\nnew_twoD_tensor = torch.from_numpy(twoD_numpy)\nprint(\"Numpy Array -&gt; Tensor:\")\nprint(\"The tensor after converting:\", new_twoD_tensor)\nprint(\"Type after converting: \", new_twoD_tensor.dtype)\n</code></pre> <p>The result shows the tensor has successfully been converted to a numpy array and then converted back to a tensor.</p> <p>Now let us try to convert a Pandas Dataframe to a tensor. The process is the  Same as the 1D conversion, we can obtain the numpy array via the attribute <code>values</code>. Then, we can use <code>torch.from_numpy()</code> to convert the value of the Pandas Series to a tensor.</p> <pre><code># Try to convert the Panda Dataframe to tensor\n\ndf = pd.DataFrame({'a':[11,21,31],'b':[12,22,312]})\n\nprint(\"Pandas Dataframe to numpy: \", df.values)\nprint(\"Type BEFORE converting: \", df.values.dtype)\n\nprint(\"================================================\")\n\nnew_tensor = torch.from_numpy(df.values)\nprint(\"Tensor AFTER converting: \", new_tensor)\nprint(\"Type AFTER converting: \", new_tensor.dtype)\n</code></pre> Practice <p>Try to convert the following Pandas Dataframe  to a tensor</p> <pre><code># Practice: try to convert Pandas Series to tensor\n\ndf = pd.DataFrame({'A':[11, 33, 22],'B':[3, 3, 2]})\n</code></pre> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/#Index_Slice","title":"Indexing and Slicing","text":"<p>You can use rectangular brackets to access the different elements of the tensor. The correspondence between the rectangular brackets and the list and the rectangular representation is shown in the following figure for a 3X3 tensor:  </p> <p></p> <p>You can access the 2nd-row 3rd-column as shown in the following figure:</p> <p></p> <p>You simply use the square brackets and the indices corresponding to the element that you want.</p> <p>Now, let us try to access the value on position 2nd-row 3rd-column. Remember that the index is always 1 less than how we count rows and columns. There are two ways to access the certain value of a tensor. The example in code will be the same as the example picture above.</p> <pre><code># Use tensor_obj[row, column] and tensor_obj[row][column] to access certain position\n\ntensor_example = torch.tensor([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\nprint(\"What is the value on 2nd-row 3rd-column? \", tensor_example[1, 2])\nprint(\"What is the value on 2nd-row 3rd-column? \", tensor_example[1][2])\n</code></pre> <p>As we can see, both methods return the true value (the same value as the picture above). Therefore, both of the methods work.</p> <p>Consider the elements shown in the following figure: </p> <p></p> <p>Use the method above, we can access the 1st-row 1st-column by <code>tensor_example[0][0]</code></p> <pre><code>tensor_example[0][0]\n</code></pre> <p>But what if we want to get the value on both 1st-row 1st-column and 1st-row 2nd-column?</p> <p>You can also use slicing in a tensor. Consider the following figure. You want to obtain the 1st two columns in the 1st row:  </p> <p></p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/#let-us-see-how-we-use-slicing-with-2d-tensors-to-get-the-values-in-the-above-picture","title":"Let us see how  we use slicing with 2D tensors to get the values in the above picture.","text":"<pre><code># Use tensor_obj[begin_row_number: end_row_number, begin_column_number: end_column number] \n# and tensor_obj[row][begin_column_number: end_column number] to do the slicing\n\ntensor_example = torch.tensor([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\nprint(\"What is the value on 1st-row first two columns? \", tensor_example[0, 0:2])\nprint(\"What is the value on 1st-row first two columns? \", tensor_example[0][0:2])\n</code></pre> <p>We get the result as <code>tensor([11, 12])</code> successfully.</p> <p>But we can't combine using slicing on row and pick one column by using the code <code>tensor_obj[begin_row_number: end_row_number][begin_column_number: end_column number]</code>. The reason is that the slicing will be applied on the tensor first. The result type will be a two dimension again. The second bracket will no longer represent the index of the column it will be the index of the row at that time. Let us see an example. </p> <pre><code># Give an idea on tensor_obj[number: number][number]\n\ntensor_example = torch.tensor([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\nsliced_tensor_example = tensor_example[1:3]\nprint(\"1. Slicing step on tensor_example: \")\nprint(\"Result after tensor_example[1:3]: \", sliced_tensor_example)\nprint(\"Dimension after tensor_example[1:3]: \", sliced_tensor_example.ndimension())\nprint(\"================================================\")\nprint(\"2. Pick an index on sliced_tensor_example: \")\nprint(\"Result after sliced_tensor_example[1]: \", sliced_tensor_example[1])\nprint(\"Dimension after sliced_tensor_example[1]: \", sliced_tensor_example[1].ndimension())\nprint(\"================================================\")\nprint(\"3. Combine these step together:\")\nprint(\"Result: \", tensor_example[1:3][1])\nprint(\"Dimension: \", tensor_example[1:3][1].ndimension())\n</code></pre> <p>See the results and dimensions in 2 and 3 are the same. Both of them contains the 3rd row in the <code>tensor_example</code>, but not the last two values in the 3rd column.</p> <p>So how can we get the elements in the 3rd column with the last two rows? As the below picture.</p> <p></p> <p>Let's see the code below.</p> <pre><code># Use tensor_obj[begin_row_number: end_row_number, begin_column_number: end_column number] \n\ntensor_example = torch.tensor([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\nprint(\"What is the value on 3rd-column last two rows? \", tensor_example[1:3, 2])\n</code></pre> <p>Fortunately, the code <code>tensor_obj[begin_row_number: end_row_number, begin_column_number: end_column number]</code> is still works.</p> Practice <p>Try to change the values on the second column and the last two rows to 0. Basically, change the values on <code>tensor_ques[1][1]</code> and <code>tensor_ques[2][1]</code> to 0.</p> <pre><code># Practice: Use slice and index to change the values on the matrix tensor_ques.\n\ntensor_ques = torch.tensor([[11, 12, 13], [21, 22, 23], [31, 32, 33]])\n</code></pre> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.1_2%20Two-Dimensional%20Tensors_v2/#Tensor_Op","title":"Tensor OperationsWhat's on your mind? Put it in the comments!","text":"<p>We can also do some calculations on 2D tensors.</p> Tensor Addition <p>You can also add tensors; the process is identical to matrix addition. Matrix addition of X and Y is shown in the following figure:</p> <p></p> <p>Let us see how tensor addition works with <code>X</code> and <code>Y</code>.</p> <pre><code># Calculate [[1, 0], [0, 1]] + [[2, 1], [1, 2]]\n\nX = torch.tensor([[1, 0],[0, 1]]) \nY = torch.tensor([[2, 1],[1, 2]])\nX_plus_Y = X + Y\nprint(\"The result of X + Y: \", X_plus_Y)\n</code></pre> <p>Like the result shown in the picture above. The result is <code>[[3, 1], [1, 3]]</code></p>  Scalar Multiplication  <p>Multiplying a tensor by a scalar is identical to multiplying a matrix by a scaler. If you multiply the matrix Y by the scalar 2, you simply multiply every element in the matrix by 2 as shown in the figure:</p> <p></p> <p>Let us try to calculate the product of 2Y.</p> <pre><code># Calculate 2 * [[2, 1], [1, 2]]\n\nY = torch.tensor([[2, 1], [1, 2]]) \ntwo_Y = 2 * Y\nprint(\"The result of 2Y: \", two_Y)\n</code></pre> Element-wise Product/Hadamard Product <p>Multiplication of two tensors corresponds to an element-wise product or Hadamard product.  Consider matrix the X and Y with the same size. The Hadamard product corresponds to multiplying each of the elements at the same position, that is, multiplying elements with the same color together. The result is a new matrix that is the same size as matrix X and Y as shown in the following figure:</p> <p> </p> <p>The code below calculates the element-wise product of the tensor X and Y:</p> <pre><code># Calculate [[1, 0], [0, 1]] * [[2, 1], [1, 2]]\n\nX = torch.tensor([[1, 0], [0, 1]])\nY = torch.tensor([[2, 1], [1, 2]]) \nX_times_Y = X * Y\nprint(\"The result of X * Y: \", X_times_Y)\n</code></pre> <p>This is a simple calculation. The result from the code matches the result shown in the picture.</p> Matrix Multiplication  <p>We can also apply matrix multiplication to two tensors, if you have learned linear algebra, you should know that in the multiplication of two matrices order matters. This means if X * Y is valid, it does not mean Y * X is valid. The number of columns of the matrix on the left side of the multiplication sign must equal to the number of rows of the matrix on the right side.</p> <p>First, let us create a tensor <code>X</code> with size 2X3. Then, let us create another tensor <code>Y</code> with size 3X2. Since the number of columns of <code>X</code> is equal to the number of rows of <code>Y</code>. We are able to perform the multiplication.</p> <p>We use <code>torch.mm()</code> for calculating the multiplication between tensors with different sizes.</p> <pre><code># Calculate [[0, 1, 1], [1, 0, 1]] * [[1, 1], [1, 1], [-1, 1]]\n\nA = torch.tensor([[0, 1, 1], [1, 0, 1]])\nB = torch.tensor([[1, 1], [1, 1], [-1, 1]])\nA_times_B = torch.mm(A,B)\nprint(\"The result of A * B: \", A_times_B)\n</code></pre> Practice <p>Try to create your own two tensors (<code>X</code> and <code>Y</code>) with different sizes, and multiply them.</p> <pre><code># Practice: Calculate the product of two tensors (X and Y) with different sizes \n\n# Type your code here\n</code></pre>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.2derivativesandGraphsinPytorch_v2/","title":"Differentiation in PyTorch","text":"Objective <ul><li> How to perform differentiation in pytorch.</li></ul> Table of Contents <p>In this lab, you will learn the basics of differentiation.</p> <ul> <li>Derivatives</li> <li>Partial Derivatives</li> </ul> <p>Estimated Time Needed: 25 min</p>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> Preparation <p>The following are the libraries we are going to use for this lab.</p> <pre><code># These are the libraries will be useing for this lab.\n\nimport torch \nimport matplotlib.pylab as plt\n</code></pre>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.2derivativesandGraphsinPytorch_v2/#Derivative","title":"Derivatives","text":"<p>Let us create the tensor <code>x</code> and set the parameter <code>requires_grad</code> to true because you are going to take the derivative of the tensor.</p> <pre><code># Create a tensor x\n\nx = torch.tensor(2.0, requires_grad = True)\nprint(\"The tensor x: \", x)\n</code></pre> <pre><code>The tensor x:  tensor(2., requires_grad=True)\n</code></pre> <p>Then let us create a tensor according to the equation \\(y=x^2\\).</p> <pre><code># Create a tensor y according to y = x^2\n\ny = x ** 2\nprint(\"The result of y = x^2: \", y)\n</code></pre> <pre><code>The result of y = x^2:  tensor(4., grad_fn=&lt;PowBackward0&gt;)\n</code></pre> <p>Then let us take the derivative with respect x at x = 2</p> <pre><code># Take the derivative. Try to print out the derivative at the value x = 2\n\ny.backward()\nprint(\"The dervative at x = 2: \", x.grad)\n</code></pre> <pre><code>The dervative at x = 2:  tensor(4.)\n</code></pre> <p>The preceding lines perform the following operation: </p> <p>\\(\\frac{\\mathrm{dy(x)}}{\\mathrm{dx}}=2x\\)</p> <p>\\(\\frac{\\mathrm{dy(x=2)}}{\\mathrm{dx}}=2(2)=4\\)</p> <pre><code>print('data:',x.data)\nprint('grad_fn:',x.grad_fn)\nprint('grad:',x.grad)\nprint(\"is_leaf:\",x.is_leaf)\nprint(\"requires_grad:\",x.requires_grad)\n</code></pre> <pre><code>data: tensor(2.)\ngrad_fn: None\ngrad: tensor(4.)\nis_leaf: True\nrequires_grad: True\n</code></pre> <pre><code>print('data:',y.data)\nprint('grad_fn:',y.grad_fn)\nprint('grad:',y.grad)\nprint(\"is_leaf:\",y.is_leaf)\nprint(\"requires_grad:\",y.requires_grad)\n</code></pre> <pre><code>data: tensor(4.)\ngrad_fn: &lt;PowBackward0 object at 0x7873c29f8070&gt;\ngrad: None\nis_leaf: False\nrequires_grad: True\n\n\n/tmp/ipykernel_17856/1355624623.py:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n  print('grad:',y.grad)\n</code></pre> <p>Let us try to calculate the derivative for a more complicated function. </p> <pre><code># Calculate the y = x^2 + 2x + 1, then find the derivative \n\nx = torch.tensor(2.0, requires_grad = True)\ny = x ** 2 + 2 * x + 1\nprint(\"The result of y = x^2 + 2x + 1: \", y)\ny.backward()\nprint(\"The dervative at x = 2: \", x.grad)\n</code></pre> <pre><code>The result of y = x^2 + 2x + 1:  tensor(9., grad_fn=&lt;AddBackward0&gt;)\nThe dervative at x = 2:  tensor(6.)\n</code></pre> <p>The function is in the following form: \\(y=x^{2}+2x+1\\)</p> <p>The derivative is given by:</p> <p>\\(\\frac{\\mathrm{dy(x)}}{\\mathrm{dx}}=2x+2\\)</p> <p>\\(\\frac{\\mathrm{dy(x=2)}}{\\mathrm{dx}}=2(2)+2=6\\)</p> Practice <p>Determine the derivative of \\(y = 2x^3+x\\) at \\(x=1\\)</p> <pre><code># Practice: Calculate the derivative of y = 2x^3 + x at x = 1\n\n# Type your code here\n</code></pre> <p>Double-click here for the solution.</p> <p>We can implement our own custom autograd Functions by subclassing     torch.autograd.Function and implementing the forward and backward passes     which operate on Tensors</p> <pre><code>class SQ(torch.autograd.Function):\n\n\n    @staticmethod\n    def forward(ctx,i):\n        \"\"\"\n        In the forward pass we receive a Tensor containing the input and return\n        a Tensor containing the output. ctx is a context object that can be used\n        to stash information for backward computation. You can cache arbitrary\n        objects for use in the backward pass using the ctx.save_for_backward method.\n        \"\"\"\n        result=i**2\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"\n        In the backward pass we receive a Tensor containing the gradient of the loss\n        with respect to the output, and we need to compute the gradient of the loss\n        with respect to the input.\n        \"\"\"\n        i, = ctx.saved_tensors\n        grad_output = 2*i\n        return grad_output\n</code></pre> <p>We can apply it the function  </p> <pre><code>x=torch.tensor(2.0,requires_grad=True )\nsq=SQ.apply\n\ny=sq(x)\ny\nprint(y.grad_fn)\ny.backward()\nx.grad\n</code></pre> <pre><code>&lt;torch.autograd.function.SQBackward object at 0x7873c29c7df0&gt;\n\n\n\n\n\ntensor(4.)\n</code></pre>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.2derivativesandGraphsinPytorch_v2/#Partial_Derivative","title":"Partial DerivativesWhat's on your mind? Put it in the comments!","text":"<p>We can also calculate Partial Derivatives. Consider the function: \\(f(u,v)=vu+u^{2}\\)</p> <p>Let us create <code>u</code> tensor, <code>v</code> tensor and  <code>f</code> tensor</p> <pre><code># Calculate f(u, v) = v * u + u^2 at u = 1, v = 2\n\nu = torch.tensor(1.0,requires_grad=True)\nv = torch.tensor(2.0,requires_grad=True)\nf = u * v + u ** 2\nprint(\"The result of v * u + u^2: \", f)\n</code></pre> <pre><code>The result of v * u + u^2:  tensor(3., grad_fn=&lt;AddBackward0&gt;)\n</code></pre> <p>This is equivalent to the following: </p> <p>\\(f(u=1,v=2)=(2)(1)+1^{2}=3\\)</p> <p>Now let us take the derivative with respect to <code>u</code>:</p> <pre><code># Calculate the derivative with respect to u\n\nf.backward()\nprint(\"The partial derivative with respect to u: \", u.grad)\n</code></pre> <pre><code>The partial derivative with respect to u:  tensor(4.)\n</code></pre> <p>the expression is given by:</p> <p>\\(\\frac{\\mathrm{\\partial f(u,v)}}{\\partial {u}}=v+2u\\)</p> <p>\\(\\frac{\\mathrm{\\partial f(u=1,v=2)}}{\\partial {u}}=2+2(1)=4\\)</p> <p>Now, take the derivative with respect to <code>v</code>:</p> <pre><code># Calculate the derivative with respect to v\n\nprint(\"The partial derivative with respect to u: \", v.grad)\n</code></pre> <pre><code>The partial derivative with respect to u:  tensor(1.)\n</code></pre> <p>The equation is given by:</p> <p>\\(\\frac{\\mathrm{\\partial f(u,v)}}{\\partial {v}}=u\\)</p> <p>\\(\\frac{\\mathrm{\\partial f(u=1,v=2)}}{\\partial {v}}=1\\)</p> <p>Calculate the derivative with respect to a function with multiple values as follows. You use the sum trick to produce a scalar valued function and then take the gradient: </p> <pre><code># Calculate the derivative with multiple values\n\nx = torch.linspace(-10, 10, 10, requires_grad = True)\nY = x ** 2\ny = torch.sum(x ** 2)\n</code></pre> <p>We can plot the function  and its derivative </p> <pre><code># Take the derivative with respect to multiple value. Plot out the function and its derivative\n\ny.backward()\n\nplt.plot(x.detach().numpy(), Y.detach().numpy(), label = 'function')\nplt.plot(x.detach().numpy(), x.grad.detach().numpy(), label = 'derivative')\nplt.xlabel('x')\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>The orange line is the slope of the blue line at the intersection point, which is the derivative of the blue line.</p> <p>The  method <code> detach()</code>  excludes further tracking of operations in the graph, and therefore the subgraph will not record operations. This allows us to then convert the tensor to a numpy array. To understand the sum operation  Click Here</p> <p>The relu activation function is an essential function in neural networks. We can take the derivative as follows: </p> <pre><code># Take the derivative of Relu with respect to multiple value. Plot out the function and its derivative\n\nx = torch.linspace(-10, 10, 1000, requires_grad = True)\nY = torch.relu(x)\ny = Y.sum()\ny.backward()\nplt.plot(x.detach().numpy(), Y.detach().numpy(), label = 'function')\nplt.plot(x.detach().numpy(), x.grad.detach().numpy(), label = 'derivative')\nplt.xlabel('x')\nplt.legend()\nplt.show()\n</code></pre> <p></p> <pre><code>y.grad_fn\n</code></pre> <pre><code>&lt;SumBackward0 at 0x7873545dfc40&gt;\n</code></pre> Practice <p>Try to determine partial derivative  \\(u\\) of the following function where \\(u=2\\) and \\(v=1\\): $ f=uv+(uv)^2$</p> <pre><code># Practice: Calculate the derivative of f = u * v + (u * v) ** 2 at u = 2, v = 1\n\n# Type the code here\n</code></pre>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.1_simple_data_set_v2/","title":"Simple Dataset","text":"Objective <ul><li> How to create a dataset in pytorch.</li><li> How to perform transformations on the dataset.</li></ul> Table of Contents <p>In this lab, you will construct a basic dataset by using PyTorch and learn how to apply basic transformations to it.</p> <ul> <li>Simple dataset</li> <li>Transforms</li> <li>Compose</li> </ul> <p>Estimated Time Needed: 30 min</p> Preparation <p>The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.</p> <pre><code># These are the libraries will be used for this lab.\n\nimport torch\nfrom torch.utils.data import Dataset\ntorch.manual_seed(1)\n</code></pre> <pre><code>&lt;torch._C.Generator at 0x71849b542f70&gt;\n</code></pre>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.1_simple_data_set_v2/#simple-dataset","title":"Simple dataset","text":"<p>Let us try to create our own dataset class.</p> <pre><code># Define class for dataset\n\nclass toy_set(Dataset):\n\n    # Constructor with defult values \n    def __init__(self, length = 10, transform = None):\n        self.len = length\n        self.x = 2 * torch.ones(length, 2)\n        self.y = torch.ones(length, 1)\n        self.transform = transform\n\n    # Getter\n    def __getitem__(self, index):\n        sample = self.x[index], self.y[index]\n        if self.transform:\n            sample = self.transform(sample)     \n        return sample\n\n    # Get Length\n    def __len__(self):\n        return self.len\n</code></pre> <p>Now, let us create our <code>toy_set</code> object, and find out the value on index 1 and the length of the inital dataset</p> <pre><code># Create Dataset Object. Find out the value on index 1. Find out the length of Dataset Object.\n\nour_dataset = toy_set()\nprint(\"Our toy_set object: \", our_dataset)\nprint(\"Value on index 0 of our toy_set object: \", our_dataset[0])\nprint(\"Our toy_set length: \", len(our_dataset))\n</code></pre> <pre><code>Our toy_set object:  &lt;__main__.toy_set object at 0x7184a1b0ec60&gt;\nValue on index 0 of our toy_set object:  (tensor([2., 2.]), tensor([1.]))\nOur toy_set length:  10\n</code></pre> <p>As a result, we can apply the same indexing convention as a <code>list</code>, and apply the fuction <code>len</code> on the <code>toy_set</code> object. We are able to customize the indexing and length method by <code>def __getitem__(self, index)</code> and <code>def __len__(self)</code>.</p> <p>Now, let us print out the first 3 elements and assign them to x and y:</p> <pre><code># Use loop to print out first 3 elements in dataset\n\nfor i in range(3):\n    x, y=our_dataset[i]\n    print(\"index: \", i, '; x:', x, '; y:', y)\n</code></pre> <pre><code>index:  0 ; x: tensor([2., 2.]) ; y: tensor([1.])\nindex:  1 ; x: tensor([2., 2.]) ; y: tensor([1.])\nindex:  2 ; x: tensor([2., 2.]) ; y: tensor([1.])\n</code></pre> <p>The dataset object is an Iterable; as a result, we  apply the loop directly on the dataset object </p> <pre><code>for x,y in our_dataset:\n    print(' x:', x, 'y:', y)\n</code></pre> <pre><code> x: tensor([2., 2.]) y: tensor([1.])\n x: tensor([2., 2.]) y: tensor([1.])\n x: tensor([2., 2.]) y: tensor([1.])\n x: tensor([2., 2.]) y: tensor([1.])\n x: tensor([2., 2.]) y: tensor([1.])\n x: tensor([2., 2.]) y: tensor([1.])\n x: tensor([2., 2.]) y: tensor([1.])\n x: tensor([2., 2.]) y: tensor([1.])\n x: tensor([2., 2.]) y: tensor([1.])\n x: tensor([2., 2.]) y: tensor([1.])\n</code></pre>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.1_simple_data_set_v2/#an-existing-dataset","title":"An existing dataset","text":"<p>For purposes of learning, we will use a simple dataset from the <code>dataidea</code> package called music. It's made up of two features, <code>age</code> and <code>gender</code> and outcome variable as <code>genre</code></p> <pre><code>import dataidea\n\n# load dataset\nmusic_data = dataidea.loadDataset('music')\n\n# get features and target\nfeatures = music_data.drop('genre', axis=1)\ntarget = music_data['genre']\n\n# display sample\nmusic_data.sample(n=3)\n</code></pre> age gender genre 4 29 1 Jazz 5 30 1 Jazz 18 35 1 Classical <pre><code># lets encode the target\nfrom sklearn.preprocessing import LabelEncoder\n\ntarget = LabelEncoder().fit_transform(target)\nprint(f'Encoded Genre: {target}')\n</code></pre> <pre><code>Encoded Genre: [3 3 3 4 4 4 1 1 1 2 2 2 0 0 0 1 1 1 1 1]\n</code></pre> <p>We can create a custom Class for this dataset as demonstrated below</p> <pre><code>class MusicDataset(Dataset):\n\n    def __init__(self):\n        self.features = torch.tensor(features.values, dtype=torch.float32)\n        self.target = torch.from_numpy(target)\n\n    def __getitem__(self, index):\n        x = self.features[index]\n        y = self.target[index]\n        return x, y\n\n    def __len__(self):\n        return len(self.features)\n</code></pre> <p>Now let's create a MusicDataset object and get use the methods to access some data and info</p> <pre><code>music_torch_dataset = MusicDataset()\n\n# Row 0 data\nprint(f\"Row 0: {music_torch_dataset[0]}\")\n\n# display no of rows\nprint(f\"Number of rows: {len(music_torch_dataset)}\")\n</code></pre> <pre><code>Row 0: (tensor([20.,  1.]), tensor(3))\nNumber of rows: 20\n</code></pre> <p>Let's have a look at the first 5 rows</p> <pre><code>for sample in range(5):\n    print(f\"Row {sample}: {music_torch_dataset[0]}\")\n</code></pre> <pre><code>Row 0: (tensor([20.,  1.]), tensor(3))\nRow 1: (tensor([20.,  1.]), tensor(3))\nRow 2: (tensor([20.,  1.]), tensor(3))\nRow 3: (tensor([20.,  1.]), tensor(3))\nRow 4: (tensor([20.,  1.]), tensor(3))\n</code></pre>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.1_simple_data_set_v2/#practice","title":"Practice","text":"<p>Try to create an <code>toy_set</code> object with length 50. Print out the length of your object.</p> <pre><code># Practice: Create a new object with length 50, and print the length of object out.\n\n# Type your code here\n</code></pre> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.1_simple_data_set_v2/#transforms","title":"Transforms","text":"<p>You can also create a class for transforming the data. In this case, we will try to add 1 to x and multiply y by 2:</p> <pre><code># Create tranform class add_mult\n\nclass add_mult(object):\n\n    # Constructor\n    def __init__(self, addx = 1, muly = 2):\n        self.addx = addx\n        self.muly = muly\n\n    # Executor\n    def __call__(self, sample):\n        x = sample[0]\n        y = sample[1]\n        x = x + self.addx\n        y = y * self.muly\n        sample = x, y\n        return sample\n</code></pre> <p>Now, create a transform object:.</p> <pre><code># Create an add_mult transform object, and an toy_set object\n\na_m = add_mult()\ndata_set = toy_set()\n</code></pre> <p>Assign the outputs of the original dataset to <code>x</code> and <code>y</code>. Then, apply the transform <code>add_mult</code> to the dataset and output the values as <code>x_</code> and <code>y_</code>, respectively: </p> <pre><code># Use loop to print out first 10 elements in dataset\n\nfor i in range(10):\n    x, y = data_set[i]\n    print('Index: ', i, 'Original x: ', x, 'Original y: ', y)\n    x_, y_ = a_m(data_set[i])\n    print('Index: ', i, 'Transformed x_:', x_, 'Transformed y_:', y_)\n</code></pre> <p>As the result, <code>x</code> has been added by 1 and y has been multiplied by 2, as [2, 2] + 1 = [3, 3] and [1] x 2 = [2]</p> <p>We can apply the transform object every time we create a new <code>toy_set object</code>? Remember, we have the constructor in toy_set class with the parameter <code>transform = None</code>. When we create a new object using the constructor, we can assign the transform object to the parameter transform, as the following code demonstrates.</p> <pre><code># Create a new data_set object with add_mult object as transform\n\ncust_data_set = toy_set(transform = a_m)\n</code></pre> <p>This applied <code>a_m</code> object (a transform method) to every element in <code>cust_data_set</code> as initialized. Let us print out the first 10 elements in <code>cust_data_set</code> in order to see whether the <code>a_m</code> applied on <code>cust_data_set</code></p> <pre><code># Use loop to print out first 10 elements in dataset\n\nfor i in range(10):\n    x, y = data_set[i]\n    print('Index: ', i, 'Original x: ', x, 'Original y: ', y)\n    x_, y_ = cust_data_set[i]\n    print('Index: ', i, 'Transformed x_:', x_, 'Transformed y_:', y_)\n</code></pre> <p>The result is the same as the previous method.</p> <pre><code># Practice: Construct your own my_add_mult transform. Apply my_add_mult on a new toy_set object. Print out the first three elements from the transformed dataset.\n\n# Type your code here.\n</code></pre> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.1_simple_data_set_v2/#compose","title":"Compose","text":"<p>You can compose multiple transforms on the dataset object. First, import <code>transforms</code> from <code>torchvision</code>:</p> <pre><code># Run the command below when you do not have torchvision installed\n# !mamba install -y torchvision\n\nfrom torchvision import transforms\n</code></pre> <p>Then, create a new transform class that multiplies each of the elements by 100: </p> <pre><code># Create tranform class mult\n\nclass mult(object):\n\n    # Constructor\n    def __init__(self, mult = 100):\n        self.mult = mult\n\n    # Executor\n    def __call__(self, sample):\n        x = sample[0]\n        y = sample[1]\n        x = x * self.mult\n        y = y * self.mult\n        sample = x, y\n        return sample\n</code></pre> <p>Now let us try to combine the transforms <code>add_mult</code> and <code>mult</code></p> <pre><code># Combine the add_mult() and mult()\n\ndata_transform = transforms.Compose([add_mult(), mult()])\nprint(\"The combination of transforms (Compose): \", data_transform)\n</code></pre> <p>The new <code>Compose</code> object will perform each transform concurrently as shown in this figure:</p> <p></p> <pre><code>data_transform(data_set[0])\n</code></pre> <pre><code>x,y=data_set[0]\nx_,y_=data_transform(data_set[0])\nprint( 'Original x: ', x, 'Original y: ', y)\n\nprint( 'Transformed x_:', x_, 'Transformed y_:', y_)\n</code></pre> <p>Now we can pass the new <code>Compose</code> object (The combination of methods <code>add_mult()</code> and <code>mult</code>) to the constructor for creating <code>toy_set</code> object.</p> <pre><code># Create a new toy_set object with compose object as transform\n\ncompose_data_set = toy_set(transform = data_transform)\n</code></pre> <p>Let us print out the first 3 elements in different <code>toy_set</code> datasets in order to compare the output after different transforms have been applied: </p> <pre><code># Use loop to print out first 3 elements in dataset\n\nfor i in range(3):\n    x, y = data_set[i]\n    print('Index: ', i, 'Original x: ', x, 'Original y: ', y)\n    x_, y_ = cust_data_set[i]\n    print('Index: ', i, 'Transformed x_:', x_, 'Transformed y_:', y_)\n    x_co, y_co = compose_data_set[i]\n    print('Index: ', i, 'Compose Transformed x_co: ', x_co ,'Compose Transformed y_co: ',y_co)\n</code></pre> <p>Let us see what happened on index 0. The original value of <code>x</code> is [2, 2], and the original value of <code>y</code> is [1]. If we only applied <code>add_mult()</code> on the original dataset, then the <code>x</code> became [3, 3] and y became [2]. Now let us see what is the value after applied both <code>add_mult()</code> and <code>mult()</code>. The result of x is [300, 300] and y is [200]. The calculation which is equavalent to the compose is  x = ([2, 2] + 1) x 100 = [300, 300], y = ([1] x 2) x 100 = 200</p> Practice <p>Try to combine the <code>mult()</code> and <code>add_mult()</code> as <code>mult()</code> to be executed first. And apply this on a new <code>toy_set</code> dataset. Print out the first 3 elements in the transformed dataset.</p> <pre><code># Practice: Make a compose as mult() execute first and then add_mult(). Apply the compose on toy_set dataset. Print out the first 3 elements in the transformed dataset.\n\n# Type your code here.\n</code></pre>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.3_pre-Built%20Datasets_and_transforms_v2/","title":"Prebuilt Datasets and Transforms","text":"Prebuilt Datasets and Transforms Objective <ul><li> How to use MNIST prebuilt dataset in pytorch.</li></ul> Table of Contents <p>In this lab, you will use a prebuilt dataset and then use some prebuilt dataset transforms.</p> <ul> <li>Prebuilt Datasets</li> <li>Torchvision Transforms</li> </ul> <p>Estimated Time Needed: 10 min</p> Preparation <p>The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.</p> <pre><code># These are the libraries will be used for this lab.\n\n# !pip install torchvision==0.9.1 torch==1.8.1 \nimport torch \nimport matplotlib.pylab as plt\nimport numpy as np\ntorch.manual_seed(0)\n</code></pre> <p>This is the function for displaying images.</p> <pre><code># Show data by diagram\n\ndef show_data(data_sample, shape = (28, 28)):\n    plt.imshow(data_sample[0].numpy().reshape(shape), cmap='gray')\n    plt.title('y = ' + str(data_sample[1]))\n</code></pre>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.3_pre-Built%20Datasets_and_transforms_v2/#Prebuilt_Dataset","title":"Prebuilt Datasets","text":"<p>You will focus on the following libraries: </p> <pre><code># Run the command below when you do not have torchvision installed\n# !mamba install -y torchvision\n\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n</code></pre> <p>We can import a prebuilt dataset. In this case, use MNIST. You'll work with several of these parameters later by placing a transform object in the argument <code>transform</code>.</p> <pre><code># Import the prebuilt dataset into variable dataset\n\n\ndataset = dsets.MNIST(\n    root = './data',  \n    download = True, \n    transform = transforms.ToTensor()\n)\n</code></pre> <p>Each element of the dataset object contains a tuple. Let us see whether the first element in the dataset is a tuple and what is in it.</p> <pre><code># Examine whether the elements in dataset MNIST are tuples, and what is in the tuple?\n\nprint(\"Type of the first element: \", type(dataset[0]))\nprint(\"The length of the tuple: \", len(dataset[0]))\nprint(\"The shape of the first element in the tuple: \", dataset[0][0].shape)\nprint(\"The type of the first element in the tuple\", type(dataset[0][0]))\nprint(\"The second element in the tuple: \", dataset[0][1])\nprint(\"The type of the second element in the tuple: \", type(dataset[0][1]))\nprint(\"As the result, the structure of the first element in the dataset is (tensor([1, 28, 28]), tensor(7)).\")\n</code></pre> <p>As shown in the output, the first element in the tuple is a cuboid tensor. As you can see, there is a dimension with only size 1, so basically, it is a rectangular tensor. The second element in the tuple is a number tensor, which indicate the real number the image shows. As the second element in the tuple is <code>tensor(7)</code>, the image should show a hand-written 7.</p> <p>Let us plot the first element in the dataset:</p> <pre><code># Plot the first element in the dataset\n\nshow_data(dataset[0])\n</code></pre> <p></p> <p>As we can see, it is a 7.</p> <p>Plot the second sample:   </p> <pre><code># Plot the second element in the dataset\n\nshow_data(dataset[1])\n</code></pre> <p></p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/1.3.3_pre-Built%20Datasets_and_transforms_v2/#Torchvision","title":"Torchvision Transforms","text":"<p>We can apply some image transform functions on the MNIST dataset.</p> <p>As an example, the images in the MNIST dataset can be cropped and converted to a tensor. We can use <code>transform.Compose</code> we learned from the previous lab to combine the two transform functions.</p> <pre><code># Combine two transforms: crop and convert to tensor. Apply the compose to MNIST dataset\n\ncroptensor_data_transform = transforms.Compose([transforms.CenterCrop(20), transforms.ToTensor()])\ndataset = dsets.MNIST(root = './data', download = True, transform = croptensor_data_transform)\nprint(\"The shape of the first element in the first tuple: \", dataset[0][0].shape)\n</code></pre> <pre><code>The shape of the first element in the first tuple:  torch.Size([1, 20, 20])\n</code></pre> <p>We can see the image is now 20 x 20 instead of 28 x 28.</p> <p>Let us plot the first image again. Notice that the black space around the 7 become less apparent.</p> <pre><code># Plot the first element in the dataset\n\nshow_data(dataset[0],shape = (20, 20))\n</code></pre> <p></p> <pre><code># Plot the second element in the dataset\n\nshow_data(dataset[1],shape = (20, 20))\n</code></pre> <p></p> <p>In the below example, we horizontally flip the image, and then convert it to a tensor. Use <code>transforms.Compose()</code> to combine these two transform functions. Plot the flipped image.</p> <pre><code># Construct the compose. Apply it on MNIST dataset. Plot the image out.\n\nfliptensor_data_transform = transforms.Compose([transforms.RandomHorizontalFlip(p = 1),transforms.ToTensor()])\ndataset = dsets.MNIST(root = './data', download = True, transform = fliptensor_data_transform)\n# show_data(dataset[1])\n</code></pre> Practice <p>Try to use the <code>RandomVerticalFlip</code> (vertically flip the image) with horizontally flip and convert to tensor as a compose. Apply the compose on image. Use <code>show_data()</code> to plot the second image (the image as 2).</p> <pre><code># Practice: Combine vertical flip, horizontal flip and convert to tensor as a compose. Apply the compose on image. Then plot the image\nrandom_vertical_flip = transforms.Compose([transforms.RandomVerticalFlip(), transforms.ToTensor()])\ndataset = dsets.MNIST(root= \".data\", download=True, transform=fliptensor_data_transform)\nshow_data(dataset[0])\n# Type your code here\n</code></pre> <p></p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/tensors_and_datasets_exercise/","title":"Tensors & Datasets Exercise","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/tensors_and_datasets_exercise/#week-1-tensors-and-datasets-5-questions","title":"Week 1: Tensors and Datasets (5 Questions)","text":""},{"location":"Deep%20Learning/Week1-Tensors-Datasets/tensors_and_datasets_exercise/#1-1-dimension-tensors","title":"1. 1 Dimension Tensors:","text":"<ul> <li>Exercise: Create a 1D tensor with 10 elements ranging from 0 to 9. Perform the following operations: find the mean, sum, and standard deviation of the tensor. Print the results.</li> </ul>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/tensors_and_datasets_exercise/#2-two-dimension-tensors","title":"2. Two Dimension Tensors:","text":"<ul> <li>Exercise: Generate a 2D tensor with shape (3, 4) filled with random numbers. Perform matrix multiplication with another 2D tensor of shape (4, 2) filled with ones. Print the resulting tensor and its shape.</li> </ul>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/tensors_and_datasets_exercise/#3-derivatives-and-graphs-in-pytorch","title":"3. Derivatives and Graphs in PyTorch:","text":"<ul> <li>Exercise: Define a simple function \\(f(x) = x^2\\) in PyTorch and compute its derivative at \\(x = 3\\). Use PyTorch\u2019s autograd to compute the gradient.</li> </ul>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/tensors_and_datasets_exercise/#4-simple-dataset","title":"4. Simple Dataset:","text":"<ul> <li>Exercise: Create a custom dataset of 100 samples with 1 feature and a corresponding label using PyTorch\u2019s <code>Dataset</code> class. Implement the <code>__getitem__</code> and <code>__len__</code> methods. Print the first 5 samples.</li> </ul>"},{"location":"Deep%20Learning/Week1-Tensors-Datasets/tensors_and_datasets_exercise/#5-pre-built-datasets","title":"5. Pre Built Datasets:","text":"<ul> <li>Exercise: Load the MNIST dataset using PyTorch\u2019s <code>torchvision.datasets</code>. Display the first image in the dataset along with its label.</li> </ul> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.1Prediction1Dregression_v3/","title":"Linear Regression 1D, Prediction","text":"Objective <ul><li> How to make the prediction for multiple inputs.</li><li> How to use linear class to build more complex models.</li><li> How to build a custom module.</li></ul> Table of Contents <p>In this lab, we will  review how to make a prediction in several different ways by using PyTorch. <ul> <li>Prediction</li> <li>Class Linear</li> <li>Build Custom Modules</li> </ul> <p>Estimated Time Needed: 15 min</p>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> Preparation   The following are the libraries we are going to use for this lab.    <pre><code># These are the libraries will be used for this lab.\n\nimport torch\n</code></pre>"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.1Prediction1Dregression_v3/#Prediction","title":"Prediction","text":"Let us create the following expressions:   $b=-1,w=2$  $\\hat{y}=-1+2x$   First, define the parameters:    <pre><code># Define w = 2 and b = -1 for y = wx + b\n\nw = torch.tensor(2.0, requires_grad = True)\nb = torch.tensor(-1.0, requires_grad = True)\n</code></pre>  Then, define the function <code>forward(x, w, b)</code> makes the prediction:     <pre><code># Function forward(x) for prediction\n\ndef forward(x):\n    yhat = w * x + b\n    return yhat\n</code></pre>  Let's make the following prediction at x = 1   $\\hat{y}=-1+2x$  $\\hat{y}=-1+2(1)$    <pre><code>m = torch.tensor([2])\nprint(m)\n\nforward(m)\n</code></pre>      tensor([2])          tensor([3.], grad_fn=)     <pre><code># Predict y = 2x - 1 at x = 1\n\nx = torch.tensor([[1.0]])\nyhat = forward(x)\nprint(\"The prediction: \", yhat)\n</code></pre>      The prediction:  tensor([[1.]], grad_fn=)      Now, let us try to make the prediction for multiple inputs:      Let us construct the <code>x</code> tensor first. Check the shape of <code>x</code>.    <pre><code># Create x Tensor and check the shape of x tensor\n\nx = torch.tensor([[1.0], [2.0]])\nprint(\"The shape of x: \", x.shape)\n</code></pre>      The shape of x:  torch.Size([2, 1])   Now make the prediction:     <pre><code># Make the prediction of y = 2x - 1 at x = [1, 2]\n\nyhat = forward(x)\nprint(\"The prediction: \", yhat)\n</code></pre>      The prediction:  tensor([[1.],             [3.]], grad_fn=)   The result is the same as what it is in the image above.    Practice   Make a prediction of the following <code>x</code> tensor using the <code>w</code> and <code>b</code> from above.    <pre><code># Practice: Make a prediction of y = 2x - 1 at x = [[1.0], [2.0], [3.0]]\n\nx = torch.tensor([[1.0], [2.0], [3.0]])\nyhat = forward(x)\nprint(\"The prediction: \", yhat)\n</code></pre>      The prediction:  tensor([[1.],             [3.],             [5.]], grad_fn=)   Double-click here for the solution."},{"location":"Deep%20Learning/Week2-Linear-Regression/2.1Prediction1Dregression_v3/#Linear","title":"Class Linear","text":"The linear class can be used to make a prediction. We can also use the linear class to build more complex models. Let's import the module:    <pre><code># Import Class Linear\n\nfrom torch.nn import Linear\n</code></pre>  Set the random seed because the parameters are randomly initialized:    <pre><code># Set random seed\n\ntorch.manual_seed(1)\n</code></pre>    Let us create the linear object by using the constructor. The parameters are randomly created. Let us print out to see what w and b. The parameters of an <code>torch.nn.Module</code> model are contained in the model\u2019s parameters accessed with <code>lr.parameters()</code>:    <pre><code># Create Linear Regression Model, and print out the parameters\n\nlr = Linear(in_features=1, out_features=1, bias=True)\nprint(\"Parameters w and b: \", list(lr.parameters()))\n</code></pre>      Parameters w and b:  [Parameter containing:     tensor([[0.2772]], requires_grad=True), Parameter containing:     tensor([-0.3058], requires_grad=True)]   This is equivalent to the following expression:     $b=-0.44, w=0.5153$  $\\hat{y}=-0.44+0.5153x$   A method  <code>state_dict()</code> Returns a Python dictionary object corresponding to the layers of each parameter  tensor.     <pre><code>print(\"Python dictionary: \",lr.state_dict())\nprint(\"keys: \",lr.state_dict().keys())\nprint(\"values: \",lr.state_dict().values())\n</code></pre>      Python dictionary:  OrderedDict({'weight': tensor([[0.3652]]), 'bias': tensor([-0.3897])})     keys:  odict_keys(['weight', 'bias'])     values:  odict_values([tensor([[0.3652]]), tensor([-0.3897])])   The keys correspond to the name of the attributes and the values correspond to the parameter value.    <pre><code>print(\"weight:\",lr.weight)\nprint(\"bias:\",lr.bias)\n</code></pre>      weight: Parameter containing:     tensor([[0.3652]], requires_grad=True)     bias: Parameter containing:     tensor([-0.3897], requires_grad=True)   Now let us make a single prediction at x = [[1.0]].    <pre><code># Make the prediction at x = [[1.0]]\n\nx = torch.tensor([[1.0]])\nyhat = lr(x)\nprint(\"The prediction: \", yhat)\n</code></pre>      The prediction:  tensor([[-0.0245]], grad_fn=)      Similarly, you can make multiple predictions:      Use model <code>lr(x)</code> to predict the result.    <pre><code># Create the prediction using linear model\n\nx = torch.tensor([[1.0], [2.0]])\nyhat = lr(x)\nprint(\"The prediction: \", yhat)\n</code></pre>      The prediction:  tensor([[-0.0286],             [ 0.2487]], grad_fn=)    Practice   Make a prediction of the following <code>x</code> tensor using the linear regression model <code>lr</code>.    <pre><code># Practice: Use the linear regression model object lr to make the prediction.\n\nx = torch.tensor([[1.0],[2.0],[3.0]])\nyhat = lr(x)\n\nprint(\"The prediction: \", yhat)\n</code></pre>      The prediction:  tensor([[-0.0286],             [ 0.2487],             [ 0.5259]], grad_fn=)   Double-click here for the solution."},{"location":"Deep%20Learning/Week2-Linear-Regression/2.1Prediction1Dregression_v3/#Cust","title":"Build Custom ModulesWhat's on your mind? Put it in the comments!","text":"Now, let's build a custom module. We can make more complex models by using this method later on.    First, import the following library.    <pre><code># Library for this section\n\nfrom torch import nn\n</code></pre>  Now, let us define the class:     <pre><code># Customize Linear Regression Class\n\nclass LR(nn.Module):\n\n    # Constructor\n    def __init__(self, input_size, output_size):\n\n        # Inherit from parent\n        super(LR, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n\n    # Prediction function\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n</code></pre>  Create an object by using the constructor. Print out the parameters we get and the model.    <pre><code># Create the linear regression model. Print out the parameters.\n\nlr = LR(1, 1)\nprint(\"The parameters: \", list(lr.parameters()))\nprint(\"Linear model: \", lr.linear)\n</code></pre>      The parameters:  [Parameter containing:     tensor([[-0.0729]], requires_grad=True), Parameter containing:     tensor([-0.0900], requires_grad=True)]     Linear model:  Linear(in_features=1, out_features=1, bias=True)      Let us try to make a prediction of a single input sample.    <pre><code># Try our customize linear regression model with single input\n\nx = torch.tensor([[1.0]])\nyhat = lr(x)\nprint(\"The prediction: \", yhat)\n</code></pre>      The prediction:  tensor([[-0.1629]], grad_fn=)      Now, let us try another example with multiple samples.    <pre><code># Try our customize linear regression model with multiple input\n\nx = torch.tensor([[1.0], [2.0]])\nyhat = lr(x)\nprint(\"The prediction: \", yhat)\n</code></pre>      The prediction:  tensor([[-0.1629],             [-0.2358]], grad_fn=)   the parameters are also stored in an ordered dictionary :    <pre><code>print(\"Python dictionary: \", lr.state_dict())\nprint(\"keys: \",lr.state_dict().keys())\nprint(\"values: \",lr.state_dict().values())\n</code></pre>      Python dictionary:  OrderedDict([('weight', tensor([[0.5153]])), ('bias', tensor([-0.4414]))])     keys:  odict_keys(['weight', 'bias'])     values:  odict_values([tensor([[0.5153]]), tensor([-0.4414])])    Practice   Create an object <code>lr1</code> from the class we created before and make a prediction by using the following tensor:     <pre><code># Practice: Use the LR class to create a model and make a prediction of the following tensor.\n\nx = torch.tensor([[1.0], [2.0], [3.0]])\nlr1 = LR(1, 1)\nyhat = lr(x)\nyhat\n# print(\"The prediction: \", yhat)\n</code></pre>         tensor([[-0.1629],             [-0.2358],             [-0.3088]], grad_fn=)"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.2_linear_regression_one_parameter_v3/","title":"Linear Regression 1D, Training One Parameter","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/2.2_linear_regression_one_parameter_v3/#linear-regression-1d-training-one-parameter","title":"Linear Regression 1D: Training One ParameterObjectiveTable of ContentsPreparation","text":"<ul><li> How to create cost or criterion function using MSE (Mean Square Error).</li></ul> <p>In this lab, you will train a model with PyTorch by using data that you created. The model only has one parameter: the slope.</p> <ul> <li>Make Some Data</li> <li>Create the Model and Cost Function (Total Loss)</li> <li>Train the Model</li> </ul> <p>Estimated Time Needed: 20 min</p>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>The following are the libraries we are going to use for this lab.</p> <pre><code># These are the libraries will be used for this lab.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n</code></pre> <p>The class <code>plot_diagram</code> helps us to visualize the data space and the parameter space during training and has nothing to do with PyTorch.</p> <pre><code># The class for plotting\n\nclass plot_diagram():\n\n    # Constructor\n    def __init__(self, X, Y, w, stop, go = False):\n        start = w.data\n        self.error = []\n        self.parameter = []\n        self.X = X.numpy()\n        self.Y = Y.numpy()\n        self.parameter_values = torch.arange(start, stop)\n        self.Loss_function = [criterion(forward(X), Y) for w.data in self.parameter_values] \n        w.data = start\n\n    # Executor\n    def __call__(self, Yhat, w, error, n):\n        self.error.append(error)\n        self.parameter.append(w.data)\n        plt.subplot(212)\n        plt.plot(self.X, Yhat.detach().numpy())\n        plt.plot(self.X, self.Y,'ro')\n        plt.xlabel(\"A\")\n        plt.ylim(-20, 20)\n        plt.subplot(211)\n        plt.title(\"Data Space (top) Estimated Line (bottom) Iteration \" + str(n))\n        plt.plot(self.parameter_values.numpy(), self.Loss_function)   \n        plt.plot(self.parameter, self.error, 'ro')\n        plt.xlabel(\"B\")\n        plt.figure()\n\n    # Destructor\n    def __del__(self):\n        plt.close('all')\n</code></pre>"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.2_linear_regression_one_parameter_v3/#Makeup_Data","title":"Make Some Data","text":"<p>Import PyTorch library:</p> <pre><code># Import the library PyTorch\n\nimport torch\n</code></pre> <p>Generate values from -3 to 3 that create a line with a slope of -3. This is the line you will estimate.</p> <pre><code># Create the f(X) with a slope of -3\n\nX = torch.arange(-3, 3, 0.1).view(-1, 1)\nf = -3 * X\n</code></pre> <p>Let us plot the line.</p> <pre><code># Plot the line with blue\n\nplt.plot(X.numpy(), f.numpy(), label = 'f')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Let us add some noise to the data in order to simulate the real data. Use <code>torch.randn(X.size())</code> to generate Gaussian noise that is the same size as <code>X</code> and has a standard deviation opf 0.1.</p> <pre><code># Add some noise to f(X) and save it in Y\n\nY = f + 0.1 * torch.randn(X.size())\n</code></pre> <p>Plot the <code>Y</code>:</p> <pre><code># Plot the data points\n\nplt.plot(X.numpy(), Y.numpy(), 'rx', label = 'Y')\n\nplt.plot(X.numpy(), f.numpy(), label = 'f')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n</code></pre> <p></p>"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.2_linear_regression_one_parameter_v3/#Model_Cost","title":"Create the Model and Cost Function (Total Loss)","text":"<p>In this section, let us create the model and the cost function (total loss) we are going to use to train the model and evaluate the result.</p> <p>First, define the <code>forward</code> function \\(y=w*x\\). (We will add the bias in the next lab.)</p> <pre><code># Create forward function for prediction\n\ndef forward(x):\n    return w * x\n</code></pre> <p>Define the cost or criterion function using MSE (Mean Square Error):</p> <pre><code># Create the MSE function for evaluate the result.\n\ndef criterion(yhat, y):\n    return torch.mean((yhat - y) ** 2)\n</code></pre> <p>Define the learning rate <code>lr</code> and an empty list <code>LOSS</code> to record the loss for each iteration:</p> <pre><code># Create Learning Rate and an empty list to record the loss for each iteration\n\nlr = 0.1\nLOSS = []\n</code></pre> <p>Now, we create a model parameter by setting the argument <code>requires_grad</code> to <code> True</code> because the system must learn it.</p> <pre><code>w = torch.tensor(-10.0, requires_grad = True)\n</code></pre> <p>Create a <code>plot_diagram</code> object to visualize the data space and the parameter space for each iteration during training:</p> <pre><code>gradient_plot = plot_diagram(X, Y, w, stop = 5)\n</code></pre>"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.2_linear_regression_one_parameter_v3/#Train","title":"Train the Model","text":"<p>Let us define a function for training the model. The steps will be described in the comments.</p> <pre><code># Define a function for train the model\n\ndef train_model(iter):\n    for epoch in range (iter):\n\n        # make the prediction as we learned in the last lab\n        Yhat = forward(X)\n\n        # calculate the iteration\n        loss = criterion(Yhat,Y)\n\n        # plot the diagram for us to have a better idea\n        # gradient_plot(Yhat, w, loss.item(), epoch)\n\n        # store the loss into list\n        LOSS.append(loss.item())\n\n        # backward pass: compute gradient of the loss with respect to all the learnable parameters\n        loss.backward()\n\n        # updata parameters\n        w.data = w.data - lr * w.grad.data\n\n        # zero the gradients before running the backward pass\n        w.grad.data.zero_()\n</code></pre> <p>Let us try to run 4 iterations of gradient descent:</p> <pre><code># Give 4 iterations for training the model here.\n\ntrain_model(4)\n</code></pre> <p>Plot the cost for each iteration:</p> <pre><code># Plot the loss for each iteration\n\nplt.plot(LOSS)\nplt.tight_layout()\nplt.xlabel(\"Epoch/Iterations\")\nplt.ylabel(\"Cost\")\n</code></pre> <pre><code>Text(38.347222222222214, 0.5, 'Cost')\n</code></pre> <p></p>"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.2_linear_regression_one_parameter_v3/#about-the-author","title":"About the Author:What's on your mind? Put it in the comments!","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning. I also enjoy developing innovative algorithms and models that can drive insights and value. I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org. Besides these //technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.3_training_slope_and_bias_v3/","title":"Linear regression 1D, Training Two Parameters","text":"Objective <ul><li> How to train the model and visualize the loss results.</li></ul> <pre><code>#| hide\n#| default_exp models\n</code></pre> Table of Contents <p>In this lab, you will train a model with PyTorch by using the data that we created. The model will have the slope and bias. And we will review how to make a prediction in several different ways by using PyTorch.</p> <ul> <li>Make Some Data</li> <li>Create the Model and Cost Function (Total Loss) </li> <li>Train the Model </li> </ul> <p>Estimated Time Needed: 20 min  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> Preparation   We'll need the following libraries:      <pre><code># These are the libraries we are going to use in the lab.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d\n</code></pre>  The class <code>plot_error_surfaces</code> is just to help you visualize the data space and the parameter space during training and has nothing to do with PyTorch.     <pre><code>#| hide\n#| export\n# The class for plot the diagram\n\nclass plot_error_surfaces(object):\n\n    # Constructor\n    def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True):\n        W = np.linspace(-w_range, w_range, n_samples)\n        B = np.linspace(-b_range, b_range, n_samples)\n        w, b = np.meshgrid(W, B)    \n        Z = np.zeros((30,30))\n        count1 = 0\n        self.y = Y.numpy()\n        self.x = X.numpy()\n        for w1, b1 in zip(w, b):\n            count2 = 0\n            for w2, b2 in zip(w1, b1):\n                Z[count1, count2] = np.mean((self.y - w2 * self.x + b2) ** 2)\n                count2 += 1\n            count1 += 1\n        self.Z = Z\n        self.w = w\n        self.b = b\n        self.W = []\n        self.B = []\n        self.LOSS = []\n        self.n = 0\n        if go == True:\n            plt.figure()\n            plt.figure(figsize = (7.5, 5))\n            plt.axes(projection='3d').plot_surface(self.w, self.b, self.Z, rstride = 1, cstride = 1,cmap = 'viridis', edgecolor = 'none')\n            plt.title('Cost/Total Loss Surface')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.show()\n            plt.figure()\n            plt.title('Cost/Total Loss Surface Contour')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.contour(self.w, self.b, self.Z)\n            plt.show()\n\n    # Setter\n    def set_para_loss(self, W, B, loss):\n        self.n = self.n + 1\n        self.W.append(W)\n        self.B.append(B)\n        self.LOSS.append(loss)\n\n    # Plot diagram\n    def final_plot(self): \n        ax = plt.axes(projection = '3d')\n        ax.plot_wireframe(self.w, self.b, self.Z)\n        ax.scatter(self.W,self.B, self.LOSS, c = 'r', marker = 'x', s = 200, alpha = 1)\n        plt.figure()\n        plt.contour(self.w,self.b, self.Z)\n        plt.scatter(self.W, self.B, c = 'r', marker = 'x')\n        plt.xlabel('w')\n        plt.ylabel('b')\n        plt.show()\n\n    # Plot diagram\n    def plot_ps(self):\n        plt.subplot(121)\n        plt.ylim\n        plt.plot(self.x, self.y, 'ro', label=\"training points\")\n        plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label = \"estimated line\")\n        plt.xlabel('x')\n        plt.ylabel('y')\n        plt.ylim((-10, 15))\n        plt.title('Data Space Iteration: ' + str(self.n))\n\n        plt.subplot(122)\n        plt.contour(self.w, self.b, self.Z)\n        plt.scatter(self.W, self.B, c = 'r', marker = 'x')\n        plt.title('Total Loss Surface Contour Iteration' + str(self.n))\n        plt.xlabel('w')\n        plt.ylabel('b')\n        plt.show()\n</code></pre>"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.3_training_slope_and_bias_v3/#Makeup_Data","title":"Make Some Data","text":"Import PyTorch:     <pre><code># Import PyTorch library\n\nimport torch\n</code></pre>  Start with generating values from -3 to 3 that create a line with a slope of 1 and a bias of -1. This is the line that you need to estimate.    <pre><code># Create f(X) with a slope of 1 and a bias of -1\n\nX = torch.arange(-3, 3, 0.1).view(-1, 1)\nf = 1 * X - 1\n</code></pre>  Now, add some noise to the data:    <pre><code># Add noise\n\nY = f + 0.1 * torch.randn(X.size())\n</code></pre>  Plot the line and <code>Y</code> with noise:    <pre><code># Plot out the line and the points with noise\n\nplt.plot(X.numpy(), Y.numpy(), 'rx', label = 'y')\nplt.plot(X.numpy(), f.numpy(), label = 'f')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\n</code></pre>       ![png](output_20_1.png)"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.3_training_slope_and_bias_v3/#Model_Cost","title":"Create the Model and Cost Function (Total Loss)","text":"Define the <code>forward</code> function:     <pre><code># Define the forward function\n\ndef forward(x):\n    return w * x + b\n</code></pre>  Define the cost or criterion function (MSE):     <pre><code># Define the MSE Loss function\n\ndef criterion(yhat,y):\n    return torch.mean((yhat-y)**2)\n</code></pre>  Create a <code> plot_error_surfaces</code> object to visualize the data space and the parameter space during training:    <pre><code># Create plot_error_surfaces for viewing the data\n\nget_surface = plot_error_surfaces(15, 15, X, Y, 30)\n</code></pre>      ![png](output_27_1.png)      ![png](output_27_2.png)"},{"location":"Deep%20Learning/Week2-Linear-Regression/2.3_training_slope_and_bias_v3/#Train","title":"Train the ModelWhat's on your mind? Put it in the comments!","text":"Create model parameters <code>w</code>, <code>b</code> by setting the argument <code>requires_grad</code> to True because we must learn it using the data.    <pre><code># Define the parameters w, b for y = wx + b\n\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\n</code></pre>  Set the learning rate to 0.1 and create an empty list <code>LOSS</code> for storing the loss for each iteration.    <pre><code># Define learning rate and create an empty list for containing the loss for each iteration.\n\nlr = 0.1\nLOSS = []\n</code></pre>  Define <code>train_model</code> function for train the model.    <pre><code># The function for training the model\n\ndef train_model(iter):\n\n    # Loop\n    for epoch in range(iter):\n\n        # make a prediction\n        Yhat = forward(X)\n\n        # calculate the loss \n        loss = criterion(Yhat, Y)\n\n        # Section for plotting\n        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n        if epoch % 3 == 0:\n            get_surface.plot_ps()\n\n        # store the loss in the list LOSS\n        LOSS.append(loss.item())\n\n        # backward pass: compute gradient of the loss with respect to all the learnable parameters\n        loss.backward()\n\n        # update parameters slope and bias\n        w.data = w.data - lr * w.grad.data\n        b.data = b.data - lr * b.grad.data\n\n        # zero the gradients before running the backward pass\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n</code></pre>  Run 15 iterations of gradient descent: bug data space is 1 iteration ahead of parameter space     <pre><code># Train the model with 15 iterations\n\ntrain_model(15)\n</code></pre>    ![png](output_37_0.png)      ![png](output_37_1.png)      ![png](output_37_2.png)      ![png](output_37_3.png)      ![png](output_37_4.png)    Plot total loss/cost surface with loss values for different parameters in red:    <pre><code># Plot out the Loss Result\n\n\nget_surface.final_plot()\nplt.plot(LOSS)\nplt.tight_layout()\nplt.xlabel(\"Epoch/Iterations\")\nplt.ylabel(\"Cost\")\n</code></pre>    ![png](output_39_0.png)      ![png](output_39_1.png)           Text(38.347222222222214, 0.5, 'Cost')      ![png](output_39_3.png)     Practice   Experiment using s learning rates 0.2 and width the following parameters. Run 15 iterations.    <pre><code># Practice: train and plot the result with lr = 0.2 and the following parameters\n\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\nlr = 0.2\nLOSS2 = []\n</code></pre>  Double-click here for the solution.    Plot the <code>LOSS</code> and <code>LOSS2</code> <pre><code># Practice: Plot the LOSS and LOSS2 in order to compare the Total Loss\n\n# Type your code here\nplt.plot(LOSS, label = \"LOSS\")\nplt.plot(LOSS2, label = \"LOSS2\")\nplt.tight_layout()\nplt.xlabel(\"Epoch/Iterations\")\nplt.ylabel(\"Cost\")\nplt.legend()\n</code></pre>       ![png](output_46_1.png)    ## About the Author:  Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning. I also enjoy developing innovative algorithms and models that can drive insights and value. I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website [jumashafara.dataidea.org](https://jumashafara.dataidea.org). Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.    <pre><code>#| hide\nimport nbdev; nbdev.nbdev_export()\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Deep%20Learning/Week2-Linear-Regression/linear_regression_exercise/","title":"Linear Regression Exercise","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/linear_regression_exercise/#week-2-linear-regression-3-questions","title":"Week 2: Linear Regression (3 Questions)","text":""},{"location":"Deep%20Learning/Week2-Linear-Regression/linear_regression_exercise/#1-linear-regression-1-dimension","title":"1. Linear Regression 1 Dimension","text":"<ul> <li>Exercise: Implement a simple linear regression model to fit a line to 1D data points generated as <code>(x, y) = (x, 3x + 7 + noise)</code>. Visualize the data and the fitted line.</li> </ul>"},{"location":"Deep%20Learning/Week2-Linear-Regression/linear_regression_exercise/#2-linear-regression-with-1-parameter","title":"2. Linear Regression with 1 Parameter","text":"<ul> <li>Exercise: Create a linear regression model with a single parameter. Train the model on a dataset with a known linear relationship, and evaluate the model\u2019s performance by plotting the predicted vs. actual values.</li> </ul>  Don't Miss Any Updates! <p> Before the last question, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Deep%20Learning/Week2-Linear-Regression/linear_regression_exercise/#3-training-slope-and-bias","title":"3. Training Slope and Bias","text":"<ul> <li>Exercise: Implement a linear regression model from scratch to learn the slope and bias for the dataset <code>(x, y) = (x, 4x + 10 + noise)</code>. Compare the learned parameters with the true values.</li> </ul> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.1_stochastic_gradient_descent_v3/","title":"Training Two Parameter Stochastic Gradient Descent","text":"Linear regression 1D: Training Two Parameter Stochastic Gradient Descent (SGD) Objective <ul><li> How to use SGD(Stochastic Gradient Descent) to train the model.</li></ul> Table of Contents <p>In this Lab, you will practice training a model by using Stochastic Gradient descent.</p> <ul> <li>Make Some Data</li> <li>Create the Model and Cost Function (Total Loss)</li> <li>Train the Model:Batch Gradient Descent</li> <li>Train the Model:Stochastic gradient descent</li> <li>Train the Model:Stochastic gradient descent with Data Loader</li> </ul> <p>Estimated Time Needed: 30 min</p>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> Preparation <p>We'll need the following libraries:  </p> <pre><code># These are the libraries we are going to use in the lab.\n\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits import mplot3d\nfrom dataidea_science.plots import plot_error_surfaces\n</code></pre> <p>The class <code>plot_error_surfaces</code> is just to help you visualize the data space and the parameter space during training and has nothing to do with PyTorch.</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.1_stochastic_gradient_descent_v3/#Makeup_Data","title":"Make Some Data","text":"<p>Set random seed: </p> <pre><code># Set random seed\n\ntorch.manual_seed(1)\n</code></pre> <pre><code>&lt;torch._C.Generator at 0x79c1633e7c10&gt;\n</code></pre> <p>Generate values from -3 to 3 that create a line with a slope of 1 and a bias of -1. This is the line that you need to estimate. Add some noise to the data:</p> <pre><code># Setup the actual data and simulated data\n\nX = torch.arange(-3, 3, 0.1).view(-1, 1)\nf = 1 * X - 1\nY = f + 0.1 * torch.randn(X.size())\n</code></pre> <p>Plot the results:</p> <pre><code># Plot out the data dots and line\nfrom dataidea.models import *\n</code></pre>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.1_stochastic_gradient_descent_v3/#Model_Cost","title":"Create the Model and Cost Function (Total Loss)","text":"<p>Define the <code>forward</code> function:</p> <pre><code># Define the forward function\n\ndef forward(x):\n    return w * x + b\n</code></pre> <p>Define the cost or criterion function (MSE): </p> <pre><code># Define the MSE Loss function\n\ndef criterion(yhat, y):\n    return torch.mean((yhat - y) ** 2)\n</code></pre> <p>Create a <code> plot_error_surfaces</code> object to visualize the data space and the parameter space during training:</p> <pre><code># Create plot_error_surfaces for viewing the data\n\nget_surface = plot_error_surfaces(15, 13, X, Y, 30)\n</code></pre> <pre><code>&lt;Figure size 640x480 with 0 Axes&gt;\n</code></pre> <p></p> <p></p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.1_stochastic_gradient_descent_v3/#BGD","title":"Train the Model: Batch Gradient Descent","text":"<p>Create model parameters <code>w</code>, <code>b</code> by setting the argument <code>requires_grad</code> to True because the system must learn it.</p> <pre><code># Define the parameters w, b for y = wx + b\n\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\n</code></pre> <p>Set the learning rate to  0.1 and create an empty list <code>LOSS</code> for storing the loss for each iteration.</p> <pre><code># Define learning rate and create an empty list for containing the loss for each iteration.\n\nlr = 0.1\nLOSS_BGD = []\n</code></pre> <p>Define <code>train_model</code> function for train the model.</p> <pre><code># The function for training the model\n\ndef train_model(iter):\n\n    # Loop\n    for epoch in range(iter):\n\n        # make a prediction\n        Yhat = forward(X)\n\n        # calculate the loss \n        loss = criterion(Yhat, Y)\n\n        # Section for plotting\n        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n        get_surface.plot_ps()\n\n        # store the loss in the list LOSS_BGD\n        LOSS_BGD.append(loss)\n\n        # backward pass: compute gradient of the loss with respect to all the learnable parameters\n        loss.backward()\n\n        # update parameters slope and bias\n        w.data = w.data - lr * w.grad.data\n        b.data = b.data - lr * b.grad.data\n\n        # zero the gradients before running the backward pass\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n</code></pre> <p>Run 10 epochs of batch gradient descent: bug data space is 1 iteration ahead of parameter space. </p> <pre><code># Train the model with 10 iterations\n\ntrain_model(10)\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.1_stochastic_gradient_descent_v3/#SGD","title":"Train the Model: Stochastic Gradient Descent","text":"<p>Create a <code>plot_error_surfaces</code> object to visualize the data space and the parameter space during training:</p> <pre><code># Create plot_error_surfaces for viewing the data\n\nget_surface = plot_error_surfaces(15, 13, X, Y, 30, go = False)\n</code></pre> <p>Define <code>train_model_SGD</code> function for training the model.</p> <pre><code># The function for training the model\n\nLOSS_SGD = []\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\n\ndef train_model_SGD(iter):\n\n    # Loop\n    for epoch in range(iter):\n\n        # SGD is an approximation of out true total loss/cost, in this line of code we calculate our true loss/cost and store it\n        Yhat = forward(X)\n\n        # store the loss \n        LOSS_SGD.append(criterion(Yhat, Y).tolist())\n\n        for x, y in zip(X, Y):\n\n            # make a pridiction\n            yhat = forward(x)\n\n            # calculate the loss \n            loss = criterion(yhat, y)\n\n            # Section for plotting\n            get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n\n            # backward pass: compute gradient of the loss with respect to all the learnable parameters\n            loss.backward()\n\n            # update parameters slope and bias\n            w.data = w.data - lr * w.grad.data\n            b.data = b.data - lr * b.grad.data\n\n            # zero the gradients before running the backward pass\n            w.grad.data.zero_()\n            b.grad.data.zero_()\n\n        #plot surface and data space after each epoch    \n        get_surface.plot_ps()\n</code></pre> <p>Run 10 epochs of stochastic gradient descent: bug data space is 1 iteration ahead of parameter space. </p> <pre><code># Train the model with 10 iterations\n\ntrain_model_SGD(10)\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Compare the loss of both batch gradient descent as SGD.</p> <pre><code>LOSS_BGD_ = [loss.item() for loss in LOSS_BGD]\n</code></pre> <pre><code># Plot out the LOSS_BGD and LOSS_SGD\n\nplt.plot(LOSS_BGD_,label = \"Batch Gradient Descent\")\nplt.plot(LOSS_SGD,label = \"Stochastic Gradient Descent\")\nplt.xlabel('epoch')\nplt.ylabel('Cost/ total loss')\nplt.legend()\nplt.show()\n</code></pre> <p></p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.1_stochastic_gradient_descent_v3/#SGD_Loader","title":"SGD with Dataset DataLoader","text":"<p>Import the module for building a dataset class: </p> <pre><code># Import the library for DataLoader\n\nfrom torch.utils.data import Dataset, DataLoader\n</code></pre> <p>Create a dataset class:</p> <pre><code># Dataset Class\n\nclass Data(Dataset):\n\n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-3, 3, 0.1).view(-1, 1)\n        self.y = 1 * self.x - 1\n        self.len = self.x.shape[0]\n\n    # Getter\n    def __getitem__(self,index):    \n        return self.x[index], self.y[index]\n\n    # Return the length\n    def __len__(self):\n        return self.len\n</code></pre> <p>Create a dataset object and check the length of the dataset.</p> <pre><code># Create the dataset and check the length\n\ndataset = Data()\nprint(\"The length of dataset: \", len(dataset))\n</code></pre> <pre><code>The length of dataset:  60\n</code></pre> <p>Obtain the first training point:  </p> <pre><code># Print the first point\n\nx, y = dataset[0]\nprint(\"(\", x, \", \", y, \")\")\n</code></pre> <pre><code>( tensor([-3.]) ,  tensor([-4.]) )\n</code></pre> <p>Similarly, obtain the first three training points:  </p> <pre><code># Print the first 3 point\n\nx, y = dataset[0:3]\nprint(\"The first 3 x: \", x)\nprint(\"The first 3 y: \", y)\n</code></pre> <pre><code>The first 3 x:  tensor([[-3.0000],\n        [-2.9000],\n        [-2.8000]])\nThe first 3 y:  tensor([[-4.0000],\n        [-3.9000],\n        [-3.8000]])\n</code></pre> <p>Create a <code>plot_error_surfaces</code> object to visualize the data space and the parameter space during training:</p> <pre><code># Create plot_error_surfaces for viewing the data\n\nget_surface = plot_error_surfaces(15, 13, X, Y, 30, go = False)\n</code></pre> <p>Create a <code>DataLoader</code> object by using the constructor: </p> <pre><code># Create DataLoader\n\ntrainloader = DataLoader(dataset = dataset, batch_size = 1)\n</code></pre> <p>Define <code>train_model_DataLoader</code> function for training the model.</p> <pre><code># The function for training the model\n\nw = torch.tensor(-15.0,requires_grad=True)\nb = torch.tensor(-10.0,requires_grad=True)\nLOSS_Loader = []\n\ndef train_model_DataLoader(epochs):\n\n    # Loop\n    for epoch in range(epochs):\n\n        # SGD is an approximation of out true total loss/cost, in this line of code we calculate our true loss/cost and store it\n        Yhat = forward(X)\n\n        # store the loss \n        LOSS_Loader.append(criterion(Yhat, Y).tolist())\n\n        for x, y in trainloader:\n\n            # make a prediction\n            yhat = forward(x)\n\n            # calculate the loss\n            loss = criterion(yhat, y)\n\n            # Section for plotting\n            get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n\n            # Backward pass: compute gradient of the loss with respect to all the learnable parameters\n            loss.backward()\n\n            # Updata parameters slope\n            w.data = w.data - lr * w.grad.data\n            b.data = b.data - lr* b.grad.data\n\n            # Clear gradients \n            w.grad.data.zero_()\n            b.grad.data.zero_()\n\n        #plot surface and data space after each epoch    \n        get_surface.plot_ps()\n</code></pre> <p>Run 10 epochs of stochastic gradient descent: bug data space is 1 iteration ahead of parameter space. </p> <pre><code># Run 10 iterations\n\ntrain_model_DataLoader(10)\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Compare the loss of both batch gradient decent as SGD. Note that SGD converges to a minimum faster, that is, it decreases faster. </p> <pre><code># Plot the LOSS_BGD and LOSS_Loader\n\nplt.plot(LOSS_BGD_,label=\"Batch Gradient Descent\")\nplt.plot(LOSS_Loader,label=\"Stochastic Gradient Descent with DataLoader\")\nplt.xlabel('epoch')\nplt.ylabel('Cost/ total loss')\nplt.legend()\nplt.show()\n</code></pre> <p></p> Practice <p>For practice, try to use SGD with DataLoader to train model with 10 iterations. Store the total loss in <code>LOSS</code>. We are going to use it in the next question.</p> <pre><code># Practice: Use SGD with trainloader to train model and store the total loss in LOSS\n\nLOSS = []\nw = torch.tensor(-12.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\n</code></pre> <p>Double-click here for the solution.</p> <p>Plot the total loss</p> <pre><code># Practice: Plot the total loss using LOSS\n\n# Type your code here\n</code></pre> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.1_stochastic_gradient_descent_v3/#about-the-author","title":"About the Author:What's on your mind? Put it in the comments!","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning. </p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value. </p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org. </p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/","title":"Training Two Parameter Mini-Batch Gradient Decent","text":"Linear Regression 1D: Training Two Parameter Mini-Batch Gradient Decent Objective <ul><li> How to use Mini-Batch Gradient Descent to train model.</li></ul> Table of Contents <p>In this Lab, you will practice training a model by using Mini-Batch Gradient Descent.</p> <ul> <li> Make Some Data</li> <li> Create the Model and Cost Function (Total Loss)</li> <li> Train the Model: Batch Gradient Descent</li> <li> Train the Model: Stochastic Gradient Descent with Dataset DataLoader</li> <li> Train the Model: Mini Batch Gradient Decent: Batch Size Equals 5</li> <li> Train the Model: Mini Batch Gradient Decent: Batch Size Equals 10</li> </ul> <p>Estimated Time Needed: 30 min</p>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/#preparation","title":"Preparation","text":"<p>We'll need the following libraries:</p> <pre><code># Import the libraries we need for this lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d\nfrom dataidea_science.plots import plot_error_surfaces\n</code></pre> <p>The class <code>plot_error_surfaces</code> is just to help you visualize the data space and the parameter space during training and has nothing to do with PyTorch.</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/#Makeup_Data","title":"Make Some Data","text":"<p>Import PyTorch and set random seed:</p> <pre><code># Import PyTorch library\n\nimport torch\ntorch.manual_seed(1)\n</code></pre> <pre><code>&lt;torch._C.Generator at 0x7dc5dfe66290&gt;\n</code></pre> <p>Generate values from -3 to 3 that create a line with a slope of 1 and a bias of -1. This is the line that you need to estimate. Add some noise to the data:</p> <pre><code># Generate the data with noise and the line\n\nX = torch.arange(-3, 3, 0.1).view(-1, 1)\nf = 1 * X - 1\nY = f + 0.1 * torch.randn(X.size())\n</code></pre> <p>Plot the results:</p> <pre><code># Plot the line and the data\n\nplt.plot(X.numpy(), Y.numpy(), 'o', label = 'y', c='g')\nplt.plot(X.numpy(), f.numpy(), label = 'f', c='b')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n</code></pre> <p></p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/#Model_Cost","title":"Create the Model and Cost Function (Total Loss) Train the Model: Batch Gradient Descent (BGD)","text":"<p>Define the <code>forward</code> function:</p> <pre><code># Define the prediction function\n\ndef forward(x):\n    return w * x + b\n</code></pre> <p>Define the cost or criterion function:</p> <pre><code># Define the cost function\n\ndef criterion(yhat, y):\n    return torch.mean((yhat - y) ** 2)\n</code></pre> <p>Create a <code> plot_error_surfaces</code> object to visualize the data space and the parameter space during training:</p> <pre><code># Create a plot_error_surfaces object.\n\nget_surface = plot_error_surfaces(15, 13, X, Y, 30)\n</code></pre> <pre><code>&lt;Figure size 640x480 with 0 Axes&gt;\n</code></pre> <p></p> <p></p> <p>Define <code>train_model_BGD</code> function.</p> <pre><code># Define the function for training model\n\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\nlr = 0.1\nLOSS_BGD = []\n\ndef train_model_BGD(epochs):\n    for epoch in range(epochs):\n        Yhat = forward(X)\n        loss = criterion(Yhat, Y)\n        LOSS_BGD.append(loss)\n        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n        get_surface.plot_ps()\n        loss.backward()\n        w.data = w.data - lr * w.grad.data\n        b.data = b.data - lr * b.grad.data\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n</code></pre> <p>Run 10 epochs of batch gradient descent: bug data space is 1 iteration ahead of parameter space.</p> <pre><code># Run train_model_BGD with 10 iterations\n\ntrain_model_BGD(10)\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/#SGD","title":"Stochastic Gradient Descent (SGD) with Dataset DataLoader","text":"<p>Create a <code>plot_error_surfaces</code> object to visualize the data space and the parameter space during training:</p> <pre><code># Create a plot_error_surfaces object.\n\nget_surface = plot_error_surfaces(15, 13, X, Y, 30, go = False)\n</code></pre> <p>Import <code>Dataset</code> and <code>DataLoader</code> libraries</p> <pre><code># Import libraries\n\nfrom torch.utils.data import Dataset, DataLoader\n</code></pre> <p>Create <code>Data</code> class</p> <pre><code># Create class Data\n\nclass Data(Dataset):\n\n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-3, 3, 0.1).view(-1, 1)\n        self.y = 1 * X - 1\n        self.len = self.x.shape[0]\n\n    # Getter\n    def __getitem__(self, index):    \n        return self.x[index], self.y[index]\n\n    # Get length\n    def __len__(self):\n        return self.len\n</code></pre> <p>Create a dataset object and a dataloader object:</p> <pre><code># Create Data object and DataLoader object\n\ndataset = Data()\ntrainloader = DataLoader(dataset = dataset, batch_size = 1)\n</code></pre> <p>Define <code>train_model_SGD</code> function for training the model.</p> <pre><code># Define train_model_SGD function\n\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\nLOSS_SGD = []\nlr = 0.1\ndef train_model_SGD(epochs):\n    for epoch in range(epochs):\n        Yhat = forward(X)\n        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), criterion(Yhat, Y).tolist())\n        get_surface.plot_ps()\n        LOSS_SGD.append(criterion(forward(X), Y).tolist())\n        for x, y in trainloader:\n            yhat = forward(x)\n            loss = criterion(yhat, y)\n            get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n            loss.backward()\n            w.data = w.data - lr * w.grad.data\n            b.data = b.data - lr * b.grad.data\n            w.grad.data.zero_()\n            b.grad.data.zero_()\n        get_surface.plot_ps()\n</code></pre> <p>Run 10 epochs of stochastic gradient descent: bug data space is 1 iteration ahead of parameter space.</p> <pre><code># Run train_model_SGD(iter) with 10 iterations\n\ntrain_model_SGD(10)\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/#Mini5","title":"Mini Batch Gradient Descent: Batch Size Equals 5","text":"<p>Create a <code> plot_error_surfaces</code> object to visualize the data space and the parameter space during training:</p> <pre><code># Create a plot_error_surfaces object.\n\nget_surface = plot_error_surfaces(15, 13, X, Y, 30, go = False)\nget_surface\n</code></pre> <pre><code>&lt;__main__.plot_error_surfaces at 0x77fc19f16660&gt;\n</code></pre> <p>Create <code>Data</code> object and create a <code>Dataloader</code> object where the batch size equals 5:</p> <pre><code># Create DataLoader object and Data object\n\ndataset = Data()\ntrainloader = DataLoader(dataset = dataset, batch_size = 5)\n</code></pre> <p>Define <code>train_model_Mini5</code> function to train the model.</p> <pre><code># Define train_model_Mini5 function\n\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\nLOSS_MINI5 = []\nlr = 0.1\n\ndef train_model_Mini5(epochs):\n    for epoch in range(epochs):\n        Yhat = forward(X)\n        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), criterion(Yhat, Y).tolist())\n        get_surface.plot_ps()\n        LOSS_MINI5.append(criterion(forward(X), Y).tolist())\n        for x, y in trainloader:\n            yhat = forward(x)\n            loss = criterion(yhat, y)\n            get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n            loss.backward()\n            w.data = w.data - lr * w.grad.data\n            b.data = b.data - lr * b.grad.data\n            w.grad.data.zero_()\n            b.grad.data.zero_()\n</code></pre> <p>Run 10 epochs of mini-batch gradient descent: bug data space is 1 iteration ahead of parameter space.</p> <pre><code># Run train_model_Mini5 with 10 iterations.\n\ntrain_model_Mini5(10)\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/#Mini10","title":"Mini Batch Gradient Descent: Batch Size Equals 10","text":"<p>Create a <code> plot_error_surfaces</code> object to visualize the data space and the parameter space during training:</p> <pre><code># Create a plot_error_surfaces object.\n\nget_surface = plot_error_surfaces(15, 13, X, Y, 30, go = False)\n</code></pre> <p>Create <code>Data</code> object and create a <code>Dataloader</code> object batch size equals 10</p> <pre><code># Create DataLoader object\n\ndataset = Data()\ntrainloader = DataLoader(dataset = dataset, batch_size = 10)\n</code></pre> <p>Define <code>train_model_Mini10</code> function for training the model.</p> <pre><code># Define train_model_Mini5 function\n\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\nLOSS_MINI10 = []\nlr = 0.1\n\ndef train_model_Mini10(epochs):\n    for epoch in range(epochs):\n        Yhat = forward(X)\n        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), criterion(Yhat, Y).tolist())\n        get_surface.plot_ps()\n        LOSS_MINI10.append(criterion(forward(X),Y).tolist())\n        for x, y in trainloader:\n            yhat = forward(x)\n            loss = criterion(yhat, y)\n            get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n            loss.backward()\n            w.data = w.data - lr * w.grad.data\n            b.data = b.data - lr * b.grad.data\n            w.grad.data.zero_()\n            b.grad.data.zero_()\n</code></pre> <p>Run 10 epochs of mini-batch gradient descent: bug data space is 1 iteration ahead of parameter space.</p> <pre><code># Run train_model_Mini5 with 10 iterations.\n\ntrain_model_Mini10(10)\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Plot the loss for each epoch:</p> <pre><code>LOSS_BGD_ = [loss.item() for loss in LOSS_BGD]\n</code></pre> <pre><code># Plot out the LOSS for each method\n\nplt.plot(LOSS_BGD_,label = \"Batch Gradient Descent\")\nplt.plot(LOSS_SGD,label = \"Stochastic Gradient Descent\")\nplt.plot(LOSS_MINI5,label = \"Mini-Batch Gradient Descent, Batch size: 5\")\nplt.plot(LOSS_MINI10,label = \"Mini-Batch Gradient Descent, Batch size: 10\")\nplt.legend()\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x77fc3583c5f0&gt;\n</code></pre> <p></p> Practice <p>Perform mini batch gradient descent with a batch size of 20. Store the total loss for each epoch in the list LOSS20.</p> <pre><code># Practice: Perform mini batch gradient descent with a batch size of 20.\n\ndataset = Data()\ntrainloader = DataLoader(dataset = dataset, batch_size = 20)\nw = torch.tensor(-15.0, requires_grad = True)\nb = torch.tensor(-10.0, requires_grad = True)\n\nLOSS_MINI20 = []\nlr = 0.1\n\ndef my_train_model(epochs):\n    for epoc in range(epochs):\n        Yhat = forward(X)\n        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), criterion(Yhat, Y).tolist())\n        get_surface.plot_ps()\n        LOSS_MINI20.append(criterion(forward(X), Y).tolist())\n        for x, y in trainloader:\n            yhat = forward(x)\n            loss = criterion(yhat, y)\n            get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n            loss.backward()\n            w.data = w.data - lr * w.grad.data\n            b.data = b.data - lr * b.grad.data\n            w.grad.data.zero_()\n            b.grad.data.zero_()\n</code></pre> <p>Double-click here for the solution.</p> <p>Plot a graph that shows the LOSS results for all the methods.</p> <pre><code># Practice: Plot a graph to show all the LOSS functions\n\n# Type your code here\n</code></pre> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.2_mini-batch_gradient_descent_v3/#about-the-author","title":"About the Author:What's on your mind? Put it in the comments!","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.3_PyTorchway_v3/","title":"PyTorch build-in functions","text":"Linear Regression 1D: Training Two Parameter Mini-Batch Gradient Descent  Objective <ul><li>     How to use PyTorch build-in functions to create a model.</li></ul> Table of Contents <p>In this lab, you will create a model the PyTroch way, this will help you as models get more complicated</p> <ul> <li> Make Some Data </li> <li> Create the Model and Cost Function the PyTorch way </li> <li> Train the Model: Batch Gradient Descent</li> </ul> <p>Estimated Time Needed: 30 min</p>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> Preparation <p>We'll need the following libraries:  </p> <pre><code># These are the libraries we are going to use in the lab.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d\nfrom dataidea_science.plots import plot_error_surfaces\n</code></pre> <p>The class <code>plot_error_surfaces</code> is just to help you visualize the data space and the parameter space during training and has nothing to do with PyTorch. </p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.3_PyTorchway_v3/#Makeup_Data","title":"Make Some Data","text":"<p>Import libraries and set random seed.</p> <pre><code># Import libraries and set random seed\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\ntorch.manual_seed(1)\n</code></pre> <pre><code>&lt;torch._C.Generator at 0x7f5c8438a2b0&gt;\n</code></pre> <p>Generate values from -3 to 3 that create a line with a slope of 1 and a bias of -1. This is the line that you need to estimate. Add some noise to the data:</p> <pre><code># Create Data Class\n\nclass Data(Dataset):\n\n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-3, 3, 0.1).view(-1, 1)\n        self.f = 1 * self.x - 1\n        self.y = self.f + 0.1 * torch.randn(self.x.size())\n        self.len = self.x.shape[0]\n\n    # Getter\n    def __getitem__(self,index):    \n        return self.x[index],self.y[index]\n\n    # Get Length\n    def __len__(self):\n        return self.len\n</code></pre> <p>Create a dataset object: </p> <pre><code># Create dataset object\n\ndataset = Data()\n</code></pre> <p>Plot out the data and the line.</p> <pre><code># Plot the data\n\nplt.plot(dataset.x.numpy(), dataset.y.numpy(), 'rx', label = 'y')\nplt.plot(dataset.x.numpy(), dataset.f.numpy(), label = 'f')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x7f5c615e7d40&gt;\n</code></pre> <p></p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.3_PyTorchway_v3/#Model_Cost","title":"Create the Model and Total Loss Function (Cost)","text":"<p>Create a linear regression class </p> <pre><code># Create a linear regression model class\n\nfrom torch import nn, optim\n\nclass linear_regression(nn.Module):\n\n    # Constructor\n    def __init__(self, input_size, output_size):\n        super(linear_regression, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n\n    # Prediction\n    def forward(self, x):\n        yhat = self.linear(x)\n        return yhat\n</code></pre> <p>Note about the <code>super()</code> method: - When <code>LinearRegression</code> is instantiated, its <code>__init__</code> method is called. - Inside <code>LinearRegression.__init__</code>, <code>super(LinearRegression, self).__init__()</code> calls the <code>__init__</code> method of the parent class (<code>nn.Module</code>). - After the parent class is initialized, the rest of the code in the <code>LinearRegression.__init__</code> method runs too.</p> <p>This mechanism ensures that the <code>nn.Module</code> class is properly initialized before any additional initialization specific to <code>LinearRegression</code> occurs.</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.3_PyTorchway_v3/#the-loss-and-optimizer-functions","title":"The Loss and Optimizer Functions","text":"<p>We will use PyTorch build-in functions to create a criterion function; this calculates the total loss or cost </p> <pre><code># Build in cost function\n\ncriterion = nn.MSELoss()\n</code></pre> <p>Create a linear regression object and optimizer object, the optimizer object will use the linear regression object.</p> <pre><code># Create optimizer\n\nmodel = linear_regression(1,1)\noptimizer = optim.SGD(model.parameters(), lr = 0.01)\n</code></pre> <pre><code>list(model.parameters())\n</code></pre> <pre><code>[Parameter containing:\n tensor([[0.3636]], requires_grad=True),\n Parameter containing:\n tensor([0.4957], requires_grad=True)]\n</code></pre> <p>Remember to construct an optimizer you have to give it an iterable containing the parameters i.e. provide <code> model.parameters()</code> as an input to the object constructor </p> <p></p> <p>Similar to the model, the optimizer has a state dictionary:</p> <pre><code>optimizer.state_dict()\n</code></pre> <pre><code>{'state': {},\n 'param_groups': [{'lr': 0.01,\n   'momentum': 0,\n   'dampening': 0,\n   'weight_decay': 0,\n   'nesterov': False,\n   'maximize': False,\n   'foreach': None,\n   'differentiable': False,\n   'fused': None,\n   'params': [0, 1]}]}\n</code></pre> <p>Many of the keys correspond to more advanced optimizers.</p> <p>Create a <code>Dataloader</code> object: </p> <pre><code># Create Dataloader object\n\ntrainloader = DataLoader(dataset = dataset, batch_size = 1)\n</code></pre> <p>PyTorch randomly initialises your model parameters. If we use those parameters, the result will not be very insightful as convergence will be extremely fast. So we will initialise the parameters such that they will take longer to converge, i.e. look cool  </p> <pre><code># Customize the weight and bias\n\nmodel.state_dict()['linear.weight'][0] = -15\nmodel.state_dict()['linear.bias'][0] = -10\n</code></pre> <p>Create a plotting object, not part of PyTroch, just used to help visualize </p> <pre><code># Create plot surface object\n\nget_surface = plot_error_surfaces(15, 13, dataset.x, dataset.y, 30, go = False)\n</code></pre>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.3_PyTorchway_v3/#BGD","title":"Train the Model via Batch Gradient Descent","text":"<p>Run 10 epochs of stochastic gradient descent: bug data space is 1 iteration ahead of parameter space. </p> <pre><code># Train Model\n\ndef train_model_BGD(iter):\n    for epoch in range(iter):\n        for x,y in trainloader:\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            get_surface.set_para_loss(model, loss.tolist())          \n            optimizer.zero_grad()\n            loss.backward()\n\n            optimizer.step()\n        get_surface.plot_ps()\n\n\ntrain_model_BGD(10)\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <pre><code>model.state_dict()\n</code></pre> <pre><code>OrderedDict([('linear.weight', tensor([[0.9932]])),\n             ('linear.bias', tensor([-1.0174]))])\n</code></pre> <p>Let's use the following diagram to help clarify the process. The model takes <code>x</code> to produce an estimate <code>yhat</code>, it will then be compared to the actual <code>y</code>  with the loss function.</p> <p></p> <p>When we call <code>backward()</code> on the loss function, it will handle the differentiation. Calling the method step on the optimizer object it will update the parameters as they were inputs when we constructed the optimizer object. The connection is shown in the following figure :</p> <p></p> Practice <p>Try to train the model via BGD with <code>lr = 0.1</code>. Use <code>optimizer</code> and the following given variables.</p> <pre><code># Practice: Train the model via BGD using optimizer\n\nmodel = linear_regression(1,1)\nmodel.state_dict()['linear.weight'][0] = -15\nmodel.state_dict()['linear.bias'][0] = -10\nget_surface = plot_error_surfaces(15, 13, dataset.x, dataset.y, 30, go = False)\n</code></pre> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.3_PyTorchway_v3/#about-the-author","title":"About the Author:What's on your mind? Put it in the comments!","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.6_training_and_validation_v3/","title":"Training and Validation Data","text":"Linear regression: Training and Validation Data Objective <ul><li> How to use learning rate hyperparameter to improve your model result.</li></ul> Table of Contents <p>In this lab, you will learn to select the best learning rate by using validation data.</p> <ul> <li> Make Some Data</li> <li> Create a Linear Regression Object, Data Loader and Criterion Function</li> <li> Different learning rates and Data Structures to Store results for Different Hyperparameters</li> <li> Train different modules for different Hyperparameters</li> <li> View Results</li> </ul> <p>Estimated Time Needed: 30 min</p>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> Preparation <p>We'll need the following libraries and set the random seed.</p> <pre><code># Import libraries we need for this lab, and set the random seed\n\nfrom torch import nn\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch import nn,optim\n</code></pre>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.6_training_and_validation_v3/#Makeup_Data","title":"Make Some Data","text":"<p>First, we'll create some artificial data in a dataset class. The class will include the option to produce training data or validation data. The training data will include outliers.</p> <pre><code># Create Data class\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass Data(Dataset):\n\n    # Constructor\n    def __init__(self, train = True):\n            self.x = torch.arange(-3, 3, 0.1).view(-1, 1)\n            self.f = -3 * self.x + 1\n            self.y = self.f + 0.1 * torch.randn(self.x.size())\n            self.len = self.x.shape[0]\n\n            #outliers \n            if train == True:\n                self.y[0] = 0\n                self.y[50:55] = 20\n            else:\n                pass\n\n    # Getter\n    def __getitem__(self, index):    \n        return self.x[index], self.y[index]\n\n    # Get Length\n    def __len__(self):\n        return self.len\n</code></pre> <p>Create two objects: one that contains training data and a second that contains validation data. Assume that the training data has the outliers. </p> <pre><code># Create training dataset and validation dataset\n\ntrain_data = Data()\nval_data = Data(train = False)\n</code></pre> <p>Overlay the training points in red over the function that generated the data. Notice the outliers at x=-3 and around x=2:</p> <pre><code># Plot out training points\n\nplt.plot(train_data.x.numpy(), train_data.y.numpy(), 'xr',label=\"training data \")\nplt.plot(train_data.x.numpy(), train_data.f.numpy(),label=\"true function  \")\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n</code></pre> <p></p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.6_training_and_validation_v3/#LR_Loader_Cost","title":"Create a Linear Regression Object,  Data Loader, and Criterion Function","text":"<pre><code># Create Linear Regression Class\n\nfrom torch import nn\n\nclass linear_regression(nn.Module):\n\n    # Constructor\n    def __init__(self, input_size, output_size):\n        super(linear_regression, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n\n    # Prediction function\n    def forward(self, x):\n        yhat = self.linear(x)\n        return yhat\n</code></pre> <p>Create the criterion function and a <code>DataLoader</code> object: </p> <pre><code># Create MSELoss function and DataLoader\n\ncriterion = nn.MSELoss()\ntrainloader = DataLoader(dataset = train_data, batch_size = 1)\n</code></pre>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.6_training_and_validation_v3/#LR_Hyper","title":"Different learning rates and Data Structures to Store results for different Hyperparameters","text":"<p>Create a list with different learning rates and a tensor (can be a list) for the training and validating cost/total loss. Include the list MODELS, which stores the training model for every value of the learning rate. </p> <pre><code># Create Learning Rate list, the error lists and the MODELS list\n\nlearning_rates=[0.0001, 0.001, 0.01, 0.1]\n\ntrain_error=torch.zeros(len(learning_rates))\nvalidation_error=torch.zeros(len(learning_rates))\n\nMODELS=[]\n</code></pre>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.6_training_and_validation_v3/#Model","title":"Train different models  for different Hyperparameters","text":"<p>Try different values of learning rates, perform stochastic gradient descent, and save the results on the training data and validation data. Finally, save each model in a list.</p> <pre><code># Define the train model function and train the model\n\ndef train_model_with_lr (iter, lr_list):\n\n    # iterate through different learning rates \n    for i, lr in enumerate(lr_list):\n        model = linear_regression(1, 1)\n        optimizer = optim.SGD(model.parameters(), lr = lr)\n        for epoch in range(iter):\n            for x, y in trainloader:\n                yhat = model(x)\n                loss = criterion(yhat, y)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n        # train data\n        Yhat = model(train_data.x)\n        train_loss = criterion(Yhat, train_data.y)\n        train_error[i] = train_loss.item()\n\n        # validation data\n        Yhat = model(val_data.x)\n        val_loss = criterion(Yhat, val_data.y)\n        validation_error[i] = val_loss.item()\n        MODELS.append(model)\n\ntrain_model_with_lr(10, learning_rates)\n</code></pre>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.6_training_and_validation_v3/#Result","title":"View the Results","text":"<p>Plot the training loss and validation loss for each learning rate:  </p> <pre><code># Plot the training loss and validation loss\n\nplt.semilogx(np.array(learning_rates), train_error.numpy(), label = 'training loss/total Loss')\nplt.semilogx(np.array(learning_rates), validation_error.numpy(), label = 'validation cost/total Loss')\nplt.ylabel('Cost\\ Total Loss')\nplt.xlabel('learning rate')\nplt.legend()\nplt.show()\n</code></pre> <pre><code>&lt;&gt;:5: SyntaxWarning: invalid escape sequence '\\ '\n&lt;&gt;:5: SyntaxWarning: invalid escape sequence '\\ '\n/tmp/ipykernel_80257/980418342.py:5: SyntaxWarning: invalid escape sequence '\\ '\n  plt.ylabel('Cost\\ Total Loss')\n</code></pre> <p></p> <p>Produce a prediction by using the validation data for each model:  </p> <pre><code># Plot the predictions\n\ni = 0\nfor model, learning_rate in zip(MODELS, learning_rates):\n    yhat = model(val_data.x)\n    plt.plot(val_data.x.numpy(), yhat.detach().numpy(), label = 'lr:' + str(learning_rate))\n    print('i', yhat.detach().numpy()[0:3])\nplt.plot(val_data.x.numpy(), val_data.f.numpy(), 'or', label = 'validation data')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n</code></pre> <pre><code>i [[0.05840436]\n [0.06799743]\n [0.0775905 ]]\ni [[5.4587383]\n [5.343816 ]\n [5.2288933]]\ni [[6.5484743]\n [6.4158573]\n [6.28324  ]]\ni [[14.55032  ]\n [14.1119585]\n [13.673596 ]]\n</code></pre> <p></p> Practice <p>The object <code>good_model</code> is the best performing model. Use the train loader to get the data samples x and y. Produce an estimate for <code>yhat</code> and print it out for every sample in a for a loop. Compare it to the actual prediction <code>y</code>.</p> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/3.6_training_and_validation_v3/#about-the-author","title":"About the Author:What's on your mind? Put it in the comments!","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/linear_regression_in_pytorch_exercise/","title":"Linear Regression (PyTorch) Exercise","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/linear_regression_in_pytorch_exercise/#week-3-linear-regression-in-pytorch-4-questions","title":"Week 3: Linear Regression in PyTorch (4 Questions)","text":""},{"location":"Deep%20Learning/Week3-LR-PyTorch/linear_regression_in_pytorch_exercise/#1-stochastic-gradient-descent","title":"1. Stochastic Gradient Descent","text":"<ul> <li>Exercise: Implement a linear regression model using stochastic gradient descent (SGD) on a synthetic dataset. Plot the loss curve to show convergence over iterations.</li> </ul>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/linear_regression_in_pytorch_exercise/#2-mini-batch-gradient-descent","title":"2. Mini-Batch Gradient Descent","text":"<ul> <li>Exercise: Modify your SGD implementation to use mini-batch gradient descent. Train the model on a dataset with mini-batches and compare the performance with the full-batch SGD approach.</li> </ul>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/linear_regression_in_pytorch_exercise/#3-pytorch-built-in-functions","title":"3. PyTorch Built-in Functions","text":"<ul> <li>Exercise: Use PyTorch's built-in functions (<code>torch.nn.Linear</code>, <code>torch.optim.SGD</code>) to build and train a linear regression model. Compare the results with your previous implementations.</li> </ul>  Don't Miss Any Updates! <p> Before the last question, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Deep%20Learning/Week3-LR-PyTorch/linear_regression_in_pytorch_exercise/#4-training-and-validation-sets","title":"4. Training and Validation Sets","text":"<ul> <li>Exercise: Split your dataset into training and validation sets. Train a linear regression model on the training set and evaluate its performance on the validation set. Plot the training and validation loss over epochs.</li> </ul> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.1.multiple_linear_regression_prediction_v2/","title":"Multiple Linear Regression","text":"Objective <ul><li> How to make the prediction for multiple inputs.</li><li> How to use linear class to build more complex models.</li><li> How to build a custom module.</li></ul> Table of Contents <p>In this lab, you will review how to make a prediction in several different ways by using PyTorch.</p> <ul> <li> Prediction</li> <li> Class Linear</li> <li> Build Custom Modules</li> </ul> <p>Estimated Time Needed: 15 min</p>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> Preparation <p>Import the libraries and set the random seed.</p> <pre><code># Import the libraries and set the random seed\n\nfrom torch import nn\nimport torch\ntorch.manual_seed(1)\n</code></pre> <pre><code>&lt;torch._C.Generator at 0x7f44041ee610&gt;\n</code></pre>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.1.multiple_linear_regression_prediction_v2/#Prediction","title":"Prediction","text":"<p>Set weight and bias.</p> <pre><code># Set the weight and bias\n\nw = torch.tensor([[2.0], [3.0]], requires_grad=True)\nb = torch.tensor([[1.0]], requires_grad=True)\n</code></pre> <p>Define the parameters. <code>torch.mm</code> uses matrix multiplication instead of scaler multiplication.</p> <pre><code># Define Prediction Function\n\ndef forward(x):\n    yhat = torch.mm(x, w) + b\n    return yhat\n</code></pre> <p>The function <code>forward</code> implements the following equation:</p> <p></p> <p>If we input a 1x2 tensor, because we have a 2x1 tensor as <code>w</code>, we will get a 1x1 tensor: </p> <pre><code># Calculate yhat\n\nx = torch.tensor([[1.0, 2.0]])\nyhat = forward(x)\nprint(\"The result: \", yhat)\n</code></pre> <pre><code>The result:  tensor([[9.]], grad_fn=&lt;AddBackward0&gt;)\n</code></pre> <p></p> <p>Each row of the following tensor represents a sample:</p> <pre><code># Sample tensor X\n\nX = torch.tensor([[1.0, 1.0], [1.0, 2.0], [1.0, 3.0]])\n</code></pre> <pre><code># Make the prediction of X \n\nyhat = forward(X)\nprint(\"The result: \", yhat)\n</code></pre> <pre><code>The result:  tensor([[ 6.],\n        [ 9.],\n        [12.]], grad_fn=&lt;AddBackward0&gt;)\n</code></pre>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.1.multiple_linear_regression_prediction_v2/#Linear","title":"Class Linear","text":"<p>We can use the linear class to make a prediction. You'll also use the linear class to build more complex models.</p> <p>Let us create a model.</p> <pre><code># Make a linear regression model using build-in function\n\nmodel = nn.Linear(2, 1)\n</code></pre> <p>Make a prediction with the first sample:</p> <pre><code># Make a prediction of x\n\nyhat = model(x)\nprint(\"The result: \", yhat)\n</code></pre> <pre><code>The result:  tensor([[-0.3969]], grad_fn=&lt;AddmmBackward0&gt;)\n</code></pre> <p>Predict with multiple samples <code>X</code>: </p> <pre><code># Make a prediction of X\n\nyhat = model(X)\nprint(\"The result: \", yhat)\n</code></pre> <pre><code>The result:  tensor([[-0.0848],\n        [-0.3969],\n        [-0.7090]], grad_fn=&lt;AddmmBackward0&gt;)\n</code></pre> <p>The function performs matrix multiplication as shown in this image:</p> <p></p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.1.multiple_linear_regression_prediction_v2/#Cust","title":"Build Custom Modules","text":"<p>Now, you'll build a custom module. You can make more complex models by using this method later. </p> <pre><code># Create LinearRegression Class\n\nclass LinearRegression(nn.Module):\n\n    # Constructor\n    def __init__(self, input_size, output_size):\n        super(LinearRegression, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n\n    # Prediction function\n    def forward(self, x):\n        yhat = self.linear(x)\n        return yhat\n</code></pre> <p>Build a linear regression object. The input feature size is two. </p> <pre><code>model = LinearRegression(2, 1)\n</code></pre> <p>This will input the following equation:</p> <p></p> <p>You can see the randomly initialized parameters by using the <code>parameters()</code> method:</p> <pre><code># Print model parameters\n\nprint(\"The parameters: \", list(model.parameters()))\n</code></pre> <pre><code>The parameters:  [Parameter containing:\ntensor([[ 0.3319, -0.6657]], requires_grad=True), Parameter containing:\ntensor([0.4241], requires_grad=True)]\n</code></pre> <p>You can also see the parameters by using the <code>state_dict()</code> method:</p> <pre><code># Print model parameters\n\nprint(\"The parameters: \", model.state_dict())\n</code></pre> <pre><code>The parameters:  OrderedDict({'linear.weight': tensor([[ 0.3319, -0.6657]]), 'linear.bias': tensor([0.4241])})\n</code></pre> <p>Now we input a 1x2 tensor, and we will get a 1x1 tensor.</p> <pre><code># Make a prediction of x\n\nyhat = model(x)\nprint(\"The result: \", yhat)\n</code></pre> <pre><code>The result:  tensor([[-0.5754]], grad_fn=&lt;AddmmBackward0&gt;)\n</code></pre> <p>The shape of the output is shown in the following image: </p> <p></p> <p>Make a prediction for multiple samples:</p> <pre><code># Make a prediction of X\n\nyhat = model(X)\nprint(\"The result: \", yhat)\n</code></pre> <pre><code>The result:  tensor([[ 0.0903],\n        [-0.5754],\n        [-1.2411]], grad_fn=&lt;AddmmBackward0&gt;)\n</code></pre> <p>The shape is shown in the following image: </p> <p></p> Practice <p>Build a model or object of type <code>linear_regression</code>. Using the <code>linear_regression</code> object will predict the following tensor: </p> <pre><code># Practice: Build a model to predict the follow tensor.\n\nX = torch.tensor([[11.0, 12.0, 13, 14], [11, 12, 13, 14]])\n</code></pre> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.1.multiple_linear_regression_prediction_v2/#about-the-author","title":"About the Author:What's on your mind? Put it in the comments!","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.2.multiple_linear_regression_training_v2/","title":"Linear Regression Multiple Outputs","text":"Objective <ul><li> How to create a complicated models using pytorch build in functions.</li></ul> Table of Contents <p>In this lab, you will create a model the PyTroch way. This will help you more complicated models.</p> <ul> <li>Make Some Data</li> <li>Create the Model and Cost Function the PyTorch way</li> <li>Train the Model: Batch Gradient Descent</li> </ul> <p>Estimated Time Needed: 20 min</p>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> Preparation <p>We'll need the following libraries:</p> <pre><code># Import the libraries we need for this lab\n\nfrom torch import nn,optim\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom torch.utils.data import Dataset, DataLoader\n</code></pre> <p>Set the random seed:</p> <pre><code># Set the random seed to 1. \n\ntorch.manual_seed(1)\n</code></pre> <pre><code>&lt;torch._C.Generator at 0x7fa4f01ea330&gt;\n</code></pre> <p>Use this function for plotting: </p> <pre><code># The function for plotting 2D\n\ndef Plot_2D_Plane(model, dataset, n=0):\n    w1 = model.state_dict()['linear.weight'].numpy()[0][0]\n    w2 = model.state_dict()['linear.weight'].numpy()[0][1]\n    b = model.state_dict()['linear.bias'].numpy()\n\n    # Data\n    x1 = data_set.x[:, 0].view(-1, 1).numpy()\n    x2 = data_set.x[:, 1].view(-1, 1).numpy()\n    y = data_set.y.numpy()\n\n    # Make plane\n    X, Y = np.meshgrid(np.arange(x1.min(), x1.max(), 0.05), np.arange(x2.min(), x2.max(), 0.05))\n    yhat = w1 * X + w2 * Y + b\n\n    # Plotting\n    fig = plt.figure()\n    ax = fig.gca(projection='3d')\n\n    ax.plot(x1[:, 0], x2[:, 0], y[:, 0],'ro', label='y') # Scatter plot\n\n    ax.plot_surface(X, Y, yhat) # Plane plot\n\n    ax.set_xlabel('x1 ')\n    ax.set_ylabel('x2 ')\n    ax.set_zlabel('y')\n    plt.title('estimated plane iteration:' + str(n))\n    ax.legend()\n\n    plt.show()\n</code></pre>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.2.multiple_linear_regression_training_v2/# #Makeup_Data","title":"Make Some Data","text":"<p>Create a dataset class with two-dimensional features:</p> <pre><code># Create a 2D dataset\n\nclass Data2D(Dataset):\n\n    # Constructor\n    def __init__(self):\n        self.x = torch.zeros(20, 2)\n        self.x[:, 0] = torch.arange(-1, 1, 0.1)\n        self.x[:, 1] = torch.arange(-1, 1, 0.1)\n        self.w = torch.tensor([[1.0], [1.0]])\n        self.b = 1\n        self.f = torch.mm(self.x, self.w) + self.b    \n        self.y = self.f + 0.1 * torch.randn((self.x.shape[0],1))\n        self.len = self.x.shape[0]\n\n    # Getter\n    def __getitem__(self, index):          \n        return self.x[index], self.y[index]\n\n    # Get Length\n    def __len__(self):\n        return self.len\n</code></pre> <p>Create a dataset object:</p> <pre><code># Create the dataset object\n\ndata_set = Data2D()\n</code></pre>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.2.multiple_linear_regression_training_v2/#Model_Cost","title":"Create the Model, Optimizer, and Total Loss Function (Cost)","text":"<p>Create a customized linear regression module: </p> <pre><code># Create a customized linear\n\nclass LinearRegression(nn.Module):\n\n    # Constructor\n    def __init__(self, input_size, output_size):\n        super(LinearRegression, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n\n    # Prediction\n    def forward(self, x):\n        yhat = self.linear(x)\n        return yhat\n</code></pre> <p>Create a model. Use two features: make the input size 2 and the output size 1: </p> <pre><code># Create the linear regression model and print the parameters\n\nmodel = LinearRegression(2,1)\nprint(\"The parameters: \", list(model.parameters()))\n</code></pre> <pre><code>The parameters:  [Parameter containing:\ntensor([[-0.1540, -0.6489]], requires_grad=True), Parameter containing:\ntensor([0.4873], requires_grad=True)]\n</code></pre> <p>Create an optimizer  object. Set the learning rate to 0.1. Don't forget to enter the model parameters in the constructor.</p> <p></p> <pre><code># Create the optimizer\n\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n</code></pre> <p>Create the criterion function that calculates the total loss or cost:</p> <pre><code># Create the cost function\n\ncriterion = nn.MSELoss()\n</code></pre> <p>Create a data loader object. Set the batch_size equal to 2: </p> <pre><code># Create the data loader\n\ntrain_loader = DataLoader(dataset=data_set, batch_size=2)\n</code></pre>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.2.multiple_linear_regression_training_v2/#BGD","title":"Train the Model via Mini-Batch Gradient Descent","text":"<p>Run 100 epochs of Mini-Batch Gradient Descent and store the total loss or cost for every iteration. Remember that this is an approximation of the true total loss or cost:</p> <pre><code># Train the model\n\nLOSS = []\n# print(\"Before Training: \")\n# Plot_2D_Plane(model, data_set)   \nepochs = 5\n\ndef train_model(epochs):    \n    for epoch in range(epochs):\n        for x,y in train_loader:\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            LOSS.append(loss.item())\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()     \ntrain_model(epochs)\n</code></pre> <pre><code># Plot out the Loss and iteration diagram\n\nplt.plot(LOSS)\nplt.xlabel(\"Iterations \")\nplt.ylabel(\"Cost/total loss \")\n</code></pre> <pre><code>Text(0, 0.5, 'Cost/total loss ')\n</code></pre> <p></p> Practice <p>Create a new <code>model1</code>. Train the model with a batch size 30 and learning rate 0.1, store the loss or total cost in a list <code>LOSS1</code>, and plot the results.</p> <pre><code># Practice create model1. Train the model with batch size 30 and learning rate 0.1, store the loss in a list &lt;code&gt;LOSS1&lt;/code&gt;. Plot the results.\n\ndata_set = Data2D()\n</code></pre> <p>Double-click here for the solution.</p> <p>Use the following validation data to calculate the total loss or cost for both models:</p> <pre><code>torch.manual_seed(2)\n\nvalidation_data = Data2D()\nY = validation_data.y\nX = validation_data.x\n</code></pre>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.2.multiple_linear_regression_training_v2/#about-the-author","title":"About the Author:What's on your mind? Put it in the comments!","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.3.multi-target_linear_regression/","title":"Clear Linear","text":"Objective <ul><li> How to make a prediction using multiple samples.</li></ul>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.3.multi-target_linear_regression/#table-of-contents","title":"Table of Contents","text":"<p>In this lab, we will  review how to make a prediction for Linear Regression with Multiple Output.</p> <ul> <li> Build Custom Modules </li> <p></p> Estimated Time Needed: 15 min </ul> <p></p> Class Linear   <pre><code>from torch import nn\nimport torch\n</code></pre> <p>Set the random seed:</p> <pre><code>torch.manual_seed(1)\n</code></pre> <pre><code>&lt;torch._C.Generator at 0x7f4538043150&gt;\n</code></pre> <p>Set the random seed:</p> <pre><code>class linear_regression(nn.Module):\n    def __init__(self,input_size,output_size):\n        super(linear_regression,self).__init__()\n        self.linear=nn.Linear(input_size,output_size)\n    def forward(self,x):\n        yhat=self.linear(x)\n        return yhat\n</code></pre> <p>create a linear regression  object, as our input and output will be two we set the parameters accordingly </p> <pre><code>model=linear_regression(1,10)\nmodel(torch.tensor([1.0]))\n</code></pre> <pre><code>tensor([ 0.7926, -0.3920,  0.1714,  0.0797, -1.0143,  0.5097, -0.0608,  0.5047,\n         1.0132,  0.1887], grad_fn=&lt;ViewBackward0&gt;)\n</code></pre> <p>we can use the diagram to represent the model or object </p> <p></p> <p>we can see the parameters </p> <pre><code>list(model.parameters())\n</code></pre> <pre><code>[Parameter containing:\n tensor([[ 0.5153],\n         [-0.4414],\n         [-0.1939],\n         [ 0.4694],\n         [-0.9414],\n         [ 0.5997],\n         [-0.2057],\n         [ 0.5087],\n         [ 0.1390],\n         [-0.1224]], requires_grad=True),\n Parameter containing:\n tensor([ 0.2774,  0.0493,  0.3652, -0.3897, -0.0729, -0.0900,  0.1449, -0.0040,\n          0.8742,  0.3112], requires_grad=True)]\n</code></pre> <p>we can create a tensor with two rows representing one sample of data</p> <pre><code>x=torch.tensor([[1.0]])\n</code></pre> <p>we can make a prediction </p> <pre><code>yhat=model(x)\nyhat\n</code></pre> <pre><code>tensor([[ 0.7926, -0.3920,  0.1714,  0.0797, -1.0143,  0.5097, -0.0608,  0.5047,\n          1.0132,  0.1887]], grad_fn=&lt;AddmmBackward0&gt;)\n</code></pre> <p>each row in the following tensor represents a different sample </p> <pre><code>X=torch.tensor([[1.0],[1.0],[3.0]])\n</code></pre> <p>we can make a prediction using multiple samples </p> <pre><code>Yhat=model(X)\nYhat\n</code></pre> <pre><code>tensor([[ 0.7926, -0.3920,  0.1714,  0.0797, -1.0143,  0.5097, -0.0608,  0.5047,\n          1.0132,  0.1887],\n        [ 0.7926, -0.3920,  0.1714,  0.0797, -1.0143,  0.5097, -0.0608,  0.5047,\n          1.0132,  0.1887],\n        [ 1.8232, -1.2748, -0.2164,  1.0184, -2.8972,  1.7091, -0.4722,  1.5222,\n          1.2912, -0.0561]], grad_fn=&lt;AddmmBackward0&gt;)\n</code></pre> <p>the following figure represents the operation, where the red and blue  represents the different parameters, and the different shades of green represent  different samples.</p> <p></p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.3.multi-target_linear_regression/#about-the-author","title":"About the Author:What's on your mind? Put it in the comments!","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.4.training_multiple_output_linear_regression/","title":"Linear Regression Multiple Outputs","text":"Objective <ul><li> How to create a complicated models using pytorch build in functions.</li></ul> Table of Contents <p>In this lab, you will create a model the Pytroch way. This will help you as models get more complicated.</p> <ul> <li> Make Some Data</li> <li> Create the Model and Cost Function the Pytorch way</li> <li> Train the Model: Batch Gradient Descent</li> <li> Practice Questions </li> <p></p> Estimated Time Needed: 20 min </ul> <p>Import the following libraries:  </p> <pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch import nn,optim\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\n</code></pre> <p>Set the random seed:</p> <pre><code>torch.manual_seed(1)\n</code></pre> <pre><code>&lt;torch._C.Generator at 0x7fa794177110&gt;\n</code></pre> <p></p> Make Some Data  <p>Create a dataset class with two-dimensional features and two targets: </p> <pre><code>from torch.utils.data import Dataset, DataLoader\nclass Data(Dataset):\n    def __init__(self):\n            self.x=torch.zeros(20,2)\n            self.x[:,0]=torch.arange(-1,1,0.1)\n            self.x[:,1]=torch.arange(-1,1,0.1)\n            self.w=torch.tensor([ [1.0,-1.0],[1.0,3.0]])\n            self.b=torch.tensor([[1.0,-1.0]])\n            self.f=torch.mm(self.x,self.w)+self.b\n\n            self.y=self.f+0.001*torch.randn((self.x.shape[0],1))\n            self.len=self.x.shape[0]\n\n    def __getitem__(self,index):\n\n        return self.x[index],self.y[index]\n\n    def __len__(self):\n        return self.len\n</code></pre> <p>create a dataset object </p> <pre><code>data_set=Data()\n</code></pre> <p></p> Create the Model, Optimizer, and Total Loss Function (cost) <p>Create a custom module:</p> <pre><code>class linear_regression(nn.Module):\n    def __init__(self,input_size,output_size):\n        super(linear_regression,self).__init__()\n        self.linear=nn.Linear(input_size,output_size)\n    def forward(self,x):\n        yhat=self.linear(x)\n        return yhat\n</code></pre> <p>Create an optimizer object and set the learning rate to 0.1. Don't forget to enter the model parameters in the constructor. </p> <pre><code>model=linear_regression(2,2)\n</code></pre> <p>Create an optimizer object and set the learning rate to 0.1. Don't forget to enter the model parameters in the constructor. </p> <p></p> <pre><code>optimizer = optim.SGD(model.parameters(), lr = 0.1)\n</code></pre> <p>Create the criterion function that calculates the total loss or cost:</p> <pre><code>criterion = nn.MSELoss()\n</code></pre> <p>Create a data loader object and set the batch_size to 5:</p> <pre><code>train_loader=DataLoader(dataset=data_set,batch_size=5)\n</code></pre> <p></p> Train the Model via Mini-Batch Gradient Descent  <p>Run 100 epochs of Mini-Batch Gradient Descent and store the total loss or cost for every iteration. Remember that this is an approximation of the true total loss or cost.</p> <pre><code>LOSS=[]\n\nepochs=100\n\nfor epoch in range(epochs):\n    for x,y in train_loader:\n        #make a prediction \n        yhat=model(x)\n        #calculate the loss\n        loss=criterion(yhat,y)\n        #store loss/cost \n        LOSS.append(loss.item())\n        #clear gradient \n        optimizer.zero_grad()\n        #Backward pass: compute gradient of the loss with respect to all the learnable parameters\n        loss.backward()\n        #the step function on an Optimizer makes an update to its parameters\n        optimizer.step()\n</code></pre> <p>Plot the cost:</p> <pre><code>plt.plot(LOSS)\nplt.xlabel(\"iterations \")\nplt.ylabel(\"Cost/total loss \")\nplt.show()\n</code></pre> <p></p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/4.4.training_multiple_output_linear_regression/#about-the-author","title":"About the Author:What's on your mind? Put it in the comments!","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/muliple_input_lr_exercise/","title":"Multiple Input LR Exercise","text":""},{"location":"Deep%20Learning/Week4-Multiple-Input-LR/muliple_input_lr_exercise/#week-4-multiple-input-linear-regression","title":"Week 4: Multiple Input Linear Regression","text":"<ol> <li>Making Predictions in Multiple Linear Regression</li> <li> <p>Exercise: Implement a multiple input linear regression model to predict a target variable based on two or more input features. Train the model on synthetic data and evaluate its performance.</p> </li> <li> <p>Training Multiple Linear Regression Models</p> </li> <li> <p>Exercise: Train multiple linear regression models with different feature sets and compare their performance. Analyze how the inclusion or exclusion of features affects the model's predictions.</p> </li> <li> <p>Multi-Target Linear Regression</p> </li> <li>Exercise: Extend your linear regression model to predict multiple target variables simultaneously. Create a dataset with multiple targets and train the model to predict all targets.</li> </ol>  Don't Miss Any Updates! <p> Before the last question, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <ol> <li>Training Multiple Output Linear Regression Models</li> <li>Exercise: Implement a model to handle multiple outputs (e.g., predicting multiple continuous variables). Train the model on a dataset with multiple output variables and assess its performance.</li> </ol> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.1logistic_regression_prediction_v2/","title":"Prediction","text":"Objective<ul><li> How to create a logistic regression object with the nn.Sequential model.</li></ul> Table of Contents <p>In this lab, we will cover logistic regression using PyTorch.</p> <ul> <li> Logistic Function</li> <li> Build a Logistic Regression Using nn.Sequential</li> <li> Build Custom Modules</li> </ul> <p>Estimated Time Needed: 15 min</p> Preparation <p>We'll need the following libraries:</p> In\u00a0[1]: Copied! <pre># Import the libraries we need for this lab\n\nimport torch.nn as nn\nimport torch\nimport matplotlib.pyplot as plt \n</pre> # Import the libraries we need for this lab  import torch.nn as nn import torch import matplotlib.pyplot as plt  <p>Set the random seed:</p> In\u00a0[2]: Copied! <pre># Set the random seed\n\ntorch.manual_seed(2)\n</pre> # Set the random seed  torch.manual_seed(2) Out[2]: <pre>&lt;torch._C.Generator at 0x714cbc14af50&gt;</pre> <p>Create a tensor ranging from -100 to 100:</p> In\u00a0[3]: Copied! <pre>z = torch.arange(-100, 100, 0.1).view(-1, 1)\nprint(\"The tensor: \", z)\n</pre> z = torch.arange(-100, 100, 0.1).view(-1, 1) print(\"The tensor: \", z) <pre>The tensor:  tensor([[-100.0000],\n        [ -99.9000],\n        [ -99.8000],\n        ...,\n        [  99.7000],\n        [  99.8000],\n        [  99.9000]])\n</pre> <p>Create a sigmoid object:</p> In\u00a0[5]: Copied! <pre># Create sigmoid object\n\nsig = nn.Sigmoid()\n</pre> # Create sigmoid object  sig = nn.Sigmoid() <p>Apply the element-wise function Sigmoid with the object:</p> In\u00a0[6]: Copied! <pre># Use sigmoid object to calculate the \n\nyhat = sig(z)\n</pre> # Use sigmoid object to calculate the   yhat = sig(z) <p>Plot the results:</p> In\u00a0[7]: Copied! <pre>plt.plot(z.numpy(), yhat.numpy())\nplt.xlabel('z')\nplt.ylabel('yhat')\n</pre> plt.plot(z.numpy(), yhat.numpy()) plt.xlabel('z') plt.ylabel('yhat') Out[7]: <pre>Text(0, 0.5, 'yhat')</pre> <p>Apply the element-wise Sigmoid from the function module and plot the results:</p> In\u00a0[8]: Copied! <pre>yhat = torch.sigmoid(z)\nplt.plot(z.numpy(), yhat.numpy())\n</pre> yhat = torch.sigmoid(z) plt.plot(z.numpy(), yhat.numpy()) Out[8]: <pre>[&lt;matplotlib.lines.Line2D at 0x714c557a9a30&gt;]</pre> <p>Create a 1x1 tensor where x represents one data sample with one dimension, and 2x1 tensor X represents two data samples of one dimension:</p> In\u00a0[9]: Copied! <pre># Create x and X tensor\n\nx = torch.tensor([[1.0]])\nX = torch.tensor([[1.0], [100]])\nprint('x = ', x)\nprint('X = ', X)\n</pre> # Create x and X tensor  x = torch.tensor([[1.0]]) X = torch.tensor([[1.0], [100]]) print('x = ', x) print('X = ', X) <pre>x =  tensor([[1.]])\nX =  tensor([[  1.],\n        [100.]])\n</pre> <p>Create a logistic regression object with the <code>nn.Sequential</code> model with a one-dimensional input:</p> In\u00a0[10]: Copied! <pre># Use sequential function to create model\n\nmodel = nn.Sequential(nn.Linear(1, 1), nn.Sigmoid())\n</pre> # Use sequential function to create model  model = nn.Sequential(nn.Linear(1, 1), nn.Sigmoid()) <p>The object is represented in the following diagram:</p> <p>In this case, the parameters are randomly initialized. You can view them the following ways:</p> In\u00a0[30]: Copied! <pre># Print the parameters\n\nprint(\"list(model.parameters()):\\n \", list(model.parameters()))\nprint(\"\\nmodel.state_dict():\\n \", model.state_dict())\n</pre> # Print the parameters  print(\"list(model.parameters()):\\n \", list(model.parameters())) print(\"\\nmodel.state_dict():\\n \", model.state_dict()) <pre>list(model.parameters()):\n  [Parameter containing:\ntensor([[-0.5717,  0.1614]], requires_grad=True), Parameter containing:\ntensor([-0.6260], requires_grad=True)]\n\nmodel.state_dict():\n  OrderedDict({'linear.weight': tensor([[-0.5717,  0.1614]]), 'linear.bias': tensor([-0.6260])})\n</pre> <p>Make a prediction with one sample:</p> In\u00a0[31]: Copied! <pre># The prediction for x\n\nyhat = model(x)\nprint(\"The prediction: \", yhat)\n</pre> # The prediction for x  yhat = model(x) print(\"The prediction: \", yhat) <pre>The prediction:  tensor([[0.2943]], grad_fn=&lt;SigmoidBackward0&gt;)\n</pre> <p>Calling the object with tensor <code>X</code> performed the following operation (code values may not be the same as the diagrams value  depending on the version of PyTorch) :</p> <p>Make a prediction with multiple samples:</p> In\u00a0[13]: Copied! <pre># The prediction for X\n\nyhat = model(X)\nyhat\n</pre> # The prediction for X  yhat = model(X) yhat Out[13]: <pre>tensor([[0.4979],\n        [1.0000]], grad_fn=&lt;SigmoidBackward0&gt;)</pre> <p>Calling the object performed the following operation:</p> <p>Create a 1x2 tensor where x represents one data sample with one dimension, and 2x3 tensor X represents one data sample of two dimensions:</p> In\u00a0[14]: Copied! <pre># Create and print samples\n\nx = torch.tensor([[1.0, 1.0]])\nX = torch.tensor([[1.0, 1.0], [1.0, 2.0], [1.0, 3.0]])\nprint('x = ', x)\nprint('X = ', X)\n</pre> # Create and print samples  x = torch.tensor([[1.0, 1.0]]) X = torch.tensor([[1.0, 1.0], [1.0, 2.0], [1.0, 3.0]]) print('x = ', x) print('X = ', X) <pre>x =  tensor([[1., 1.]])\nX =  tensor([[1., 1.],\n        [1., 2.],\n        [1., 3.]])\n</pre> <p>Create a logistic regression object with the <code>nn.Sequential</code> model with a two-dimensional input:</p> In\u00a0[15]: Copied! <pre># Create new model using nn.sequential()\n\nmodel = nn.Sequential(nn.Linear(2, 1), nn.Sigmoid())\n</pre> # Create new model using nn.sequential()  model = nn.Sequential(nn.Linear(2, 1), nn.Sigmoid()) <p>The object will apply the Sigmoid function to the output of the linear function as shown in the following diagram:</p> <p>In this case, the parameters are randomly initialized. You can view them the following ways:</p> In\u00a0[16]: Copied! <pre># Print the parameters\n\nprint(\"list(model.parameters()):\\n \", list(model.parameters()))\nprint(\"\\nmodel.state_dict():\\n \", model.state_dict())\n</pre> # Print the parameters  print(\"list(model.parameters()):\\n \", list(model.parameters())) print(\"\\nmodel.state_dict():\\n \", model.state_dict()) <pre>list(model.parameters()):\n  [Parameter containing:\ntensor([[ 0.1939, -0.0361]], requires_grad=True), Parameter containing:\ntensor([0.3021], requires_grad=True)]\n\nmodel.state_dict():\n  OrderedDict({'0.weight': tensor([[ 0.1939, -0.0361]]), '0.bias': tensor([0.3021])})\n</pre> <p>Make a prediction with one sample:</p> In\u00a0[17]: Copied! <pre># Make the prediction of x\n\nyhat = model(x)\nprint(\"The prediction: \", yhat)\n</pre> # Make the prediction of x  yhat = model(x) print(\"The prediction: \", yhat) <pre>The prediction:  tensor([[0.6130]], grad_fn=&lt;SigmoidBackward0&gt;)\n</pre> <p>The operation is represented in the following diagram:</p> <p>Make a prediction with multiple samples:</p> In\u00a0[18]: Copied! <pre># The prediction of X\n\nyhat = model(X)\nprint(\"The prediction: \", yhat)\n</pre> # The prediction of X  yhat = model(X) print(\"The prediction: \", yhat) <pre>The prediction:  tensor([[0.6130],\n        [0.6044],\n        [0.5957]], grad_fn=&lt;SigmoidBackward0&gt;)\n</pre> <p>The operation is represented in the following diagram:</p> <p>In this section, you will build a custom Module or class. The model or object function is identical to using <code>nn.Sequential</code>.</p> <p>Create a logistic regression custom module:</p> In\u00a0[19]: Copied! <pre># Create logistic_regression custom class\n\nclass logistic_regression(nn.Module):\n    \n    # Constructor\n    def __init__(self, n_inputs):\n        super(logistic_regression, self).__init__()\n        self.linear = nn.Linear(n_inputs, 1)\n    \n    # Prediction\n    def forward(self, x):\n        yhat = torch.sigmoid(self.linear(x))\n        return yhat\n</pre> # Create logistic_regression custom class  class logistic_regression(nn.Module):          # Constructor     def __init__(self, n_inputs):         super(logistic_regression, self).__init__()         self.linear = nn.Linear(n_inputs, 1)          # Prediction     def forward(self, x):         yhat = torch.sigmoid(self.linear(x))         return yhat <p>Create a 1x1 tensor where x represents one data sample with one dimension, and 3x1 tensor where $X$ represents one data sample of one dimension:</p> In\u00a0[20]: Copied! <pre># Create x and X tensor\n\nx = torch.tensor([[1.0]])\nX = torch.tensor([[-100], [0], [100.0]])\nprint('x = ', x)\nprint('X = ', X)\n</pre> # Create x and X tensor  x = torch.tensor([[1.0]]) X = torch.tensor([[-100], [0], [100.0]]) print('x = ', x) print('X = ', X) <pre>x =  tensor([[1.]])\nX =  tensor([[-100.],\n        [   0.],\n        [ 100.]])\n</pre> <p>Create a model to predict one dimension:</p> In\u00a0[21]: Copied! <pre># Create logistic regression model\n\nmodel = logistic_regression(1)\n</pre> # Create logistic regression model  model = logistic_regression(1) <p>In this case, the parameters are randomly initialized. You can view them the following ways:</p> In\u00a0[22]: Copied! <pre># Print parameters \n\nprint(\"list(model.parameters()):\\n \", list(model.parameters()))\nprint(\"\\nmodel.state_dict():\\n \", model.state_dict())\n</pre> # Print parameters   print(\"list(model.parameters()):\\n \", list(model.parameters())) print(\"\\nmodel.state_dict():\\n \", model.state_dict()) <pre>list(model.parameters()):\n  [Parameter containing:\ntensor([[0.2381]], requires_grad=True), Parameter containing:\ntensor([-0.1149], requires_grad=True)]\n\nmodel.state_dict():\n  OrderedDict({'linear.weight': tensor([[0.2381]]), 'linear.bias': tensor([-0.1149])})\n</pre> <p>Make a prediction with one sample:</p> In\u00a0[23]: Copied! <pre># Make the prediction of x\n\nyhat = model(x)\nprint(\"The prediction result: \\n\", yhat)\n</pre> # Make the prediction of x  yhat = model(x) print(\"The prediction result: \\n\", yhat) <pre>The prediction result: \n tensor([[0.5307]], grad_fn=&lt;SigmoidBackward0&gt;)\n</pre> <p>Make a prediction with multiple samples:</p> In\u00a0[24]: Copied! <pre># Make the prediction of X\n\nyhat = model(X)\nprint(\"The prediction result: \\n\", yhat)\n</pre> # Make the prediction of X  yhat = model(X) print(\"The prediction result: \\n\", yhat) <pre>The prediction result: \n tensor([[4.0805e-11],\n        [4.7130e-01],\n        [1.0000e+00]], grad_fn=&lt;SigmoidBackward0&gt;)\n</pre> <p>Create a logistic regression object with a function with two inputs:</p> In\u00a0[25]: Copied! <pre># Create logistic regression model\n\nmodel = logistic_regression(2)\n</pre> # Create logistic regression model  model = logistic_regression(2) <p>Create a 1x2 tensor where x represents one data sample with one dimension, and 3x2 tensor X represents one data sample of one dimension:</p> In\u00a0[26]: Copied! <pre># Create x and X tensor\n\nx = torch.tensor([[1.0, 2.0]])\nX = torch.tensor([[100, -100], [0.0, 0.0], [-100, 100]])\nprint('x = ', x)\nprint('X = ', X)\n</pre> # Create x and X tensor  x = torch.tensor([[1.0, 2.0]]) X = torch.tensor([[100, -100], [0.0, 0.0], [-100, 100]]) print('x = ', x) print('X = ', X) <pre>x =  tensor([[1., 2.]])\nX =  tensor([[ 100., -100.],\n        [   0.,    0.],\n        [-100.,  100.]])\n</pre> <p>Make a prediction with one sample:</p> In\u00a0[27]: Copied! <pre># Make the prediction of x\n\nyhat = model(x)\nprint(\"The prediction result: \\n\", yhat)\n</pre> # Make the prediction of x  yhat = model(x) print(\"The prediction result: \\n\", yhat) <pre>The prediction result: \n tensor([[0.2943]], grad_fn=&lt;SigmoidBackward0&gt;)\n</pre> <p>Make a prediction with multiple samples:</p> In\u00a0[28]: Copied! <pre># Make the prediction of X\n\nyhat = model(X)\nprint(\"The prediction result: \\n\", yhat)\n</pre> # Make the prediction of X  yhat = model(X) print(\"The prediction result: \\n\", yhat) <pre>The prediction result: \n tensor([[7.7529e-33],\n        [3.4841e-01],\n        [1.0000e+00]], grad_fn=&lt;SigmoidBackward0&gt;)\n</pre> Practice <p>Make your own model <code>my_model</code> as applying linear regression first and then logistic regression using <code>nn.Sequential()</code>. Print out your prediction.</p> In\u00a0[29]: Copied! <pre># Practice: Make your model and make the prediction\n\nX = torch.tensor([-10.0])\n\nmy_model = nn.Sequential(nn.Linear(1, 1), nn.Sigmoid())\n\nmy_model(X)\n</pre> # Practice: Make your model and make the prediction  X = torch.tensor([-10.0])  my_model = nn.Sequential(nn.Linear(1, 1), nn.Sigmoid())  my_model(X) Out[29]: <pre>tensor([0.2231], grad_fn=&lt;SigmoidBackward0&gt;)</pre> <p>Double-click here for the solution.</p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.1logistic_regression_prediction_v2/#author-juma-shafara-date-2024-08-08-title-logistic-regression-keywords-training-two-parameter-mini-batch-gradient-decent-training-two-parameter-mini-batch-gradient-decent-description-in-this-lab-we-will-cover-logistic-regression-using-pytorch","title":"author: Juma Shafara date: \"2024-08-08\" title: Logistic Regression keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, we will cover logistic regression using PyTorch.\u00b6","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.1logistic_regression_prediction_v2/#Log","title":"Logistic Function","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.1logistic_regression_prediction_v2/#Seq","title":"Build a Logistic Regression with <code>nn.Sequential</code>","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.1logistic_regression_prediction_v2/#Model","title":"Build Custom Modules","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.1logistic_regression_prediction_v2/#about-the-author","title":"About the Author:\u00b6What's on your mind? Put it in the comments!","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.2.2bad_inshilization_logistic_regression_with_mean_square_error_v2/","title":"MSE Issues","text":"Objective<ul><li> How bad initialization value can affects the accuracy of model. .</li></ul> <p>We'll need the following libraries:</p> In\u00a0[1]: Copied! <pre># Import the libraries we need for this lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nfrom mpl_toolkits import mplot3d\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n</pre> # Import the libraries we need for this lab  import numpy as np import matplotlib.pyplot as plt  from mpl_toolkits import mplot3d import torch from torch.utils.data import Dataset, DataLoader import torch.nn as nn <p>The class <code>plot_error_surfaces</code> is just to help you visualize the data space and the Parameter space during training and has nothing to do with Pytorch.</p> In\u00a0[2]: Copied! <pre># Create class for plotting and the function for plotting\n\nclass plot_error_surfaces(object):\n    \n    # Construstor\n    def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True):\n        W = np.linspace(-w_range, w_range, n_samples)\n        B = np.linspace(-b_range, b_range, n_samples)\n        w, b = np.meshgrid(W, B)    \n        Z = np.zeros((30, 30))\n        count1 = 0\n        self.y = Y.numpy()\n        self.x = X.numpy()\n        for w1, b1 in zip(w, b):\n            count2 = 0\n            for w2, b2 in zip(w1, b1):\n                Z[count1, count2] = np.mean((self.y - (1 / (1 + np.exp(-1*w2 * self.x - b2)))) ** 2)\n                count2 += 1   \n            count1 += 1\n        self.Z = Z\n        self.w = w\n        self.b = b\n        self.W = []\n        self.B = []\n        self.LOSS = []\n        self.n = 0\n        if go == True:\n            plt.figure()\n            plt.figure(figsize=(7.5, 5))\n            plt.axes(projection='3d').plot_surface(self.w, self.b, self.Z, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n            plt.title('Loss Surface')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.show()\n            plt.figure()\n            plt.title('Loss Surface Contour')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.contour(self.w, self.b, self.Z)\n            plt.show()\n            \n     # Setter\n    def set_para_loss(self, model, loss):\n        self.n = self.n + 1\n        self.W.append(list(model.parameters())[0].item())\n        self.B.append(list(model.parameters())[1].item())\n        self.LOSS.append(loss)\n    \n    # Plot diagram\n    def final_plot(self): \n        ax = plt.axes(projection='3d')\n        ax.plot_wireframe(self.w, self.b, self.Z)\n        ax.scatter(self.W, self.B, self.LOSS, c='r', marker='x', s=200, alpha=1)\n        plt.figure()\n        plt.contour(self.w, self.b, self.Z)\n        plt.scatter(self.W, self.B, c='r', marker='x')\n        plt.xlabel('w')\n        plt.ylabel('b')\n        plt.show()\n        \n    # Plot diagram\n    def plot_ps(self):\n        plt.subplot(121)\n        plt.ylim\n        plt.plot(self.x, self.y, 'ro', label=\"training points\")\n        plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label=\"estimated line\")\n        plt.plot(self.x, 1 / (1 + np.exp(-1 * (self.W[-1] * self.x + self.B[-1]))), label='sigmoid')\n        plt.xlabel('x')\n        plt.ylabel('y')\n        plt.ylim((-0.1, 2))\n        plt.title('Data Space Iteration: ' + str(self.n))\n        plt.show()\n        plt.subplot(122)\n        plt.contour(self.w, self.b, self.Z)\n        plt.scatter(self.W, self.B, c='r', marker='x')\n        plt.title('Loss Surface Contour Iteration' + str(self.n))\n        plt.xlabel('w')\n        plt.ylabel('b')\n        \n# Plot the diagram\n\ndef PlotStuff(X, Y, model, epoch, leg=True):\n    plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch)))\n    plt.plot(X.numpy(), Y.numpy(), 'r')\n    if leg == True:\n        plt.legend()\n    else:\n        pass\n</pre> # Create class for plotting and the function for plotting  class plot_error_surfaces(object):          # Construstor     def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True):         W = np.linspace(-w_range, w_range, n_samples)         B = np.linspace(-b_range, b_range, n_samples)         w, b = np.meshgrid(W, B)             Z = np.zeros((30, 30))         count1 = 0         self.y = Y.numpy()         self.x = X.numpy()         for w1, b1 in zip(w, b):             count2 = 0             for w2, b2 in zip(w1, b1):                 Z[count1, count2] = np.mean((self.y - (1 / (1 + np.exp(-1*w2 * self.x - b2)))) ** 2)                 count2 += 1                count1 += 1         self.Z = Z         self.w = w         self.b = b         self.W = []         self.B = []         self.LOSS = []         self.n = 0         if go == True:             plt.figure()             plt.figure(figsize=(7.5, 5))             plt.axes(projection='3d').plot_surface(self.w, self.b, self.Z, rstride=1, cstride=1, cmap='viridis', edgecolor='none')             plt.title('Loss Surface')             plt.xlabel('w')             plt.ylabel('b')             plt.show()             plt.figure()             plt.title('Loss Surface Contour')             plt.xlabel('w')             plt.ylabel('b')             plt.contour(self.w, self.b, self.Z)             plt.show()                   # Setter     def set_para_loss(self, model, loss):         self.n = self.n + 1         self.W.append(list(model.parameters())[0].item())         self.B.append(list(model.parameters())[1].item())         self.LOSS.append(loss)          # Plot diagram     def final_plot(self):          ax = plt.axes(projection='3d')         ax.plot_wireframe(self.w, self.b, self.Z)         ax.scatter(self.W, self.B, self.LOSS, c='r', marker='x', s=200, alpha=1)         plt.figure()         plt.contour(self.w, self.b, self.Z)         plt.scatter(self.W, self.B, c='r', marker='x')         plt.xlabel('w')         plt.ylabel('b')         plt.show()              # Plot diagram     def plot_ps(self):         plt.subplot(121)         plt.ylim         plt.plot(self.x, self.y, 'ro', label=\"training points\")         plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label=\"estimated line\")         plt.plot(self.x, 1 / (1 + np.exp(-1 * (self.W[-1] * self.x + self.B[-1]))), label='sigmoid')         plt.xlabel('x')         plt.ylabel('y')         plt.ylim((-0.1, 2))         plt.title('Data Space Iteration: ' + str(self.n))         plt.show()         plt.subplot(122)         plt.contour(self.w, self.b, self.Z)         plt.scatter(self.W, self.B, c='r', marker='x')         plt.title('Loss Surface Contour Iteration' + str(self.n))         plt.xlabel('w')         plt.ylabel('b')          # Plot the diagram  def PlotStuff(X, Y, model, epoch, leg=True):     plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch)))     plt.plot(X.numpy(), Y.numpy(), 'r')     if leg == True:         plt.legend()     else:         pass <p>Set the random seed:</p> In\u00a0[3]: Copied! <pre># Set random seed\n\ntorch.manual_seed(0)\n</pre> # Set random seed  torch.manual_seed(0) Out[3]: <pre>&lt;torch._C.Generator at 0x7c89b4a521f0&gt;</pre> <p>Create the <code>Data</code> class</p> In\u00a0[4]: Copied! <pre># Create the data class\n\nclass Data(Dataset):\n    \n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-1, 1, 0.1).view(-1, 1)\n        self.y = torch.zeros(self.x.shape[0], 1)\n        self.y[self.x[:, 0] &gt; 0.2] = 1\n        self.len = self.x.shape[0]\n        \n    # Getter\n    def __getitem__(self, index):      \n        return self.x[index], self.y[index]\n    \n    # Get Length\n    def __len__(self):\n        return self.len\n</pre> # Create the data class  class Data(Dataset):          # Constructor     def __init__(self):         self.x = torch.arange(-1, 1, 0.1).view(-1, 1)         self.y = torch.zeros(self.x.shape[0], 1)         self.y[self.x[:, 0] &gt; 0.2] = 1         self.len = self.x.shape[0]              # Getter     def __getitem__(self, index):               return self.x[index], self.y[index]          # Get Length     def __len__(self):         return self.len <p>Make <code>Data</code> object</p> In\u00a0[5]: Copied! <pre># Create Data object\n\ndata_set = Data()\n</pre> # Create Data object  data_set = Data() <p>Create a custom module for logistic regression:</p> In\u00a0[6]: Copied! <pre># Create logistic_regression class\n\nclass logistic_regression(nn.Module):\n    \n    # Constructor\n    def __init__(self, n_inputs):\n        super(logistic_regression, self).__init__()\n        self.linear = nn.Linear(n_inputs, 1)\n        \n    # Prediction\n    def forward(self, x):\n        yhat = torch.sigmoid(self.linear(x))\n        return yhat\n</pre> # Create logistic_regression class  class logistic_regression(nn.Module):          # Constructor     def __init__(self, n_inputs):         super(logistic_regression, self).__init__()         self.linear = nn.Linear(n_inputs, 1)              # Prediction     def forward(self, x):         yhat = torch.sigmoid(self.linear(x))         return yhat <p>Create a logistic regression object or model:</p> In\u00a0[7]: Copied! <pre># Create the logistic_regression result\n\nmodel = logistic_regression(1)\n</pre> # Create the logistic_regression result  model = logistic_regression(1) <p>Replace the random initialized variable values with some predetermined values that will not converge:</p> In\u00a0[8]: Copied! <pre># Set the weight and bias\n\nmodel.state_dict() ['linear.weight'].data[0] = torch.tensor([[-5]])\nmodel.state_dict() ['linear.bias'].data[0] = torch.tensor([[-10]])\nprint(\"The parameters: \", model.state_dict())\n</pre> # Set the weight and bias  model.state_dict() ['linear.weight'].data[0] = torch.tensor([[-5]]) model.state_dict() ['linear.bias'].data[0] = torch.tensor([[-10]]) print(\"The parameters: \", model.state_dict()) <pre>The parameters:  OrderedDict({'linear.weight': tensor([[-5.]]), 'linear.bias': tensor([-10.])})\n</pre> <p>Create a <code> plot_error_surfaces</code> object to visualize the data space and the parameter space during training:</p> In\u00a0[9]: Copied! <pre># Create the plot_error_surfaces object\n\nget_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1], 30)\n</pre> # Create the plot_error_surfaces object  get_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1], 30) <pre>&lt;Figure size 640x480 with 0 Axes&gt;</pre> <p>Define the dataloader, the cost or criterion function, the optimizer:</p> In\u00a0[10]: Copied! <pre># Create dataloader object, crierion function and optimizer.\n\ntrainloader = DataLoader(dataset=data_set, batch_size=3)\ncriterion_rms = nn.MSELoss()\nlearning_rate = 2\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n</pre> # Create dataloader object, crierion function and optimizer.  trainloader = DataLoader(dataset=data_set, batch_size=3) criterion_rms = nn.MSELoss() learning_rate = 2 optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) <p></p> Train the Model via Batch Gradient Descent  <p>Train the model</p> In\u00a0[11]: Copied! <pre># Train the model\n\ndef train_model(epochs):\n    for epoch in range(epochs):\n        for x, y in trainloader: \n            yhat = model(x)\n            loss = criterion_rms(yhat, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            get_surface.set_para_loss(model, loss.tolist())\n        if epoch % 20 == 0:\n            get_surface.plot_ps()\n\ntrain_model(100)\n</pre> # Train the model  def train_model(epochs):     for epoch in range(epochs):         for x, y in trainloader:              yhat = model(x)             loss = criterion_rms(yhat, y)             optimizer.zero_grad()             loss.backward()             optimizer.step()             get_surface.set_para_loss(model, loss.tolist())         if epoch % 20 == 0:             get_surface.plot_ps()  train_model(100) <p>Get the actual class of each sample and calculate the accuracy on the test data:</p> In\u00a0[12]: Copied! <pre># Make the Prediction\n\nyhat = model(data_set.x)\nlabel = yhat &gt; 0.5\nprint(\"The accuracy: \", torch.mean((label == data_set.y.type(torch.ByteTensor)).type(torch.float)))\n</pre> # Make the Prediction  yhat = model(data_set.x) label = yhat &gt; 0.5 print(\"The accuracy: \", torch.mean((label == data_set.y.type(torch.ByteTensor)).type(torch.float))) <pre>The accuracy:  tensor(0.6500)\n</pre> <p>Accuracy is 60% compared to 100% in the last lab using a good Initialization value.</p> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.2.2bad_inshilization_logistic_regression_with_mean_square_error_v2/#author-juma-shafara-date-2024-08-12-title-logistic-regression-and-bad-initialization-value-keywords-training-two-parameter-mini-batch-gradient-decent-training-two-parameter-mini-batch-gradient-decent-description-in-this-lab-you-will-see-what-happens-when-you-use-the-root-mean-square-error-cost-or-total-loss-function-and-select-a-bad-initialization-value-for-the-parameter-values","title":"author: Juma Shafara date: \"2024-08-12\" title: Logistic Regression and Bad Initialization Value keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will see what happens when you use the root mean square error cost or total loss function and select a bad initialization value for the parameter values.\u00b6","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.2.2bad_inshilization_logistic_regression_with_mean_square_error_v2/#table-of-contents","title":"Table of Contents\u00b6","text":"<p>In this lab, you will see what happens when you use the root mean square error cost or total loss function and select a bad initialization value for the parameter values.</p> <ul> <li> Make Some Data</li> <li> Create the Model and Cost Function the PyTorch way</li> <li> Train the Model:Batch Gradient Descent</li> </ul> <p>Estimated Time Needed: 30 min</p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.2.2bad_inshilization_logistic_regression_with_mean_square_error_v2/#preparation","title":"Preparation\u00b6","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.2.2bad_inshilization_logistic_regression_with_mean_square_error_v2/#helper-functions","title":"Helper functions\u00b6","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.2.2bad_inshilization_logistic_regression_with_mean_square_error_v2/#Makeup_Data","title":"Get Some Data","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.2.2bad_inshilization_logistic_regression_with_mean_square_error_v2/#Model_Cost","title":"Create the Model and Total Loss Function (Cost)","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.2.2bad_inshilization_logistic_regression_with_mean_square_error_v2/#about-the-author","title":"About the Author:\u00b6","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.3_cross_entropy_logistic_regression_v2/","title":"Cross Entropy","text":"Objective<ul><li> How Cross-Entropy using random initialization influence the accuracy of the model.</li></ul> Table of Contents <p>In this lab, you will see what happens when you use the Cross-Entropy or total loss function using random initialization for a parameter value.</p> <ul> <li> Make Some Data</li> <li> Create the Model and Cost Function the PyTorch way </li> <li> Train the Model: Batch Gradient Descent</li> </ul> <p>Estimated Time Needed: 30 min</p> Preparation <p>We'll need the following libraries:</p> In\u00a0[1]: Copied! <pre># Import the libraries we need for this lab\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nfrom mpl_toolkits import mplot3d\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n</pre> # Import the libraries we need for this lab  import numpy as np import matplotlib.pyplot as plt  from mpl_toolkits import mplot3d import torch from torch.utils.data import Dataset, DataLoader import torch.nn as nn <p>The class <code>plot_error_surfaces</code> is just to help you visualize the data space and the parameter space during training and has nothing to do with Pytorch.</p> In\u00a0[2]: Copied! <pre># Create class for plotting and the function for plotting\n\nclass plot_error_surfaces(object):\n    \n    # Construstor\n    def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True):\n        W = np.linspace(-w_range, w_range, n_samples)\n        B = np.linspace(-b_range, b_range, n_samples)\n        w, b = np.meshgrid(W, B)    \n        Z = np.zeros((30, 30))\n        count1 = 0\n        self.y = Y.numpy()\n        self.x = X.numpy()\n        for w1, b1 in zip(w, b):\n            count2 = 0\n            for w2, b2 in zip(w1, b1):\n                yhat= 1 / (1 + np.exp(-1*(w2*self.x+b2)))\n                Z[count1,count2]=-1*np.mean(self.y*np.log(yhat+1e-16) +(1-self.y)*np.log(1-yhat+1e-16))\n                count2 += 1   \n            count1 += 1\n        self.Z = Z\n        self.w = w\n        self.b = b\n        self.W = []\n        self.B = []\n        self.LOSS = []\n        self.n = 0\n        if go == True:\n            plt.figure()\n            plt.figure(figsize=(7.5, 5))\n            plt.axes(projection='3d').plot_surface(self.w, self.b, self.Z, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n            plt.title('Loss Surface')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.show()\n            plt.figure()\n            plt.title('Loss Surface Contour')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.contour(self.w, self.b, self.Z)\n            plt.show()\n            \n     # Setter\n    def set_para_loss(self, model, loss):\n        self.n = self.n + 1\n        self.W.append(list(model.parameters())[0].item())\n        self.B.append(list(model.parameters())[1].item())\n        self.LOSS.append(loss)\n    \n    # Plot diagram\n    def final_plot(self): \n        ax = plt.axes(projection='3d')\n        ax.plot_wireframe(self.w, self.b, self.Z)\n        ax.scatter(self.W, self.B, self.LOSS, c='r', marker='x', s=200, alpha=1)\n        plt.figure()\n        plt.contour(self.w, self.b, self.Z)\n        plt.scatter(self.W, self.B, c='r', marker='x')\n        plt.xlabel('w')\n        plt.ylabel('b')\n        plt.show()\n        \n    # Plot diagram\n    def plot_ps(self):\n        plt.subplot(121)\n        plt.ylim\n        plt.plot(self.x, self.y, 'ro', label=\"training points\")\n        plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label=\"estimated line\")\n        plt.plot(self.x, 1 / (1 + np.exp(-1 * (self.W[-1] * self.x + self.B[-1]))), label='sigmoid')\n        plt.xlabel('x')\n        plt.ylabel('y')\n        plt.ylim((-0.1, 2))\n        plt.title('Data Space Iteration: ' + str(self.n))\n        plt.show()\n        plt.subplot(122)\n        plt.contour(self.w, self.b, self.Z)\n        plt.scatter(self.W, self.B, c='r', marker='x')\n        plt.title('Loss Surface Contour Iteration' + str(self.n))\n        plt.xlabel('w')\n        plt.ylabel('b')\n        \n# Plot the diagram\n\ndef PlotStuff(X, Y, model, epoch, leg=True):\n    plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch)))\n    plt.plot(X.numpy(), Y.numpy(), 'r')\n    if leg == True:\n        plt.legend()\n    else:\n        pass\n</pre> # Create class for plotting and the function for plotting  class plot_error_surfaces(object):          # Construstor     def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True):         W = np.linspace(-w_range, w_range, n_samples)         B = np.linspace(-b_range, b_range, n_samples)         w, b = np.meshgrid(W, B)             Z = np.zeros((30, 30))         count1 = 0         self.y = Y.numpy()         self.x = X.numpy()         for w1, b1 in zip(w, b):             count2 = 0             for w2, b2 in zip(w1, b1):                 yhat= 1 / (1 + np.exp(-1*(w2*self.x+b2)))                 Z[count1,count2]=-1*np.mean(self.y*np.log(yhat+1e-16) +(1-self.y)*np.log(1-yhat+1e-16))                 count2 += 1                count1 += 1         self.Z = Z         self.w = w         self.b = b         self.W = []         self.B = []         self.LOSS = []         self.n = 0         if go == True:             plt.figure()             plt.figure(figsize=(7.5, 5))             plt.axes(projection='3d').plot_surface(self.w, self.b, self.Z, rstride=1, cstride=1, cmap='viridis', edgecolor='none')             plt.title('Loss Surface')             plt.xlabel('w')             plt.ylabel('b')             plt.show()             plt.figure()             plt.title('Loss Surface Contour')             plt.xlabel('w')             plt.ylabel('b')             plt.contour(self.w, self.b, self.Z)             plt.show()                   # Setter     def set_para_loss(self, model, loss):         self.n = self.n + 1         self.W.append(list(model.parameters())[0].item())         self.B.append(list(model.parameters())[1].item())         self.LOSS.append(loss)          # Plot diagram     def final_plot(self):          ax = plt.axes(projection='3d')         ax.plot_wireframe(self.w, self.b, self.Z)         ax.scatter(self.W, self.B, self.LOSS, c='r', marker='x', s=200, alpha=1)         plt.figure()         plt.contour(self.w, self.b, self.Z)         plt.scatter(self.W, self.B, c='r', marker='x')         plt.xlabel('w')         plt.ylabel('b')         plt.show()              # Plot diagram     def plot_ps(self):         plt.subplot(121)         plt.ylim         plt.plot(self.x, self.y, 'ro', label=\"training points\")         plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label=\"estimated line\")         plt.plot(self.x, 1 / (1 + np.exp(-1 * (self.W[-1] * self.x + self.B[-1]))), label='sigmoid')         plt.xlabel('x')         plt.ylabel('y')         plt.ylim((-0.1, 2))         plt.title('Data Space Iteration: ' + str(self.n))         plt.show()         plt.subplot(122)         plt.contour(self.w, self.b, self.Z)         plt.scatter(self.W, self.B, c='r', marker='x')         plt.title('Loss Surface Contour Iteration' + str(self.n))         plt.xlabel('w')         plt.ylabel('b')          # Plot the diagram  def PlotStuff(X, Y, model, epoch, leg=True):     plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch)))     plt.plot(X.numpy(), Y.numpy(), 'r')     if leg == True:         plt.legend()     else:         pass <p>Set the random seed:</p> In\u00a0[3]: Copied! <pre># Set random seed\n\ntorch.manual_seed(0)\n</pre> # Set random seed  torch.manual_seed(0) Out[3]: <pre>&lt;torch._C.Generator at 0x75ee4c9721f0&gt;</pre> In\u00a0[4]: Copied! <pre># Create the data class\n\nclass Data(Dataset):\n    \n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-1, 1, 0.1).view(-1, 1)\n        self.y = torch.zeros(self.x.shape[0], 1)\n        self.y[self.x[:, 0] &gt; 0.2] = 1\n        self.len = self.x.shape[0]\n    \n    # Getter\n    def __getitem__(self, index):      \n        return self.x[index], self.y[index]\n    \n    # Get length\n    def __len__(self):\n        return self.len\n</pre> # Create the data class  class Data(Dataset):          # Constructor     def __init__(self):         self.x = torch.arange(-1, 1, 0.1).view(-1, 1)         self.y = torch.zeros(self.x.shape[0], 1)         self.y[self.x[:, 0] &gt; 0.2] = 1         self.len = self.x.shape[0]          # Getter     def __getitem__(self, index):               return self.x[index], self.y[index]          # Get length     def __len__(self):         return self.len <p>Make <code>Data</code> object</p> In\u00a0[5]: Copied! <pre># Create Data object\n\ndata_set = Data()\n</pre> # Create Data object  data_set = Data() <p>Create a custom module for logistic regression:</p> In\u00a0[6]: Copied! <pre># Create logistic_regression class\n\nclass logistic_regression(nn.Module):\n    \n    # Constructor\n    def __init__(self, n_inputs):\n        super(logistic_regression, self).__init__()\n        self.linear = nn.Linear(n_inputs, 1)\n        \n    # Prediction\n    def forward(self, x):\n        yhat = torch.sigmoid(self.linear(x))\n        return yhat\n</pre> # Create logistic_regression class  class logistic_regression(nn.Module):          # Constructor     def __init__(self, n_inputs):         super(logistic_regression, self).__init__()         self.linear = nn.Linear(n_inputs, 1)              # Prediction     def forward(self, x):         yhat = torch.sigmoid(self.linear(x))         return yhat <p>Create a logistic regression object or model:</p> In\u00a0[7]: Copied! <pre># Create the logistic_regression result\n\nmodel = logistic_regression(1)\n</pre> # Create the logistic_regression result  model = logistic_regression(1) <p>Replace the random initialized variable values. Theses random initialized variable values did convergence for the RMS Loss but will converge for the Cross-Entropy Loss.</p> In\u00a0[8]: Copied! <pre># Set the weight and bias\n\nmodel.state_dict() ['linear.weight'].data[0] = torch.tensor([[-5]])\nmodel.state_dict() ['linear.bias'].data[0] = torch.tensor([[-10]])\nprint(\"The parameters: \", model.state_dict())\n</pre> # Set the weight and bias  model.state_dict() ['linear.weight'].data[0] = torch.tensor([[-5]]) model.state_dict() ['linear.bias'].data[0] = torch.tensor([[-10]]) print(\"The parameters: \", model.state_dict()) <pre>The parameters:  OrderedDict({'linear.weight': tensor([[-5.]]), 'linear.bias': tensor([-10.])})\n</pre> <p>Create a <code> plot_error_surfaces</code> object to visualize the data space and the parameter space during training:</p> In\u00a0[9]: Copied! <pre># Create the plot_error_surfaces object\n\nget_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1], 30)\n</pre> # Create the plot_error_surfaces object  get_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1], 30) <pre>&lt;Figure size 640x480 with 0 Axes&gt;</pre> <p>Define the cost or criterion function:</p> In\u00a0[10]: Copied! <pre># Create dataloader, criterion function and optimizer\n\ndef criterion(yhat,y):\n    out = -1 * torch.mean(y * torch.log(yhat) + (1 - y) * torch.log(1 - yhat))\n    return out\n\n# Build in criterion\n# criterion = nn.BCELoss()\n\ntrainloader = DataLoader(dataset = data_set, batch_size = 3)\nlearning_rate = 2\noptimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n</pre> # Create dataloader, criterion function and optimizer  def criterion(yhat,y):     out = -1 * torch.mean(y * torch.log(yhat) + (1 - y) * torch.log(1 - yhat))     return out  # Build in criterion # criterion = nn.BCELoss()  trainloader = DataLoader(dataset = data_set, batch_size = 3) learning_rate = 2 optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) <p>Train the model</p> In\u00a0[11]: Copied! <pre># Train the Model\n\ndef train_model(epochs):\n    for epoch in range(epochs):\n        for x, y in trainloader:\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            get_surface.set_para_loss(model, loss.tolist())\n        if epoch % 20 == 0:\n            get_surface.plot_ps()\n            \ntrain_model(100)\n</pre> # Train the Model  def train_model(epochs):     for epoch in range(epochs):         for x, y in trainloader:             yhat = model(x)             loss = criterion(yhat, y)             optimizer.zero_grad()             loss.backward()             optimizer.step()             get_surface.set_para_loss(model, loss.tolist())         if epoch % 20 == 0:             get_surface.plot_ps()              train_model(100) <p>Get the actual class of each sample and calculate the accuracy on the test data:</p> In\u00a0[12]: Copied! <pre># Make the Prediction\n\nyhat = model(data_set.x)\nlabel = yhat &gt; 0.5\nprint(\"The accuracy: \", torch.mean((label == data_set.y.type(torch.ByteTensor)).type(torch.float)))\n</pre> # Make the Prediction  yhat = model(data_set.x) label = yhat &gt; 0.5 print(\"The accuracy: \", torch.mean((label == data_set.y.type(torch.ByteTensor)).type(torch.float))) <pre>The accuracy:  tensor(1.)\n</pre> <p>The accuracy is perfect.</p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.3_cross_entropy_logistic_regression_v2/#author-juma-shafara-date-2024-08-12-title-training-negative-log-likelihood-cross-entropy-keywords-training-two-parameter-mini-batch-gradient-decent-training-two-parameter-mini-batch-gradient-decent-description-in-this-lab-you-will-see-what-happens-when-you-use-the-cross-entropy-or-total-loss-function-using-random-initialization-for-a-parameter-value","title":"author: Juma Shafara date: \"2024-08-12\" title: Training Negative Log likelihood (Cross-Entropy) keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will see what happens when you use the Cross-Entropy or total loss function using random initialization for a parameter value.\u00b6","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.3_cross_entropy_logistic_regression_v2/#Makeup_Data","title":"Get Some Data","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.3_cross_entropy_logistic_regression_v2/#Model_Cost","title":"Create the Model and Total Loss Function (Cost)","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.3_cross_entropy_logistic_regression_v2/#BGD","title":"Train the Model via Batch Gradient Descent","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.3_cross_entropy_logistic_regression_v2/#about-the-author","title":"About the Author:\u00b6What's on your mind? Put it in the comments!","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.4softmax_in_one_dimension_v2/","title":"Softmax","text":"Objective<ul><li> How to build a Softmax classifier by using the Sequential module in pytorch.</li></ul> Table of Contents <p>In this lab, you will use Softmax to classify three linearly separable classes, the features are in one dimension </p> <ul> <li> Make Some Data</li> <li> Build Softmax Classifier</li> <li> Train the Model</li> <li> Analyze Results</li> </ul> <p>Estimated Time Needed: 25 min</p> Preparation <p>We'll need the following libraries:</p> In\u00a0[1]: Copied! <pre># Import the libraries we need for this lab\n\nimport torch.nn as nn\nimport torch\nimport matplotlib.pyplot as plt \nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\n</pre> # Import the libraries we need for this lab  import torch.nn as nn import torch import matplotlib.pyplot as plt  import numpy as np from torch.utils.data import Dataset, DataLoader <p>Use the helper function to plot labeled data points:</p> In\u00a0[2]: Copied! <pre># Create class for plotting\n\ndef plot_data(data_set, model = None, n = 1, color = False):\n    X = data_set[:][0]\n    Y = data_set[:][1]\n    plt.plot(X[Y == 0, 0].numpy(), Y[Y == 0].numpy(), 'bo', label = 'y = 0')\n    plt.plot(X[Y == 1, 0].numpy(), 0 * Y[Y == 1].numpy(), 'ro', label = 'y = 1')\n    plt.plot(X[Y == 2, 0].numpy(), 0 * Y[Y == 2].numpy(), 'go', label = 'y = 2')\n    plt.ylim((-0.1, 3))\n    plt.legend()\n    if model != None:\n        w = list(model.parameters())[0][0].detach()\n        b = list(model.parameters())[1][0].detach()\n        y_label = ['yhat=0', 'yhat=1', 'yhat=2']\n        y_color = ['b', 'r', 'g']\n        Y = []\n        for w, b, y_l, y_c in zip(model.state_dict()['0.weight'], model.state_dict()['0.bias'], y_label, y_color):\n            Y.append((w * X + b).numpy())\n            plt.plot(X.numpy(), (w * X + b).numpy(), y_c, label = y_l)\n        if color == True:\n            x = X.numpy()\n            x = x.reshape(-1)\n            top = np.ones(x.shape)\n            y0 = Y[0].reshape(-1)\n            y1 = Y[1].reshape(-1)\n            y2 = Y[2].reshape(-1)\n            plt.fill_between(x, y0, where = y1 &gt; y1, interpolate = True, color = 'blue')\n            plt.fill_between(x, y0, where = y1 &gt; y2, interpolate = True, color = 'blue')\n            plt.fill_between(x, y1, where = y1 &gt; y0, interpolate = True, color = 'red')\n            plt.fill_between(x, y1, where = ((y1 &gt; y2) * (y1 &gt; y0)),interpolate = True, color = 'red')\n            plt.fill_between(x, y2, where = (y2 &gt; y0) * (y0 &gt; 0),interpolate = True, color = 'green')\n            plt.fill_between(x, y2, where = (y2 &gt; y1), interpolate = True, color = 'green')\n    plt.legend()\n    plt.show()\n</pre> # Create class for plotting  def plot_data(data_set, model = None, n = 1, color = False):     X = data_set[:][0]     Y = data_set[:][1]     plt.plot(X[Y == 0, 0].numpy(), Y[Y == 0].numpy(), 'bo', label = 'y = 0')     plt.plot(X[Y == 1, 0].numpy(), 0 * Y[Y == 1].numpy(), 'ro', label = 'y = 1')     plt.plot(X[Y == 2, 0].numpy(), 0 * Y[Y == 2].numpy(), 'go', label = 'y = 2')     plt.ylim((-0.1, 3))     plt.legend()     if model != None:         w = list(model.parameters())[0][0].detach()         b = list(model.parameters())[1][0].detach()         y_label = ['yhat=0', 'yhat=1', 'yhat=2']         y_color = ['b', 'r', 'g']         Y = []         for w, b, y_l, y_c in zip(model.state_dict()['0.weight'], model.state_dict()['0.bias'], y_label, y_color):             Y.append((w * X + b).numpy())             plt.plot(X.numpy(), (w * X + b).numpy(), y_c, label = y_l)         if color == True:             x = X.numpy()             x = x.reshape(-1)             top = np.ones(x.shape)             y0 = Y[0].reshape(-1)             y1 = Y[1].reshape(-1)             y2 = Y[2].reshape(-1)             plt.fill_between(x, y0, where = y1 &gt; y1, interpolate = True, color = 'blue')             plt.fill_between(x, y0, where = y1 &gt; y2, interpolate = True, color = 'blue')             plt.fill_between(x, y1, where = y1 &gt; y0, interpolate = True, color = 'red')             plt.fill_between(x, y1, where = ((y1 &gt; y2) * (y1 &gt; y0)),interpolate = True, color = 'red')             plt.fill_between(x, y2, where = (y2 &gt; y0) * (y0 &gt; 0),interpolate = True, color = 'green')             plt.fill_between(x, y2, where = (y2 &gt; y1), interpolate = True, color = 'green')     plt.legend()     plt.show() <p>Set the random seed:</p> In\u00a0[4]: Copied! <pre>#Set the random seed\n\ntorch.manual_seed(0)\n</pre> #Set the random seed  torch.manual_seed(0) Out[4]: <pre>&lt;torch._C.Generator at 0x7b9882167170&gt;</pre> <p>Create some linearly separable data with three classes:</p> In\u00a0[5]: Copied! <pre># Create the data class\n\nclass Data(Dataset):\n    \n    # Constructor\n    def __init__(self):\n        self.x = torch.arange(-2, 2, 0.1).view(-1, 1)\n        self.y = torch.zeros(self.x.shape[0])\n        self.y[(self.x &gt; -1.0)[:, 0] * (self.x &lt; 1.0)[:, 0]] = 1\n        self.y[(self.x &gt;= 1.0)[:, 0]] = 2\n        self.y = self.y.type(torch.LongTensor)\n        self.len = self.x.shape[0]\n        \n    # Getter\n    def __getitem__(self,index):      \n        return self.x[index], self.y[index]\n    \n    # Get Length\n    def __len__(self):\n        return self.len\n</pre> # Create the data class  class Data(Dataset):          # Constructor     def __init__(self):         self.x = torch.arange(-2, 2, 0.1).view(-1, 1)         self.y = torch.zeros(self.x.shape[0])         self.y[(self.x &gt; -1.0)[:, 0] * (self.x &lt; 1.0)[:, 0]] = 1         self.y[(self.x &gt;= 1.0)[:, 0]] = 2         self.y = self.y.type(torch.LongTensor)         self.len = self.x.shape[0]              # Getter     def __getitem__(self,index):               return self.x[index], self.y[index]          # Get Length     def __len__(self):         return self.len <p>Create the dataset object:</p> In\u00a0[6]: Copied! <pre># Create the dataset object and plot the dataset object\n\ndata_set = Data()\ndata_set.x\nplot_data(data_set)\n</pre> # Create the dataset object and plot the dataset object  data_set = Data() data_set.x plot_data(data_set) <p>Build a Softmax classifier by using the Sequential module:</p> In\u00a0[37]: Copied! <pre># Build Softmax Classifier technically you only need nn.Linear\n\nmodel = nn.Sequential(nn.Linear(1, 3))\nmodel.state_dict()\n</pre> # Build Softmax Classifier technically you only need nn.Linear  model = nn.Sequential(nn.Linear(1, 3)) model.state_dict() Out[37]: <pre>OrderedDict([('0.weight',\n              tensor([[ 0.5239],\n                      [-0.4626],\n                      [-0.4925]])),\n             ('0.bias', tensor([-0.0875, -0.0961, -0.7790]))])</pre> In\u00a0[8]: Copied! <pre>data_set[0]\n</pre> data_set[0] Out[8]: <pre>(tensor([-2.]), tensor(0))</pre> In\u00a0[9]: Copied! <pre>model(data_set[0][0])\n</pre> model(data_set[0][0]) Out[9]: <pre>tensor([-0.7210, -1.4580,  1.9142], grad_fn=&lt;ViewBackward0&gt;)</pre> <p>Create the criterion function, the optimizer and the dataloader</p> In\u00a0[10]: Copied! <pre># Create criterion function, optimizer, and dataloader\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\ntrainloader = DataLoader(dataset = data_set, batch_size = 5)\n</pre> # Create criterion function, optimizer, and dataloader  criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr = 0.01) trainloader = DataLoader(dataset = data_set, batch_size = 5) <p>Train the model for every 50 epochs plot, the line generated for each class.</p> In\u00a0[11]: Copied! <pre># Train the model\n\nLOSS = []\ndef train_model(epochs):\n    for epoch in range(epochs):\n        if epoch % 50 == 0:\n            pass\n            plot_data(data_set, model)\n        for x, y in trainloader:\n            optimizer.zero_grad()\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            LOSS.append(loss)\n            loss.backward()\n            optimizer.step()\ntrain_model(300)\n</pre> # Train the model  LOSS = [] def train_model(epochs):     for epoch in range(epochs):         if epoch % 50 == 0:             pass             plot_data(data_set, model)         for x, y in trainloader:             optimizer.zero_grad()             yhat = model(x)             loss = criterion(yhat, y)             LOSS.append(loss)             loss.backward()             optimizer.step() train_model(300) <p>Find the predicted class on the test data:</p> In\u00a0[33]: Copied! <pre>pred_0 = model(data_set.x[10:20])\n_, yhat_0 = pred_0.max(1)\nprint(yhat_0)\n</pre> pred_0 = model(data_set.x[10:20]) _, yhat_0 = pred_0.max(1) print(yhat_0) <pre>tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n</pre> In\u00a0[52]: Copied! <pre># Make the prediction\n\nz =  model(data_set.x)\n_, yhat = z.max(1)\nprint(\"The prediction:\", yhat)\n</pre> # Make the prediction  z =  model(data_set.x) _, yhat = z.max(1) print(\"The prediction:\", yhat)  <pre>The prediction: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n</pre> <p>Calculate the accuracy on the test data:</p> In\u00a0[53]: Copied! <pre># Print the accuracy\n\ncorrect = (data_set.y == yhat).sum().item()\naccuracy = correct / len(data_set)\nprint(\"The accuracy: \", accuracy)\n</pre> # Print the accuracy  correct = (data_set.y == yhat).sum().item() accuracy = correct / len(data_set) print(\"The accuracy: \", accuracy) <pre>The accuracy:  0.225\n</pre> <p>You can also use the softmax function to convert the output to a probability,first, we create a Softmax object:</p> In\u00a0[54]: Copied! <pre>Softmax_fn=nn.Softmax(dim=-1)\n</pre> Softmax_fn=nn.Softmax(dim=-1) <p>The result is a tensor <code> Probability </code>, where each row corresponds to a different sample, and each column corresponds to that sample  belonging to a particular class</p> In\u00a0[55]: Copied! <pre>Probability = Softmax_fn(z)\n</pre> Probability = Softmax_fn(z) <p>we can obtain the probability of the first sample belonging to the first, second and third class respectively as follows:</p> In\u00a0[58]: Copied! <pre>for i in range(3):\n    print(f\"probability of class {i} isg given by  {Probability[0,i]}\")\n</pre> for i in range(3):     print(f\"probability of class {i} isg given by  {Probability[0,i]}\") <pre>probability of class 0 isg given by  0.08364968001842499\nprobability of class 1 isg given by  0.5964528322219849\nprobability of class 2 isg given by  0.31989753246307373\n</pre> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.4softmax_in_one_dimension_v2/#author-juma-shafara-date-2024-08-12-title-softmax-classifer-1d-keywords-training-two-parameter-mini-batch-gradient-decent-training-two-parameter-mini-batch-gradient-decent-description-in-this-lab-you-will-use-softmax-to-classify-three-linearly-separable-classes-the-features-are-in-one-dimension","title":"author: Juma Shafara date: \"2024-08-12\" title: Softmax Classifer 1D keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will use Softmax to classify three linearly separable classes, the features are in one dimension\u00b6","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.4softmax_in_one_dimension_v2/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.4softmax_in_one_dimension_v2/#Softmax","title":"Build a Softmax Classifier","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.4softmax_in_one_dimension_v2/#Model","title":"Train the Model","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.4softmax_in_one_dimension_v2/#Result","title":"Analyze Results","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/5.4softmax_in_one_dimension_v2/#about-the-author","title":"About the Author:\u00b6","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/logistic_regression_exercise/","title":"Exercise","text":"What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week5-Logistic-Regression/logistic_regression_exercise/#title-logistic-regression-exercise-author-juma-shafara-date-2024-08-29-keywords-data-science-data-analysis-programming-dataidea-description-python-for-data-science-is-a-subject-weve-designed-to-explore-the-various-programming-components-of-data-science","title":"title: Logistic Regression Exercise author: Juma Shafara date: \"2024-08-29\" keywords: [data science, data analysis, programming, dataidea] description: Python for Data Science is a subject we\u2019ve designed to explore the various programming components of data science.\u00b6","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/logistic_regression_exercise/#week-5-logistic-regression","title":"Week 5: Logistic Regression\u00b6","text":""},{"location":"Deep%20Learning/Week5-Logistic-Regression/logistic_regression_exercise/#1-making-predictions-in-logistic-regression","title":"1. Making Predictions in Logistic Regression\u00b6","text":"<ul> <li>Exercise: Implement a logistic regression model to classify data into two classes. Use a synthetic dataset with two features and evaluate the model\u2019s accuracy using confusion matrix.</li> </ul>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/logistic_regression_exercise/#2-logistic-regression-and-bad-initialization-values","title":"2. Logistic Regression and Bad Initialization Values\u00b6","text":"<ul> <li>Exercise: Train a logistic regression model with various initialization strategies. Analyze how different initial weights affect the convergence and performance of the model.</li> </ul>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/logistic_regression_exercise/#3-cross-entropy-loss-function","title":"3. Cross Entropy Loss Function\u00b6","text":"<ul> <li>Exercise: Implement the cross-entropy loss function from scratch for a logistic regression model. Compare its output with PyTorch\u2019s built-in <code>torch.nn.CrossEntropyLoss</code> function.</li> </ul>  Don't Miss Any Updates! <p> Before the last, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Deep%20Learning/Week5-Logistic-Regression/logistic_regression_exercise/#4-softmax-activation-in-1-dimension","title":"4. Softmax Activation in 1 Dimension\u00b6","text":"<ul> <li>Exercise: Implement the softmax activation function for a vector of logits in a 1D tensor. Use this function to convert raw model outputs into probabilities and interpret the results.</li> </ul>"},{"location":"Deep%20Learning/Week6-Practice/6.1_custom_datasets/","title":"Custom Datasets","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport dataidea_science as ds\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n</pre> import pandas as pd import dataidea_science as ds import torch from torch.utils.data import Dataset from torch.utils.data import DataLoader In\u00a0[2]: Copied! <pre>boston_ = ds.loadDataset('boston')\n</pre> boston_ = ds.loadDataset('boston') In\u00a0[3]: Copied! <pre>class BostonDataset(Dataset): \n\n    def __init__(self):\n        # define our dataset\n        self.data = boston_\n        self.x = torch.tensor(self.data.drop('MEDV', axis=1).values, dtype=torch.float32)\n        self.y = torch.tensor(self.data.MEDV.values, dtype=torch.float32)\n        self.samples = self.data.shape[0]\n\n    def __getitem__(self, index):\n        # access samples\n        return self.x[index], self.y[index]\n    \n    def __len__(self):\n        # len(dataset)\n        return self.samples\n</pre> class BostonDataset(Dataset):       def __init__(self):         # define our dataset         self.data = boston_         self.x = torch.tensor(self.data.drop('MEDV', axis=1).values, dtype=torch.float32)         self.y = torch.tensor(self.data.MEDV.values, dtype=torch.float32)         self.samples = self.data.shape[0]      def __getitem__(self, index):         # access samples         return self.x[index], self.y[index]          def __len__(self):         # len(dataset)         return self.samples In\u00a0[4]: Copied! <pre>boston_dataset = BostonDataset()\n\nrow_1 = boston_dataset[1]\nprint('Row 1 Features:', row_1[0])\nprint('Row 1 Outcome:', row_1[1])\n\nlength_ = len(boston_dataset)\nprint('Total Samples: ', length_)\n</pre> boston_dataset = BostonDataset()  row_1 = boston_dataset[1] print('Row 1 Features:', row_1[0]) print('Row 1 Outcome:', row_1[1])  length_ = len(boston_dataset) print('Total Samples: ', length_) <pre>Row 1 Features: tensor([2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01, 6.4210e+00,\n        7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01, 3.9690e+02,\n        9.1400e+00])\nRow 1 Outcome: tensor(21.6000)\nTotal Samples:  506\n</pre>"},{"location":"Deep%20Learning/Week6-Practice/6.1_custom_datasets/#author-juma-shafara-date-2024-09-04-title-custom-datasets-practice-keywords-training-two-parameter-mini-batch-gradient-decent-training-two-parameter-mini-batch-gradient-decent-description-in-this-lab-you-will-review-how-to-make-a-prediction-in-several-different-ways-by-using-pytorch","title":"author: Juma Shafara date: \"2024-09-04\" title: Custom Datasets Practice keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will review how to make a prediction in several different ways by using PyTorch.\u00b6","text":""},{"location":"Deep%20Learning/Week6-Practice/6.1_custom_datasets/#custom-dataset-class","title":"Custom Dataset Class\u00b6","text":""},{"location":"Deep%20Learning/Week6-Practice/6.2_dataloaders/","title":"DataLoaders","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport dataidea_science as ds\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n</pre> import pandas as pd import dataidea_science as ds import torch from torch.utils.data import Dataset from torch.utils.data import DataLoader In\u00a0[2]: Copied! <pre>boston_ = ds.loadDataset('boston')\n</pre> boston_ = ds.loadDataset('boston') In\u00a0[3]: Copied! <pre>class BostonDataset(Dataset): \n\n    def __init__(self):\n        # define our dataset\n        self.data = boston_\n        self.x = torch.tensor(self.data.drop('MEDV', axis=1).values, dtype=torch.float32)\n        self.y = torch.tensor(self.data.MEDV.values, dtype=torch.float32)\n        self.samples = self.data.shape[0]\n\n    def __getitem__(self, index):\n        # access samples\n        return self.x[index], self.y[index]\n    \n    def __len__(self):\n        # len(dataset)\n        return self.samples\n</pre> class BostonDataset(Dataset):       def __init__(self):         # define our dataset         self.data = boston_         self.x = torch.tensor(self.data.drop('MEDV', axis=1).values, dtype=torch.float32)         self.y = torch.tensor(self.data.MEDV.values, dtype=torch.float32)         self.samples = self.data.shape[0]      def __getitem__(self, index):         # access samples         return self.x[index], self.y[index]          def __len__(self):         # len(dataset)         return self.samples In\u00a0[4]: Copied! <pre>boston_dataset = BostonDataset()\n\nrow_1 = boston_dataset[1]\nprint('Row 1 Features:', row_1[0])\nprint('Row 1 Outcome:', row_1[1])\n\nlength_ = len(boston_dataset)\nprint('Total Samples: ', length_)\n</pre> boston_dataset = BostonDataset()  row_1 = boston_dataset[1] print('Row 1 Features:', row_1[0]) print('Row 1 Outcome:', row_1[1])  length_ = len(boston_dataset) print('Total Samples: ', length_) <pre>Row 1 Features: tensor([2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01, 6.4210e+00,\n        7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01, 3.9690e+02,\n        9.1400e+00])\nRow 1 Outcome: tensor(21.6000)\nTotal Samples:  506\n</pre> In\u00a0[10]: Copied! <pre>boston_dataloader = DataLoader(dataset=boston_dataset,\n                               batch_size=3,\n                               shuffle=True,\n                               num_workers=2)\n</pre> boston_dataloader = DataLoader(dataset=boston_dataset,                                batch_size=3,                                shuffle=True,                                num_workers=2) In\u00a0[12]: Copied! <pre>for batch_no, (x, y) in enumerate(boston_dataloader):\n    print(f'Batch: {batch_no}:')\n    print(f'Data: {x}')\n    print(f'Labels: {y}')\n\n    if batch_no == 0:\n        break\n</pre>  for batch_no, (x, y) in enumerate(boston_dataloader):     print(f'Batch: {batch_no}:')     print(f'Data: {x}')     print(f'Labels: {y}')      if batch_no == 0:         break <pre>Batch: 0:\nData: tensor([[2.9819e-01, 0.0000e+00, 6.2000e+00, 0.0000e+00, 5.0400e-01, 7.6860e+00,\n         1.7000e+01, 3.3751e+00, 8.0000e+00, 3.0700e+02, 1.7400e+01, 3.7751e+02,\n         3.9200e+00],\n        [6.8012e+00, 0.0000e+00, 1.8100e+01, 0.0000e+00, 7.1300e-01, 6.0810e+00,\n         8.4400e+01, 2.7175e+00, 2.4000e+01, 6.6600e+02, 2.0200e+01, 3.9690e+02,\n         1.4700e+01],\n        [1.5874e+01, 0.0000e+00, 1.8100e+01, 0.0000e+00, 6.7100e-01, 6.5450e+00,\n         9.9100e+01, 1.5192e+00, 2.4000e+01, 6.6600e+02, 2.0200e+01, 3.9690e+02,\n         2.1080e+01]])\nLabels: tensor([46.7000, 20.0000, 10.9000])\n</pre>"},{"location":"Deep%20Learning/Week6-Practice/6.2_dataloaders/#author-juma-shafara-date-2024-09-04-title-dataloaders-practice-keywords-training-two-parameter-mini-batch-gradient-decent-training-two-parameter-mini-batch-gradient-decent-description-in-this-lab-you-will-review-how-to-make-a-prediction-in-several-different-ways-by-using-pytorch","title":"author: Juma Shafara date: \"2024-09-04\" title: DataLoaders Practice keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will review how to make a prediction in several different ways by using PyTorch.\u00b6","text":""},{"location":"Deep%20Learning/Week6-Practice/6.2_dataloaders/#custom-dataset","title":"Custom Dataset\u00b6","text":""},{"location":"Deep%20Learning/Week6-Practice/6.2_dataloaders/#dataloaders","title":"DataLoaders\u00b6","text":""},{"location":"Deep%20Learning/Week6-Practice/6.3_transforms/","title":"Transforms","text":"In\u00a0[1]: Copied! <pre>import dataidea_science as ds\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n</pre> import dataidea_science as ds import torch from torch.utils.data import Dataset from torch.utils.data import DataLoader In\u00a0[2]: Copied! <pre>boston_ = ds.loadDataset('boston')\n</pre> boston_ = ds.loadDataset('boston') In\u00a0[3]: Copied! <pre>class BostonDataset(Dataset): \n\n    def __init__(self, transform=None):\n        # define our dataset\n        self.data = boston_\n        self.x = self.data.drop('MEDV', axis=1).values\n        self.y = self.data.MEDV.values\n        self.samples = self.data.shape[0]\n        self.transform = transform\n\n    def __getitem__(self, index):\n        # access samples\n        sample = (self.x[index], self.y[index])\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n    \n    def __len__(self):\n        # len(dataset)\n        return self.samples\n</pre> class BostonDataset(Dataset):       def __init__(self, transform=None):         # define our dataset         self.data = boston_         self.x = self.data.drop('MEDV', axis=1).values         self.y = self.data.MEDV.values         self.samples = self.data.shape[0]         self.transform = transform      def __getitem__(self, index):         # access samples         sample = (self.x[index], self.y[index])          if self.transform:             sample = self.transform(sample)          return sample          def __len__(self):         # len(dataset)         return self.samples In\u00a0[4]: Copied! <pre>boston_dataset = BostonDataset()\n\nrow_1 = boston_dataset[1]\nprint('Row 1 Features:', row_1[0])\nprint('Row 1 Outcome:', row_1[1])\n\nlength_ = len(boston_dataset)\nprint('Total Samples: ', length_)\n</pre> boston_dataset = BostonDataset()  row_1 = boston_dataset[1] print('Row 1 Features:', row_1[0]) print('Row 1 Outcome:', row_1[1])  length_ = len(boston_dataset) print('Total Samples: ', length_) <pre>Row 1 Features: [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n 7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n 9.1400e+00]\nRow 1 Outcome: 21.6\nTotal Samples:  506\n</pre> In\u00a0[5]: Copied! <pre>boston_dataloader = DataLoader(dataset=boston_dataset,\n                               batch_size=3,\n                               shuffle=True,\n                               num_workers=2)\n</pre> boston_dataloader = DataLoader(dataset=boston_dataset,                                batch_size=3,                                shuffle=True,                                num_workers=2) In\u00a0[6]: Copied! <pre>for batch_no, (x, y) in enumerate(boston_dataloader):\n    print(f'Batch: {batch_no}:')\n    print(f'Data: {x}')\n    print(f'Labels: {y}')\n\n    if batch_no == 0:\n        break\n</pre>  for batch_no, (x, y) in enumerate(boston_dataloader):     print(f'Batch: {batch_no}:')     print(f'Data: {x}')     print(f'Labels: {y}')      if batch_no == 0:         break <pre>Batch: 0:\nData: tensor([[9.7617e-01, 0.0000e+00, 2.1890e+01, 0.0000e+00, 6.2400e-01, 5.7570e+00,\n         9.8400e+01, 2.3460e+00, 4.0000e+00, 4.3700e+02, 2.1200e+01, 2.6276e+02,\n         1.7310e+01],\n        [2.9090e-01, 0.0000e+00, 2.1890e+01, 0.0000e+00, 6.2400e-01, 6.1740e+00,\n         9.3600e+01, 1.6119e+00, 4.0000e+00, 4.3700e+02, 2.1200e+01, 3.8808e+02,\n         2.4160e+01],\n        [5.5007e-01, 2.0000e+01, 3.9700e+00, 0.0000e+00, 6.4700e-01, 7.2060e+00,\n         9.1600e+01, 1.9301e+00, 5.0000e+00, 2.6400e+02, 1.3000e+01, 3.8789e+02,\n         8.1000e+00]], dtype=torch.float64)\nLabels: tensor([15.6000, 14.0000, 36.5000], dtype=torch.float64)\n</pre> In\u00a0[8]: Copied! <pre>class TensorTransformer:\n\n    def __init__(self, dtype=torch.float32):\n        self.dtype = dtype\n\n    def __call__(self, sample):\n        x_tensor = torch.tensor(data=sample[0], dtype=self.dtype)\n        y_tensor = torch.tensor(data=sample[1], dtype=self.dtype)\n        return x_tensor, y_tensor\n</pre> class TensorTransformer:      def __init__(self, dtype=torch.float32):         self.dtype = dtype      def __call__(self, sample):         x_tensor = torch.tensor(data=sample[0], dtype=self.dtype)         y_tensor = torch.tensor(data=sample[1], dtype=self.dtype)         return x_tensor, y_tensor In\u00a0[9]: Copied! <pre>boston_dataset = BostonDataset(transform=TensorTransformer())\n\nrow_1 = boston_dataset[1]\nprint('Row 1 Features:', row_1[0])\nprint('Row 1 Outcome:', row_1[1])\n</pre> boston_dataset = BostonDataset(transform=TensorTransformer())  row_1 = boston_dataset[1] print('Row 1 Features:', row_1[0]) print('Row 1 Outcome:', row_1[1]) <pre>Row 1 Features: tensor([2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01, 6.4210e+00,\n        7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01, 3.9690e+02,\n        9.1400e+00])\nRow 1 Outcome: tensor(21.6000)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Deep%20Learning/Week6-Practice/6.3_transforms/#author-juma-shafara-date-2024-09-04-title-transform-practice-keywords-training-two-parameter-mini-batch-gradient-decent-training-two-parameter-mini-batch-gradient-decent-description-in-this-lab-you-will-review-how-to-make-a-prediction-in-several-different-ways-by-using-pytorch","title":"author: Juma Shafara date: \"2024-09-04\" title: Transform Practice keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will review how to make a prediction in several different ways by using PyTorch.\u00b6","text":""},{"location":"Deep%20Learning/Week6-Practice/6.3_transforms/#transforms","title":"Transforms\u00b6","text":""},{"location":"Deep%20Learning/Week6-Practice/6.3_transforms/#custom-dataset","title":"Custom Dataset\u00b6","text":""},{"location":"Deep%20Learning/Week6-Practice/6.3_transforms/#dataloader","title":"DataLoader\u00b6","text":""},{"location":"Deep%20Learning/Week6-Practice/6.3_transforms/#transformer","title":"Transformer\u00b6","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.1_simple1hiddenlayer/","title":"Simple Hidden Layer","text":"Objective<ul><li> How to create simple Neural Network in pytorch.</li></ul> Table of Contents <p>In this lab, you will use a single-layer neural network to classify non linearly seprable data in 1-Ddatabase.</p> <ul> <li> Neural Network Module and Training Function</li> <li> Make Some Data</li> <li> Define the Neural Network, Criterion Function, Optimizer, and Train the Model</li> </ul> <p>Estimated Time Needed: 25 min</p>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> Preparation <p>We'll need the following libraries</p> In\u00a0[1]: Copied! <pre># Import the libraries we need for this lab\n\nimport torch \nimport torch.nn as nn\nfrom torch import sigmoid\nimport matplotlib.pylab as plt\nimport numpy as np\ntorch.manual_seed(0)\n</pre> # Import the libraries we need for this lab  import torch  import torch.nn as nn from torch import sigmoid import matplotlib.pylab as plt import numpy as np torch.manual_seed(0) Out[1]: <pre>&lt;torch._C.Generator at 0x7c4f98773170&gt;</pre> <p>Used for plotting the model</p> In\u00a0[2]: Copied! <pre># The function for plotting the model\n\ndef PlotStuff(X, Y, model, epoch, leg=True):\n    \n    plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch)))\n    plt.plot(X.numpy(), Y.numpy(), 'r')\n    plt.xlabel('x')\n    if leg == True:\n        plt.legend()\n    else:\n        pass\n</pre> # The function for plotting the model  def PlotStuff(X, Y, model, epoch, leg=True):          plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch)))     plt.plot(X.numpy(), Y.numpy(), 'r')     plt.xlabel('x')     if leg == True:         plt.legend()     else:         pass <p>Define the activations and the output of the first linear layer as an attribute. Note that this is not good practice.</p> In\u00a0[3]: Copied! <pre># Define the class Net\n\nclass Net(nn.Module):\n    \n    # Constructor\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n        # hidden layer \n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n        # Define the first linear layer as an attribute, this is not good practice\n        self.a1 = None\n        self.l1 = None\n        self.l2=None\n    \n    # Prediction\n    def forward(self, x):\n        self.l1 = self.linear1(x)\n        self.a1 = sigmoid(self.l1)\n        self.l2=self.linear2(self.a1)\n        yhat = sigmoid(self.linear2(self.a1))\n        return yhat\n</pre> # Define the class Net  class Net(nn.Module):          # Constructor     def __init__(self, D_in, H, D_out):         super(Net, self).__init__()         # hidden layer          self.linear1 = nn.Linear(D_in, H)         self.linear2 = nn.Linear(H, D_out)         # Define the first linear layer as an attribute, this is not good practice         self.a1 = None         self.l1 = None         self.l2=None          # Prediction     def forward(self, x):         self.l1 = self.linear1(x)         self.a1 = sigmoid(self.l1)         self.l2=self.linear2(self.a1)         yhat = sigmoid(self.linear2(self.a1))         return yhat <p>Define the training function:</p> In\u00a0[4]: Copied! <pre># Define the training function\n\ndef train(Y, X, model, optimizer, criterion, epochs=1000):\n    cost = []\n    total=0\n    for epoch in range(epochs):\n        total=0\n        for y, x in zip(Y, X):\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            #cumulative loss \n            total+=loss.item() \n        cost.append(total)\n        if epoch % 300 == 0:    \n            PlotStuff(X, Y, model, epoch, leg=True)\n            plt.show()\n            model(X)\n            plt.scatter(model.a1.detach().numpy()[:, 0], model.a1.detach().numpy()[:, 1], c=Y.numpy().reshape(-1))\n            plt.title('activations')\n            plt.show()\n    return cost\n</pre> # Define the training function  def train(Y, X, model, optimizer, criterion, epochs=1000):     cost = []     total=0     for epoch in range(epochs):         total=0         for y, x in zip(Y, X):             yhat = model(x)             loss = criterion(yhat, y)             loss.backward()             optimizer.step()             optimizer.zero_grad()             #cumulative loss              total+=loss.item()          cost.append(total)         if epoch % 300 == 0:                 PlotStuff(X, Y, model, epoch, leg=True)             plt.show()             model(X)             plt.scatter(model.a1.detach().numpy()[:, 0], model.a1.detach().numpy()[:, 1], c=Y.numpy().reshape(-1))             plt.title('activations')             plt.show()     return cost In\u00a0[5]: Copied! <pre># Make some data\n\nX = torch.arange(-20, 20, 1).view(-1, 1).type(torch.FloatTensor)\nY = torch.zeros(X.shape[0])\nY[(X[:, 0] &gt; -4) &amp; (X[:, 0] &lt; 4)] = 1.0\n</pre> # Make some data  X = torch.arange(-20, 20, 1).view(-1, 1).type(torch.FloatTensor) Y = torch.zeros(X.shape[0]) Y[(X[:, 0] &gt; -4) &amp; (X[:, 0] &lt; 4)] = 1.0 <p>Create the Cross-Entropy loss function:</p> In\u00a0[6]: Copied! <pre># The loss function\n\ndef criterion_cross(outputs, labels):\n    out = -1 * torch.mean(labels * torch.log(outputs) + (1 - labels) * torch.log(1 - outputs))\n    return out\n</pre> # The loss function  def criterion_cross(outputs, labels):     out = -1 * torch.mean(labels * torch.log(outputs) + (1 - labels) * torch.log(1 - outputs))     return out <p>Define the Neural Network, Optimizer, and Train the Model:</p> In\u00a0[7]: Copied! <pre># Train the model\n# size of input \nD_in = 1\n# size of hidden layer \nH = 2\n# number of outputs \nD_out = 1\n# learning rate \nlearning_rate = 0.1\n# create the model \nmodel = Net(D_in, H, D_out)\n#optimizer \noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n#train the model usein\ncost_cross = train(Y, X, model, optimizer, criterion_cross, epochs=1000)\n#plot the loss\nplt.plot(cost_cross)\nplt.xlabel('epoch')\nplt.title('cross entropy loss')\n</pre> # Train the model # size of input  D_in = 1 # size of hidden layer  H = 2 # number of outputs  D_out = 1 # learning rate  learning_rate = 0.1 # create the model  model = Net(D_in, H, D_out) #optimizer  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) #train the model usein cost_cross = train(Y, X, model, optimizer, criterion_cross, epochs=1000) #plot the loss plt.plot(cost_cross) plt.xlabel('epoch') plt.title('cross entropy loss') Out[7]: <pre>Text(0.5, 1.0, 'cross entropy loss')</pre> <p>By examining the output of the  activation, you see by the 600th epoch that the data has been mapped to a linearly separable space.</p> <p>we can make a prediction for a arbitrary one tensors</p> In\u00a0[\u00a0]: Copied! <pre>x=torch.tensor([0.0])\nyhat=model(x)\nyhat\n</pre> x=torch.tensor([0.0]) yhat=model(x) yhat <p>we can make a prediction for some arbitrary one tensors</p> In\u00a0[\u00a0]: Copied! <pre>X_=torch.tensor([[0.0],[2.0],[3.0]])\nYhat=model(X_)\nYhat\n</pre> X_=torch.tensor([[0.0],[2.0],[3.0]]) Yhat=model(X_) Yhat <p>we  can threshold the predication</p> In\u00a0[\u00a0]: Copied! <pre>Yhat=Yhat&gt;0.5\nYhat\n</pre> Yhat=Yhat&gt;0.5 Yhat Practice <p>Repeat the previous steps above by using the MSE cost or total loss:</p> In\u00a0[\u00a0]: Copied! <pre># Practice: Train the model with MSE Loss Function\n\n# Type your code here\n</pre> # Practice: Train the model with MSE Loss Function  # Type your code here <p>Double-click here for the solution.</p> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.1_simple1hiddenlayer/#author-juma-shafara-date-2024-08-09-title-simple-one-hidden-layer-neural-network-keywords-training-two-parameter-mini-batch-gradient-decent-training-two-parameter-mini-batch-gradient-decent-description-in-this-lab-you-will-use-a-single-layer-neural-network-to-classify-non-linearly-seprable-data-in-1-ddatabase","title":"author: Juma Shafara date: \"2024-08-09\" title: Simple One Hidden Layer Neural Network keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will use a single-layer neural network to classify non linearly seprable data in 1-Ddatabase.\u00b6","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.1_simple1hiddenlayer/#Model","title":"Neural Network Module and Training Function","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.1_simple1hiddenlayer/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.1_simple1hiddenlayer/#Train","title":"Define the Neural Network, Criterion Function, Optimizer and Train the Model","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.1_simple1hiddenlayer/#about-the-author","title":"About the Author:\u00b6","text":"<p>Hi, My name is Juma Shafara. Am a Data Scientist and Instructor at DATAIDEA. I have taught hundreds of peope Programming, Data Analysis and Machine Learning.</p> <p>I also enjoy developing innovative algorithms and models that can drive insights and value.</p> <p>I regularly share some content that I find useful throughout my learning/teaching journey to simplify concepts in Machine Learning, Mathematics, Programming, and related topics on my website jumashafara.dataidea.org.</p> <p>Besides these technical stuff, I enjoy watching soccer, movies and reading mystery books.</p>"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.2multiple_neurons/","title":"this is for exercises","text":"Objective<ul><li> How to create complex Neural Network in pytorch.</li></ul> Table of Contents <ul> <li> Preperation</li> <li> Get Our Data</li> <li> Define the Neural Network, Optimizer, and Train the Model</li> </ul> <p>Estimated Time Needed: 25 min</p>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>We'll need to import the following libraries for this lab.</p> In\u00a0[1]: Copied! <pre>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n</pre> import torch import numpy as np import matplotlib.pyplot as plt  %matplotlib inline import torch.nn as nn import torch.nn.functional as F from torch.utils.data import Dataset, DataLoader <p>Define the plotting functions.</p> In\u00a0[2]: Copied! <pre>def get_hist(model,data_set):\n    activations=model.activation(data_set.x)\n    for i,activation in enumerate(activations):\n        plt.hist(activation.numpy(),4,density=True)\n        plt.title(\"Activation layer \" + str(i+1))\n        plt.xlabel(\"Activation\")\n        plt.xlabel(\"Activation\")\n        plt.legend()\n        plt.show()\n</pre> def get_hist(model,data_set):     activations=model.activation(data_set.x)     for i,activation in enumerate(activations):         plt.hist(activation.numpy(),4,density=True)         plt.title(\"Activation layer \" + str(i+1))         plt.xlabel(\"Activation\")         plt.xlabel(\"Activation\")         plt.legend()         plt.show() In\u00a0[3]: Copied! <pre>def PlotStuff(X,Y,model=None,leg=False):\n    \n    plt.plot(X[Y==0].numpy(),Y[Y==0].numpy(),'or',label='training points y=0 ' )\n    plt.plot(X[Y==1].numpy(),Y[Y==1].numpy(),'ob',label='training points y=1 ' )\n\n    if model!=None:\n        plt.plot(X.numpy(),model(X).detach().numpy(),label='neral network ')\n\n    plt.legend()\n    plt.show()\n</pre> def PlotStuff(X,Y,model=None,leg=False):          plt.plot(X[Y==0].numpy(),Y[Y==0].numpy(),'or',label='training points y=0 ' )     plt.plot(X[Y==1].numpy(),Y[Y==1].numpy(),'ob',label='training points y=1 ' )      if model!=None:         plt.plot(X.numpy(),model(X).detach().numpy(),label='neral network ')      plt.legend()     plt.show() <p>Define the class to get our dataset.</p> In\u00a0[4]: Copied! <pre>class Data(Dataset):\n    def __init__(self):\n        self.x=torch.linspace(-20, 20, 100).view(-1,1)\n  \n        self.y=torch.zeros(self.x.shape[0])\n        self.y[(self.x[:,0]&gt;-10)&amp; (self.x[:,0]&lt;-5)]=1\n        self.y[(self.x[:,0]&gt;5)&amp; (self.x[:,0]&lt;10)]=1\n        self.y=self.y.view(-1,1)\n        self.len=self.x.shape[0]\n    def __getitem__(self,index):    \n            \n        return self.x[index],self.y[index]\n    def __len__(self):\n        return self.len\n</pre> class Data(Dataset):     def __init__(self):         self.x=torch.linspace(-20, 20, 100).view(-1,1)            self.y=torch.zeros(self.x.shape[0])         self.y[(self.x[:,0]&gt;-10)&amp; (self.x[:,0]&lt;-5)]=1         self.y[(self.x[:,0]&gt;5)&amp; (self.x[:,0]&lt;10)]=1         self.y=self.y.view(-1,1)         self.len=self.x.shape[0]     def __getitem__(self,index):                          return self.x[index],self.y[index]     def __len__(self):         return self.len   <p>Define the class for creating our model.</p> In\u00a0[5]: Copied! <pre>class Net(nn.Module):\n    def __init__(self,D_in,H,D_out):\n        super(Net,self).__init__()\n        self.linear1=nn.Linear(D_in,H)\n        self.linear2=nn.Linear(H,D_out)\n\n        \n    def forward(self,x):\n        x=torch.sigmoid(self.linear1(x))  \n        x=torch.sigmoid(self.linear2(x))\n        return x\n</pre> class Net(nn.Module):     def __init__(self,D_in,H,D_out):         super(Net,self).__init__()         self.linear1=nn.Linear(D_in,H)         self.linear2=nn.Linear(H,D_out)               def forward(self,x):         x=torch.sigmoid(self.linear1(x))           x=torch.sigmoid(self.linear2(x))         return x <p>Create the function to train our model, which accumulate lost for each iteration to obtain the cost.</p> In\u00a0[6]: Copied! <pre>def train(data_set,model,criterion, train_loader, optimizer, epochs=5,plot_number=10):\n    cost=[]\n    \n    for epoch in range(epochs):\n        total=0\n        \n        for x,y in train_loader:\n            optimizer.zero_grad()\n            \n            yhat=model(x)\n            loss=criterion(yhat,y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total+=loss.item()\n            \n        if epoch%plot_number==0:\n            PlotStuff(data_set.x,data_set.y,model)\n        \n        cost.append(total)\n    plt.figure()\n    plt.plot(cost)\n    plt.xlabel('epoch')\n    plt.ylabel('cost')\n    plt.show()\n    return cost\n</pre> def train(data_set,model,criterion, train_loader, optimizer, epochs=5,plot_number=10):     cost=[]          for epoch in range(epochs):         total=0                  for x,y in train_loader:             optimizer.zero_grad()                          yhat=model(x)             loss=criterion(yhat,y)             optimizer.zero_grad()             loss.backward()             optimizer.step()             total+=loss.item()                      if epoch%plot_number==0:             PlotStuff(data_set.x,data_set.y,model)                  cost.append(total)     plt.figure()     plt.plot(cost)     plt.xlabel('epoch')     plt.ylabel('cost')     plt.show()     return cost In\u00a0[7]: Copied! <pre>data_set=Data()\n</pre> data_set=Data() In\u00a0[8]: Copied! <pre>PlotStuff(data_set.x,data_set.y,leg=False)\n</pre> PlotStuff(data_set.x,data_set.y,leg=False) <p>Create our model with 9 neurons in the hidden layer. And then create a BCE loss and an Adam optimizer.</p> In\u00a0[9]: Copied! <pre>torch.manual_seed(0)\nmodel=Net(1,9,1)\nlearning_rate=0.1\ncriterion=nn.BCELoss()\noptimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\ntrain_loader=DataLoader(dataset=data_set,batch_size=100)\nCOST=train(data_set,model,criterion, train_loader, optimizer, epochs=600,plot_number=200)\n</pre> torch.manual_seed(0) model=Net(1,9,1) learning_rate=0.1 criterion=nn.BCELoss() optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate) train_loader=DataLoader(dataset=data_set,batch_size=100) COST=train(data_set,model,criterion, train_loader, optimizer, epochs=600,plot_number=200) # this is for exercises model= torch.nn.Sequential(     torch.nn.Linear(1, 6),      torch.nn.Sigmoid(),     torch.nn.Linear(6,1),     torch.nn.Sigmoid()  ) In\u00a0[10]: Copied! <pre>plt.plot(COST)\n</pre> plt.plot(COST) Out[10]: <pre>[&lt;matplotlib.lines.Line2D at 0x707076694800&gt;]</pre> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.2multiple_neurons/#author-juma-shafara-date-2024-08-12-title-softmax-classifer-1d-keywords-training-two-parameter-mini-batch-gradient-decent-training-two-parameter-mini-batch-gradient-decent-description-how-to-create-complex-neural-network-in-pytorch","title":"author: Juma Shafara date: \"2024-08-12\" title: Softmax Classifer 1D keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: How to create complex Neural Network in pytorch.\u00b6","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.2multiple_neurons/#Prep","title":"Preparation","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.2multiple_neurons/#Data","title":"Get Our Data","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.2multiple_neurons/#Train","title":"Define the Neural Network, Optimizer and Train the Model","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/","title":"XOR Problem","text":"Practice: Neural Networks with One Hidden Layer: Noisy XOR Objective<ul><li> How to create a neural network model with multiple neurons.</li></ul> Table of Contents <p>In this lab, you will see how many neurons it takes to classify noisy XOR data with one hidden layer neural network.</p> <ul> <li> Neural Network Module and Training Function</li> <li> Make Some Data</li> <li> One Neuron</li> <li> Two Neurons</li> <li> Three Neurons</li> </ul> <p>Estimated Time Needed: 25 min</p>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> Preparation <p>We'll need the following libraries</p> In\u00a0[\u00a0]: Copied! <pre># Import the libraries we need for this lab\n\n\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt \nfrom matplotlib.colors import ListedColormap\nfrom torch.utils.data import Dataset, DataLoader\n</pre> # Import the libraries we need for this lab    import numpy as np import torch import torch.nn as nn import torch.nn.functional as F import matplotlib.pyplot as plt  from matplotlib.colors import ListedColormap from torch.utils.data import Dataset, DataLoader <p>Use the following function to plot the data:</p> In\u00a0[\u00a0]: Copied! <pre># Plot the data\n\ndef plot_decision_regions_2class(model,data_set):\n    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#00AAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])\n    X = data_set.x.numpy()\n    y = data_set.y.numpy()\n    h = .02\n    x_min, x_max = X[:, 0].min() - 0.1 , X[:, 0].max() + 0.1 \n    y_min, y_max = X[:, 1].min() - 0.1 , X[:, 1].max() + 0.1 \n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n    XX = torch.Tensor(np.c_[xx.ravel(), yy.ravel()])\n\n    yhat = np.logical_not((model(XX)[:, 0] &gt; 0.5).numpy()).reshape(xx.shape)\n    plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)\n    plt.plot(X[y[:, 0] == 0, 0], X[y[:, 0] == 0, 1], 'o', label='y=0')\n    plt.plot(X[y[:, 0] == 1, 0], X[y[:, 0] == 1, 1], 'ro', label='y=1')\n    plt.title(\"decision region\")\n    plt.legend()\n</pre> # Plot the data  def plot_decision_regions_2class(model,data_set):     cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#00AAFF'])     cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])     X = data_set.x.numpy()     y = data_set.y.numpy()     h = .02     x_min, x_max = X[:, 0].min() - 0.1 , X[:, 0].max() + 0.1      y_min, y_max = X[:, 1].min() - 0.1 , X[:, 1].max() + 0.1      xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))     XX = torch.Tensor(np.c_[xx.ravel(), yy.ravel()])      yhat = np.logical_not((model(XX)[:, 0] &gt; 0.5).numpy()).reshape(xx.shape)     plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)     plt.plot(X[y[:, 0] == 0, 0], X[y[:, 0] == 0, 1], 'o', label='y=0')     plt.plot(X[y[:, 0] == 1, 0], X[y[:, 0] == 1, 1], 'ro', label='y=1')     plt.title(\"decision region\")     plt.legend() <p>Use the following function to calculate accuracy:</p> In\u00a0[\u00a0]: Copied! <pre># Calculate the accuracy\n\ndef accuracy(model, data_set):\n    return np.mean(data_set.y.view(-1).numpy() == (model(data_set.x)[:, 0] &gt; 0.5).numpy())\n</pre> # Calculate the accuracy  def accuracy(model, data_set):     return np.mean(data_set.y.view(-1).numpy() == (model(data_set.x)[:, 0] &gt; 0.5).numpy()) <p>Define the neural network module or class:</p> In\u00a0[\u00a0]: Copied! <pre># Define the class Net with one hidden layer \n\nclass Net(nn.Module):\n    \n    # Constructor\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n        #hidden layer \n        self.linear1 = nn.Linear(D_in, H)\n        #output layer \n        self.linear2 = nn.Linear(H, D_out)\n\n    # Prediction    \n    def forward(self, x):\n        x = torch.sigmoid(self.linear1(x))  \n        x = torch.sigmoid(self.linear2(x))\n        return x\n</pre> # Define the class Net with one hidden layer   class Net(nn.Module):          # Constructor     def __init__(self, D_in, H, D_out):         super(Net, self).__init__()         #hidden layer          self.linear1 = nn.Linear(D_in, H)         #output layer          self.linear2 = nn.Linear(H, D_out)      # Prediction         def forward(self, x):         x = torch.sigmoid(self.linear1(x))           x = torch.sigmoid(self.linear2(x))         return x <p>Define a function to train the model:</p> In\u00a0[\u00a0]: Copied! <pre># Define the train model\n\ndef train(data_set, model, criterion, train_loader, optimizer, epochs=5):\n    COST = []\n    ACC = []\n    for epoch in range(epochs):\n        total=0\n        for x, y in train_loader:\n            optimizer.zero_grad()\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            #cumulative loss \n            total+=loss.item()\n        ACC.append(accuracy(model, data_set))\n        COST.append(total)\n        \n    fig, ax1 = plt.subplots()\n    color = 'tab:red'\n    ax1.plot(COST, color=color)\n    ax1.set_xlabel('epoch', color=color)\n    ax1.set_ylabel('total loss', color=color)\n    ax1.tick_params(axis='y', color=color)\n    \n    ax2 = ax1.twinx()  \n    color = 'tab:blue'\n    ax2.set_ylabel('accuracy', color=color)  # we already handled the x-label with ax1\n    ax2.plot(ACC, color=color)\n    ax2.tick_params(axis='y', color=color)\n    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n    \n    plt.show()\n\n    return COST\n</pre> # Define the train model  def train(data_set, model, criterion, train_loader, optimizer, epochs=5):     COST = []     ACC = []     for epoch in range(epochs):         total=0         for x, y in train_loader:             optimizer.zero_grad()             yhat = model(x)             loss = criterion(yhat, y)             optimizer.zero_grad()             loss.backward()             optimizer.step()             #cumulative loss              total+=loss.item()         ACC.append(accuracy(model, data_set))         COST.append(total)              fig, ax1 = plt.subplots()     color = 'tab:red'     ax1.plot(COST, color=color)     ax1.set_xlabel('epoch', color=color)     ax1.set_ylabel('total loss', color=color)     ax1.tick_params(axis='y', color=color)          ax2 = ax1.twinx()       color = 'tab:blue'     ax2.set_ylabel('accuracy', color=color)  # we already handled the x-label with ax1     ax2.plot(ACC, color=color)     ax2.tick_params(axis='y', color=color)     fig.tight_layout()  # otherwise the right y-label is slightly clipped          plt.show()      return COST <p>Dataset class:</p> In\u00a0[\u00a0]: Copied! <pre># Define the class XOR_Data\n\nclass XOR_Data(Dataset):\n    \n    # Constructor\n    def __init__(self, N_s=100):\n        self.x = torch.zeros((N_s, 2))\n        self.y = torch.zeros((N_s, 1))\n        for i in range(N_s // 4):\n            self.x[i, :] = torch.Tensor([0.0, 0.0]) \n            self.y[i, 0] = torch.Tensor([0.0])\n\n            self.x[i + N_s // 4, :] = torch.Tensor([0.0, 1.0])\n            self.y[i + N_s // 4, 0] = torch.Tensor([1.0])\n    \n            self.x[i + N_s // 2, :] = torch.Tensor([1.0, 0.0])\n            self.y[i + N_s // 2, 0] = torch.Tensor([1.0])\n    \n            self.x[i + 3 * N_s // 4, :] = torch.Tensor([1.0, 1.0])\n            self.y[i + 3 * N_s // 4, 0] = torch.Tensor([0.0])\n\n            self.x = self.x + 0.01 * torch.randn((N_s, 2))\n        self.len = N_s\n\n    # Getter\n    def __getitem__(self, index):    \n        return self.x[index],self.y[index]\n    \n    # Get Length\n    def __len__(self):\n        return self.len\n    \n    # Plot the data\n    def plot_stuff(self):\n        plt.plot(self.x[self.y[:, 0] == 0, 0].numpy(), self.x[self.y[:, 0] == 0, 1].numpy(), 'o', label=\"y=0\")\n        plt.plot(self.x[self.y[:, 0] == 1, 0].numpy(), self.x[self.y[:, 0] == 1, 1].numpy(), 'ro', label=\"y=1\")\n        plt.legend()\n</pre> # Define the class XOR_Data  class XOR_Data(Dataset):          # Constructor     def __init__(self, N_s=100):         self.x = torch.zeros((N_s, 2))         self.y = torch.zeros((N_s, 1))         for i in range(N_s // 4):             self.x[i, :] = torch.Tensor([0.0, 0.0])              self.y[i, 0] = torch.Tensor([0.0])              self.x[i + N_s // 4, :] = torch.Tensor([0.0, 1.0])             self.y[i + N_s // 4, 0] = torch.Tensor([1.0])                  self.x[i + N_s // 2, :] = torch.Tensor([1.0, 0.0])             self.y[i + N_s // 2, 0] = torch.Tensor([1.0])                  self.x[i + 3 * N_s // 4, :] = torch.Tensor([1.0, 1.0])             self.y[i + 3 * N_s // 4, 0] = torch.Tensor([0.0])              self.x = self.x + 0.01 * torch.randn((N_s, 2))         self.len = N_s      # Getter     def __getitem__(self, index):             return self.x[index],self.y[index]          # Get Length     def __len__(self):         return self.len          # Plot the data     def plot_stuff(self):         plt.plot(self.x[self.y[:, 0] == 0, 0].numpy(), self.x[self.y[:, 0] == 0, 1].numpy(), 'o', label=\"y=0\")         plt.plot(self.x[self.y[:, 0] == 1, 0].numpy(), self.x[self.y[:, 0] == 1, 1].numpy(), 'ro', label=\"y=1\")         plt.legend() <p>Dataset object:</p> In\u00a0[\u00a0]: Copied! <pre># Create dataset object\n\ndata_set = XOR_Data()\ndata_set.plot_stuff()\n</pre> # Create dataset object  data_set = XOR_Data() data_set.plot_stuff() Try <p>Create a neural network <code>model</code> with one neuron. Then, use the following code to train it:</p> In\u00a0[\u00a0]: Copied! <pre># Practice: create a model with one neuron\n\n# Type your code here\n</pre> # Practice: create a model with one neuron  # Type your code here <p>Double-click here for the solution.</p> In\u00a0[\u00a0]: Copied! <pre># Train the model\n\nlearning_rate = 0.001\ncriterion = nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ntrain_loader = DataLoader(dataset=data_set, batch_size=1)\nLOSS12 = train(data_set, model, criterion, train_loader, optimizer, epochs=500)\nplot_decision_regions_2class(model, data_set)\n</pre> # Train the model  learning_rate = 0.001 criterion = nn.BCELoss() optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) train_loader = DataLoader(dataset=data_set, batch_size=1) LOSS12 = train(data_set, model, criterion, train_loader, optimizer, epochs=500) plot_decision_regions_2class(model, data_set) Try <p>Create a neural network <code>model</code> with two neurons. Then, use the following code to train it:</p> In\u00a0[\u00a0]: Copied! <pre># Practice: create a model with two neuron\n\n# Type your code here\n</pre> # Practice: create a model with two neuron  # Type your code here <p>Double-click here for the solution.</p> In\u00a0[\u00a0]: Copied! <pre># Train the model\n\nlearning_rate = 0.1\ncriterion = nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ntrain_loader = DataLoader(dataset=data_set, batch_size=1)\nLOSS12 = train(data_set, model, criterion, train_loader, optimizer, epochs=500)\nplot_decision_regions_2class(model, data_set)\n</pre> # Train the model  learning_rate = 0.1 criterion = nn.BCELoss() optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) train_loader = DataLoader(dataset=data_set, batch_size=1) LOSS12 = train(data_set, model, criterion, train_loader, optimizer, epochs=500) plot_decision_regions_2class(model, data_set) Try <p>Create a neural network <code>model</code> with three neurons. Then, use the following code to train it:</p> In\u00a0[\u00a0]: Copied! <pre># Practice: create a model with two neuron\nmodel = Net(2, 4, 1)\n# Type your code here\n</pre> # Practice: create a model with two neuron model = Net(2, 4, 1) # Type your code here <p>Double-click here for the solution.</p> In\u00a0[\u00a0]: Copied! <pre># Train the model\n\nlearning_rate = 0.1\ncriterion = nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ntrain_loader = DataLoader(dataset=data_set, batch_size=1)\nLOSS12 = train(data_set, model, criterion, train_loader, optimizer, epochs=500)\nplot_decision_regions_2class(model, data_set)\n</pre> # Train the model  learning_rate = 0.1 criterion = nn.BCELoss() optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) train_loader = DataLoader(dataset=data_set, batch_size=1) LOSS12 = train(data_set, model, criterion, train_loader, optimizer, epochs=500) plot_decision_regions_2class(model, data_set)  What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/#author-juma-shafara-date-2024-08-12-title-noisy-xo-keywords-training-two-parameter-mini-batch-gradient-decent-training-two-parameter-mini-batch-gradient-decent-description-in-this-lab-you-will-see-how-many-neurons-it-takes-to-classify-noisy-xor-data-with-one-hidden-layer-neural-network","title":"author: Juma Shafara date: \"2024-08-12\" title: Noisy XO keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will see how many neurons it takes to classify noisy XOR data with one hidden layer neural network.\u00b6","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/#Model","title":"Neural Network Module and Training Function","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/#One","title":"One Neuron","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/#Two","title":"Two Neurons","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.3xor_v2/#Three","title":"Three Neurons","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.4one_layer_neural_network_MNIST/","title":"MNIST","text":"Neural Networks with One Hidden Layer Objective<ul><li> How to classify handwritten digits using Neural Network.</li></ul> Table of Contents <p>In this lab, you will use a single layer neural network to classify handwritten digits from the MNIST database.</p> <ul> <li>Neural Network Module and Training Function</li> <li>Make Some Data</li> <li>Define the Neural Network, Optimizer, and Train the  Model</li> <li>Analyze Results</li> </ul> <p>Estimated Time Needed: 25 min</p>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> Preparation <p>We'll need the following libraries</p> In\u00a0[2]: Copied! <pre># Import the libraries we need for this lab\n\n# Using the following line code to install the torchvision library\n# !mamba install -y torchvision\n\nimport torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport torch.nn.functional as F\nimport matplotlib.pylab as plt\nimport numpy as np\n</pre> # Import the libraries we need for this lab  # Using the following line code to install the torchvision library # !mamba install -y torchvision  import torch  import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets import torch.nn.functional as F import matplotlib.pylab as plt import numpy as np <p>Use the following helper functions for plotting the loss:</p> In\u00a0[3]: Copied! <pre># Define a function to plot accuracy and loss\n\ndef plot_accuracy_loss(training_results): \n    plt.subplot(2, 1, 1)\n    plt.plot(training_results['training_loss'], 'r')\n    plt.ylabel('loss')\n    plt.title('training loss iterations')\n    plt.subplot(2, 1, 2)\n    plt.plot(training_results['validation_accuracy'])\n    plt.ylabel('accuracy')\n    plt.xlabel('epochs')   \n    plt.show()\n</pre> # Define a function to plot accuracy and loss  def plot_accuracy_loss(training_results):      plt.subplot(2, 1, 1)     plt.plot(training_results['training_loss'], 'r')     plt.ylabel('loss')     plt.title('training loss iterations')     plt.subplot(2, 1, 2)     plt.plot(training_results['validation_accuracy'])     plt.ylabel('accuracy')     plt.xlabel('epochs')        plt.show() <p>Use the following function for printing the model parameters:</p> In\u00a0[4]: Copied! <pre># Define a function to plot model parameters\n\ndef print_model_parameters(model):\n    count = 0\n    for ele in model.state_dict():\n        count += 1\n        if count % 2 != 0:\n            print (\"The following are the parameters for the layer \", count // 2 + 1)\n        if ele.find(\"bias\") != -1:\n            print(\"The size of bias: \", model.state_dict()[ele].size())\n        else:\n            print(\"The size of weights: \", model.state_dict()[ele].size())\n</pre> # Define a function to plot model parameters  def print_model_parameters(model):     count = 0     for ele in model.state_dict():         count += 1         if count % 2 != 0:             print (\"The following are the parameters for the layer \", count // 2 + 1)         if ele.find(\"bias\") != -1:             print(\"The size of bias: \", model.state_dict()[ele].size())         else:             print(\"The size of weights: \", model.state_dict()[ele].size()) <p>Define the neural network module or class:</p> In\u00a0[5]: Copied! <pre># Define a function to display data\n\ndef show_data(data_sample):\n    plt.imshow(data_sample.numpy().reshape(28, 28), cmap='gray')\n    plt.show()\n</pre> # Define a function to display data  def show_data(data_sample):     plt.imshow(data_sample.numpy().reshape(28, 28), cmap='gray')     plt.show() <p>Define the neural network module or class:</p> In\u00a0[6]: Copied! <pre># Define a Neural Network class\n\nclass Net(nn.Module):\n    \n    # Constructor\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n\n    # Prediction    \n    def forward(self, x):\n        x = torch.sigmoid(self.linear1(x))  \n        x = self.linear2(x)\n        return x\n</pre> # Define a Neural Network class  class Net(nn.Module):          # Constructor     def __init__(self, D_in, H, D_out):         super(Net, self).__init__()         self.linear1 = nn.Linear(D_in, H)         self.linear2 = nn.Linear(H, D_out)      # Prediction         def forward(self, x):         x = torch.sigmoid(self.linear1(x))           x = self.linear2(x)         return x <p>Define a function to train the model. In this case, the function returns a Python dictionary to store the training loss and accuracy on the validation data.</p> In\u00a0[7]: Copied! <pre># Define a training function to train the model\n\ndef train(model, criterion, train_loader, validation_loader, optimizer, epochs=100):\n    i = 0\n    useful_stuff = {'training_loss': [],'validation_accuracy': []}  \n    for epoch in range(epochs):\n        for i, (x, y) in enumerate(train_loader): \n            optimizer.zero_grad()\n            z = model(x.view(-1, 28 * 28))\n            loss = criterion(z, y)\n            loss.backward()\n            optimizer.step()\n             #loss for every iteration\n            useful_stuff['training_loss'].append(loss.data.item())\n        correct = 0\n        for x, y in validation_loader:\n            #validation \n            z = model(x.view(-1, 28 * 28))\n            _, label = torch.max(z, 1)\n            correct += (label == y).sum().item()\n        accuracy = 100 * (correct / len(validation_dataset))\n        useful_stuff['validation_accuracy'].append(accuracy)\n    return useful_stuff\n</pre> # Define a training function to train the model  def train(model, criterion, train_loader, validation_loader, optimizer, epochs=100):     i = 0     useful_stuff = {'training_loss': [],'validation_accuracy': []}       for epoch in range(epochs):         for i, (x, y) in enumerate(train_loader):              optimizer.zero_grad()             z = model(x.view(-1, 28 * 28))             loss = criterion(z, y)             loss.backward()             optimizer.step()              #loss for every iteration             useful_stuff['training_loss'].append(loss.data.item())         correct = 0         for x, y in validation_loader:             #validation              z = model(x.view(-1, 28 * 28))             _, label = torch.max(z, 1)             correct += (label == y).sum().item()         accuracy = 100 * (correct / len(validation_dataset))         useful_stuff['validation_accuracy'].append(accuracy)     return useful_stuff <p>Load the training dataset by setting the parameters <code>train</code> to <code>True</code> and convert it to a tensor by placing a transform object in the argument <code>transform</code>.</p> In\u00a0[8]: Copied! <pre># Create training dataset\n\ntrain_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n</pre> # Create training dataset  train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor()) <pre>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n</pre> <pre>100.0%\n</pre> <pre>Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n</pre> <pre>100.0%\n</pre> <pre>Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n</pre> <pre>100.0%\n</pre> <pre>Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n</pre> <pre>100.0%</pre> <pre>Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n</pre> <pre>\n</pre> <p>Load the testing dataset and convert it to a tensor by placing a transform object in the argument <code>transform</code>:</p> In\u00a0[9]: Copied! <pre># Create validating dataset\n\nvalidation_dataset = dsets.MNIST(root='./data', download=False, transform=transforms.ToTensor())\n</pre> # Create validating dataset  validation_dataset = dsets.MNIST(root='./data', download=False, transform=transforms.ToTensor()) <p>Create the criterion function:</p> In\u00a0[10]: Copied! <pre># Create criterion function\n\ncriterion = nn.CrossEntropyLoss()\n</pre> # Create criterion function  criterion = nn.CrossEntropyLoss() <p>Create the training-data loader and the validation-data loader objects:</p> In\u00a0[11]: Copied! <pre># Create data loader for both train dataset and valdiate dataset\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)\n</pre> # Create data loader for both train dataset and valdiate dataset  train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True) validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False) <p>Create the model with 100 neurons:</p> In\u00a0[12]: Copied! <pre># Create the model with 100 neurons\n\ninput_dim = 28 * 28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = Net(input_dim, hidden_dim, output_dim)\n</pre> # Create the model with 100 neurons  input_dim = 28 * 28 hidden_dim = 100 output_dim = 10  model = Net(input_dim, hidden_dim, output_dim) <p>Print the model parameters:</p> In\u00a0[13]: Copied! <pre># Print the parameters for model\n\nprint_model_parameters(model)\n</pre> # Print the parameters for model  print_model_parameters(model) <pre>The following are the parameters for the layer  1\nThe size of weights:  torch.Size([100, 784])\nThe size of bias:  torch.Size([100])\nThe following are the parameters for the layer  2\nThe size of weights:  torch.Size([10, 100])\nThe size of bias:  torch.Size([10])\n</pre> <p>Define the optimizer object with a learning rate of 0.01:</p> In\u00a0[14]: Copied! <pre># Set the learning rate and the optimizer\n\nlearning_rate = 0.01\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n</pre> # Set the learning rate and the optimizer  learning_rate = 0.01 optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) <p>Train the model by using 100 epochs (this process takes time):</p> In\u00a0[15]: Copied! <pre># Train the model\n\ntraining_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=30)\n</pre> # Train the model  training_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=30) <p>Plot the training total loss or cost for every iteration and plot the training accuracy for every epoch:</p> In\u00a0[16]: Copied! <pre># Plot the accuracy and loss\n\nplot_accuracy_loss(training_results)\n</pre> # Plot the accuracy and loss  plot_accuracy_loss(training_results) <p>Plot the first five misclassified samples:</p> In\u00a0[17]: Copied! <pre># Plot the first five misclassified samples\n\ncount = 0\nfor x, y in validation_dataset:\n    z = model(x.reshape(-1, 28 * 28))\n    _,yhat = torch.max(z, 1)\n    if yhat != y:\n        show_data(x)\n        count += 1\n    if count &gt;= 5:\n        break\n</pre> # Plot the first five misclassified samples  count = 0 for x, y in validation_dataset:     z = model(x.reshape(-1, 28 * 28))     _,yhat = torch.max(z, 1)     if yhat != y:         show_data(x)         count += 1     if count &gt;= 5:         break Practice <p>Use <code>nn.Sequential</code> to build exactly the same model as you just built. Use the function train to train the model and use the function <code>plot_accuracy_loss</code> to see the metrics. Also, try different epoch numbers.</p> In\u00a0[18]: Copied! <pre># Practice: Use nn.Sequential to build the same model. Use plot_accuracy_loss to print out the accuarcy and loss\n\n# Type your code here\n</pre> # Practice: Use nn.Sequential to build the same model. Use plot_accuracy_loss to print out the accuarcy and loss  # Type your code here <p>Double-click here for the solution.</p> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.4one_layer_neural_network_MNIST/#author-juma-shafara-date-2024-08-12-title-one-hidden-layer-keywords-training-two-parameter-mini-batch-gradient-decent-training-two-parameter-mini-batch-gradient-decent-description-in-this-lab-you-will-see-how-many-neurons-it-takes-to-classify-noisy-xor-data-with-one-hidden-layer-neural-network","title":"author: Juma Shafara date: \"2024-08-12\" title: One Hidden Layer keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will see how many neurons it takes to classify noisy XOR data with one hidden layer neural network.\u00b6","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.4one_layer_neural_network_MNIST/#Model","title":"Neural Network Module and Training Function","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.4one_layer_neural_network_MNIST/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.4one_layer_neural_network_MNIST/#Train","title":"Define the Neural Network, Optimizer, and Train the Model","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.4one_layer_neural_network_MNIST/#Result","title":"Analyze Results","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.1activationfuction_v2/","title":"Activation Functions","text":"Objective<ul><li> How to apply different Activation functions in Neural Network.</li></ul> Table of Contents <p>In this lab, you will cover logistic regression by using PyTorch.</p> <ul> <li> Logistic Function</li> <li> Tanh</li> <li> Relu</li> <li> Compare Activation Functions</li> </ul> <p>Estimated Time Needed: 15 min</p>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>We'll need the following libraries</p> In\u00a0[1]: Copied! <pre># Import the libraries we need for this lab\n\nimport torch.nn as nn\nimport torch\n\nimport matplotlib.pyplot as plt\ntorch.manual_seed(2)\n</pre> # Import the libraries we need for this lab  import torch.nn as nn import torch  import matplotlib.pyplot as plt torch.manual_seed(2) Out[1]: <pre>&lt;torch._C.Generator at 0x7759bb532ff0&gt;</pre> <p>Create a tensor ranging from -10 to 10:</p> In\u00a0[2]: Copied! <pre># Create a tensor\n\nz = torch.arange(-10, 10, 0.1,).view(-1, 1)\n</pre> # Create a tensor  z = torch.arange(-10, 10, 0.1,).view(-1, 1) <p>When you use sequential, you can create a sigmoid object:</p> In\u00a0[3]: Copied! <pre># Create a sigmoid object\n\nsig = nn.Sigmoid()\n</pre> # Create a sigmoid object  sig = nn.Sigmoid() <p>Apply the element-wise function Sigmoid with the object:</p> In\u00a0[4]: Copied! <pre># Make a prediction of sigmoid function\n\nyhat = sig(z)\n</pre> # Make a prediction of sigmoid function  yhat = sig(z) <p>Plot the results:</p> In\u00a0[5]: Copied! <pre># Plot the result\n\nplt.plot(z.detach().numpy(),yhat.detach().numpy())\nplt.xlabel('z')\nplt.ylabel('yhat')\n</pre> # Plot the result  plt.plot(z.detach().numpy(),yhat.detach().numpy()) plt.xlabel('z') plt.ylabel('yhat') Out[5]: <pre>Text(0, 0.5, 'yhat')</pre> <p>For custom modules, call the sigmoid from the torch (<code>nn.functional</code> for the old version), which applies the element-wise sigmoid from the function module and plots the results:</p> In\u00a0[6]: Copied! <pre># Use the build in function to predict the result\n\nyhat = torch.sigmoid(z)\nplt.plot(z.numpy(), yhat.numpy())\n\nplt.show()\n</pre> # Use the build in function to predict the result  yhat = torch.sigmoid(z) plt.plot(z.numpy(), yhat.numpy())  plt.show() <p>When you use sequential, you can create a tanh object:</p> In\u00a0[7]: Copied! <pre># Create a tanh object\n\nTANH = nn.Tanh()\n</pre> # Create a tanh object  TANH = nn.Tanh() <p>Call the object and plot it:</p> In\u00a0[8]: Copied! <pre># Make the prediction using tanh object\n\nyhat = TANH(z)\nplt.plot(z.numpy(), yhat.numpy())\nplt.show()\n</pre> # Make the prediction using tanh object  yhat = TANH(z) plt.plot(z.numpy(), yhat.numpy()) plt.show() <p>For custom modules, call the Tanh object from the torch (nn.functional for the old version), which applies the element-wise sigmoid from the function module and plots the results:</p> In\u00a0[9]: Copied! <pre># Make the prediction using the build-in tanh object\n\nyhat = torch.tanh(z)\nplt.plot(z.numpy(), yhat.numpy())\nplt.show()\n</pre> # Make the prediction using the build-in tanh object  yhat = torch.tanh(z) plt.plot(z.numpy(), yhat.numpy()) plt.show() <p>When you use sequential, you can create a Relu object:</p> In\u00a0[10]: Copied! <pre># Create a relu object and make the prediction\n\nRELU = nn.ReLU()\nyhat = RELU(z)\nplt.plot(z.numpy(), yhat.numpy())\n</pre> # Create a relu object and make the prediction  RELU = nn.ReLU() yhat = RELU(z) plt.plot(z.numpy(), yhat.numpy()) Out[10]: <pre>[&lt;matplotlib.lines.Line2D at 0x77595cfe2c60&gt;]</pre> <p>For custom modules, call the relu object from the nn.functional, which applies the element-wise sigmoid from the function module and plots the results:</p> In\u00a0[11]: Copied! <pre># Use the build-in function to make the prediction\n\nyhat = torch.relu(z)\nplt.plot(z.numpy(), yhat.numpy())\nplt.show()\n</pre> # Use the build-in function to make the prediction  yhat = torch.relu(z) plt.plot(z.numpy(), yhat.numpy()) plt.show() <p></p>  Compare Activation Functions  In\u00a0[12]: Copied! <pre># Plot the results to compare the activation functions\n\nx = torch.arange(-2, 2, 0.1).view(-1, 1)\nplt.plot(x.numpy(), torch.relu(x).numpy(), label='relu')\nplt.plot(x.numpy(), torch.sigmoid(x).numpy(), label='sigmoid')\nplt.plot(x.numpy(), torch.tanh(x).numpy(), label='tanh')\nplt.legend()\n</pre> # Plot the results to compare the activation functions  x = torch.arange(-2, 2, 0.1).view(-1, 1) plt.plot(x.numpy(), torch.relu(x).numpy(), label='relu') plt.plot(x.numpy(), torch.sigmoid(x).numpy(), label='sigmoid') plt.plot(x.numpy(), torch.tanh(x).numpy(), label='tanh') plt.legend() Out[12]: <pre>&lt;matplotlib.legend.Legend at 0x77595f989340&gt;</pre> <p></p>  Practice  <p>Compare the activation functions with a tensor in the range (-1, 1)</p> In\u00a0[\u00a0]: Copied! <pre># Practice: Compare the activation functions again using a tensor in the range (-1, 1)\n\n# Type your code here\n</pre> # Practice: Compare the activation functions again using a tensor in the range (-1, 1)  # Type your code here <p>Double-click here for the solution.</p> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.1activationfuction_v2/#author-juma-shafara-date-2024-08-12-title-activation-functions-keywords-training-two-parameter-mini-batch-gradient-decent-training-two-parameter-mini-batch-gradient-decent-description-how-to-apply-different-activation-functions-in-neural-network","title":"author: Juma Shafara date: \"2024-08-12\" title: Activation Functions keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: How to apply different Activation functions in Neural Network.\u00b6","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.1activationfuction_v2/#Log","title":"Logistic Function","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.1activationfuction_v2/#Tanh","title":"Tanh","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.1activationfuction_v2/#Relu","title":"Relu","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.2mist1layer_v2/","title":"MNIST One Layer","text":"Test Sigmoid, Tanh, and Relu Activations Functions on the MNIST Dataset Objective<ul><li> How to apply different activation functions on the MNIST dataset.</li></ul> Table of Contents <p>In this lab, you will test sigmoid, tanh, and relu activation functions on the MNIST dataset.</p> <ul> <li>Neural Network Module and Training Function</li> <li>Make Some Data</li> <li>Define Several Neural Network, Criterion Function, and Optimizer</li> <li>Test Sigmoid, Tanh, and Relu</li> <li>Analyze Results</li> </ul> <p></p> Estimated Time Needed: 25 min  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> Preparation <p>We'll need the following libraries</p> In\u00a0[1]: Copied! <pre># Uncomment the following line to install the torchvision library\n# !mamba install -y torchvision\n\n# Import the libraries we need for this lab\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\nimport matplotlib.pylab as plt\nimport numpy as np\n</pre> # Uncomment the following line to install the torchvision library # !mamba install -y torchvision  # Import the libraries we need for this lab  import torch import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets  import matplotlib.pylab as plt import numpy as np <p>Define the neural network module or class using the sigmoid activation function:</p> In\u00a0[2]: Copied! <pre># Build the model with sigmoid function\n\nclass Net(nn.Module):\n    \n    # Constructor\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n    \n    # Prediction\n    def forward(self, x):\n        x = torch.sigmoid(self.linear1(x))  \n        x = self.linear2(x)\n        return x\n</pre> # Build the model with sigmoid function  class Net(nn.Module):          # Constructor     def __init__(self, D_in, H, D_out):         super(Net, self).__init__()         self.linear1 = nn.Linear(D_in, H)         self.linear2 = nn.Linear(H, D_out)          # Prediction     def forward(self, x):         x = torch.sigmoid(self.linear1(x))           x = self.linear2(x)         return x <p>Define the neural network module or class using the Tanh activation function:</p> In\u00a0[3]: Copied! <pre># Build the model with Tanh function\n\nclass NetTanh(nn.Module):\n\n    # Constructor\n    def __init__(self, D_in, H, D_out):\n        super(NetTanh, self).__init__()\n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n\n    # Prediction\n    def forward(self, x):\n        x = torch.tanh(self.linear1(x))\n        x = self.linear2(x)\n        return x\n</pre> # Build the model with Tanh function  class NetTanh(nn.Module):      # Constructor     def __init__(self, D_in, H, D_out):         super(NetTanh, self).__init__()         self.linear1 = nn.Linear(D_in, H)         self.linear2 = nn.Linear(H, D_out)      # Prediction     def forward(self, x):         x = torch.tanh(self.linear1(x))         x = self.linear2(x)         return x <p>Define the neural network module or class using the Relu activation function:</p> In\u00a0[4]: Copied! <pre># Build the model with Relu function\n\nclass NetRelu(nn.Module):\n\n    # Constructor\n    def __init__(self, D_in, H, D_out):\n        super(NetRelu, self).__init__()\n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n\n    # Prediction\n    def forward(self, x):\n        x = torch.relu(self.linear1(x))\n        x = self.linear2(x)\n        return x\n</pre> # Build the model with Relu function  class NetRelu(nn.Module):      # Constructor     def __init__(self, D_in, H, D_out):         super(NetRelu, self).__init__()         self.linear1 = nn.Linear(D_in, H)         self.linear2 = nn.Linear(H, D_out)      # Prediction     def forward(self, x):         x = torch.relu(self.linear1(x))         x = self.linear2(x)         return x <p>Define a function to train the model. In this case, the function returns a Python dictionary to store the training loss for each iteration  and accuracy on the validation data.</p> In\u00a0[5]: Copied! <pre># Define the function for training the model\n\ndef train(model, criterion, train_loader, validation_loader, optimizer, epochs = 100):\n    i = 0\n    useful_stuff = {'training_loss':[], 'validation_accuracy':[]}  \n\n    for epoch in range(epochs):\n        for i, (x, y) in enumerate(train_loader):\n            optimizer.zero_grad()\n            z = model(x.view(-1, 28 * 28))\n            loss = criterion(z, y)\n            loss.backward()\n            optimizer.step()\n            useful_stuff['training_loss'].append(loss.item())\n\n        correct = 0\n        for x, y in validation_loader:\n            z = model(x.view(-1, 28 * 28))\n            _, label=torch.max(z, 1)\n            correct += (label == y).sum().item()\n        accuracy = 100 * (correct / len(validation_dataset))\n        useful_stuff['validation_accuracy'].append(accuracy)\n\n    return useful_stuff\n</pre> # Define the function for training the model  def train(model, criterion, train_loader, validation_loader, optimizer, epochs = 100):     i = 0     useful_stuff = {'training_loss':[], 'validation_accuracy':[]}        for epoch in range(epochs):         for i, (x, y) in enumerate(train_loader):             optimizer.zero_grad()             z = model(x.view(-1, 28 * 28))             loss = criterion(z, y)             loss.backward()             optimizer.step()             useful_stuff['training_loss'].append(loss.item())          correct = 0         for x, y in validation_loader:             z = model(x.view(-1, 28 * 28))             _, label=torch.max(z, 1)             correct += (label == y).sum().item()         accuracy = 100 * (correct / len(validation_dataset))         useful_stuff['validation_accuracy'].append(accuracy)      return useful_stuff <p>Load the training dataset by setting the parameters <code>train</code> to <code>True</code> and convert it to a tensor by placing a transform object in the argument <code>transform</code>.</p> In\u00a0[6]: Copied! <pre># Create the training dataset\n\ntrain_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n</pre> # Create the training dataset  train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor()) <p>Load the testing dataset by setting the parameter <code>train</code> to <code>False</code> and convert it to a tensor by placing a transform object in the argument <code>transform</code>.</p> In\u00a0[7]: Copied! <pre># Create the validation  dataset\n\nvalidation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n</pre> # Create the validation  dataset  validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor()) <p>Create the criterion function:</p> In\u00a0[8]: Copied! <pre># Create the criterion function\n\ncriterion = nn.CrossEntropyLoss()\n</pre> # Create the criterion function  criterion = nn.CrossEntropyLoss() <p>Create the training-data loader and the validation-data loader object:</p> In\u00a0[9]: Copied! <pre># Create the training data loader and validation data loader object\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)\n</pre> # Create the training data loader and validation data loader object  train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True) validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False) <p>Create the criterion function:</p> In\u00a0[10]: Copied! <pre># Create the criterion function\n\ncriterion = nn.CrossEntropyLoss()\n</pre> # Create the criterion function  criterion = nn.CrossEntropyLoss() <p>Create the model with 100 hidden neurons:</p> In\u00a0[11]: Copied! <pre># Create the model object\n\ninput_dim = 28 * 28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = Net(input_dim, hidden_dim, output_dim)\n</pre> # Create the model object  input_dim = 28 * 28 hidden_dim = 100 output_dim = 10  model = Net(input_dim, hidden_dim, output_dim) <p>Train the network by using the sigmoid activations function:</p> In\u00a0[12]: Copied! <pre># Train a model with sigmoid function\n\nlearning_rate = 0.01\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ntraining_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=30)\n</pre> # Train a model with sigmoid function  learning_rate = 0.01 optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) training_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=30) <p>Train the network by using the Tanh activations function:</p> In\u00a0[\u00a0]: Copied! <pre># Train a model with Tanh function\n\nmodel_Tanh = NetTanh(input_dim, hidden_dim, output_dim)\noptimizer = torch.optim.SGD(model_Tanh.parameters(), lr=learning_rate)\ntraining_results_tanch = train(model_Tanh, criterion, train_loader, validation_loader, optimizer, epochs=30)\n</pre> # Train a model with Tanh function  model_Tanh = NetTanh(input_dim, hidden_dim, output_dim) optimizer = torch.optim.SGD(model_Tanh.parameters(), lr=learning_rate) training_results_tanch = train(model_Tanh, criterion, train_loader, validation_loader, optimizer, epochs=30) <p>Train the network by using the Relu activations function:</p> In\u00a0[\u00a0]: Copied! <pre># Train a model with Relu function\n\nmodelRelu = NetRelu(input_dim, hidden_dim, output_dim)\noptimizer = torch.optim.SGD(modelRelu.parameters(), lr=learning_rate)\ntraining_results_relu = train(modelRelu, criterion, train_loader, validation_loader, optimizer, epochs=30)\n</pre> # Train a model with Relu function  modelRelu = NetRelu(input_dim, hidden_dim, output_dim) optimizer = torch.optim.SGD(modelRelu.parameters(), lr=learning_rate) training_results_relu = train(modelRelu, criterion, train_loader, validation_loader, optimizer, epochs=30) <p>Compare the training loss for each activation:</p> In\u00a0[\u00a0]: Copied! <pre># Compare the training loss\n\nplt.plot(training_results_tanch['training_loss'], label='tanh')\nplt.plot(training_results['training_loss'], label='sigmoid')\nplt.plot(training_results_relu['training_loss'], label='relu')\nplt.ylabel('loss')\nplt.title('training loss iterations')\nplt.legend()\nplt.show()\n</pre> # Compare the training loss  plt.plot(training_results_tanch['training_loss'], label='tanh') plt.plot(training_results['training_loss'], label='sigmoid') plt.plot(training_results_relu['training_loss'], label='relu') plt.ylabel('loss') plt.title('training loss iterations') plt.legend() plt.show() <p>Compare the validation loss for each model:</p> In\u00a0[\u00a0]: Copied! <pre># Compare the validation loss\n\nplt.plot(training_results_tanch['validation_accuracy'], label='tanh')\nplt.plot(training_results['validation_accuracy'], label='sigmoid')\nplt.plot(training_results_relu['validation_accuracy'], label='relu') \nplt.ylabel('validation accuracy')\nplt.xlabel('epochs ')\nplt.legend()\nplt.show()\n</pre> # Compare the validation loss  plt.plot(training_results_tanch['validation_accuracy'], label='tanh') plt.plot(training_results['validation_accuracy'], label='sigmoid') plt.plot(training_results_relu['validation_accuracy'], label='relu')  plt.ylabel('validation accuracy') plt.xlabel('epochs ') plt.legend() plt.show()"},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.2mist1layer_v2/#author-juma-shafara-date-2024-08-12-title-test-activation-functions-on-mnist-keywords-training-two-parameter-mini-batch-gradient-decent-training-two-parameter-mini-batch-gradient-decent-description-in-this-lab-you-will-test-sigmoid-tanh-and-relu-activation-functions-on-the-mnist-dataset","title":"author: Juma Shafara date: \"2024-08-12\" title: Test Activation Functions on MNIST keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will test sigmoid, tanh, and relu activation functions on the MNIST dataset.\u00b6","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.2mist1layer_v2/#Model","title":"Neural Network Module and Training Function","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.2mist1layer_v2/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.2mist1layer_v2/#Train","title":"Define the Neural Network, Criterion Function, Optimizer, and Train the Model","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.2mist1layer_v2/#Test","title":"Test Sigmoid, Tanh, and Relu","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.2mist1layer_v2/#Result","title":"Analyze Results","text":""},{"location":"Deep%20Learning/Week7-Shallow-Networks/7.5.2mist1layer_v2/#which-activation-function-performed-best","title":"Which activation function performed best ?\u00b6","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.1mist2layer_v2/","title":"MNIST Two Layer","text":"Hidden Layer Deep Network: Sigmoid, Tanh and Relu Activations Functions MNIST Dataset Objective for this Notebook  1. Define Several Neural Network, Criterion function, Optimizer.  2. Test Sigmoid ,Tanh and Relu.   3. Analyse Results.  Table of Contents <p>In this lab, you will test Sigmoid, Tanh and Relu activation functions on the MNIST dataset with two hidden Layers.</p> <ul> <li>Neural Network Module and Training Function</li> <li>Make Some Data</li> <li>Define Several Neural Network, Criterion function, Optimizer</li> <li>Test Sigmoid ,Tanh and Relu </li> <li>Analyse Results</li> </ul> <p>Estimated Time Needed: 25 min</p> <p>We'll need the following libraries</p> In\u00a0[\u00a0]: Copied! <pre># Import the libraries we need for this lab\n\n# Using the following line code to install the torchvision library\n# !mamba install -y torchvision\n\n!pip install torchvision==0.9.1 torch==1.8.1 \nimport torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport torch.nn.functional as F\nimport matplotlib.pylab as plt\nimport numpy as np\ntorch.manual_seed(2)\n</pre> # Import the libraries we need for this lab  # Using the following line code to install the torchvision library # !mamba install -y torchvision  !pip install torchvision==0.9.1 torch==1.8.1  import torch  import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets import torch.nn.functional as F import matplotlib.pylab as plt import numpy as np torch.manual_seed(2) <p>Define the neural network module or class, with two hidden Layers</p> In\u00a0[\u00a0]: Copied! <pre># Create the model class using sigmoid as the activation function\n\nclass Net(nn.Module):\n    \n    # Constructor\n    def __init__(self, D_in, H1, H2, D_out):\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(D_in, H1)\n        self.linear2 = nn.Linear(H1, H2)\n        self.linear3 = nn.Linear(H2, D_out)\n    \n    # Prediction\n    def forward(self,x):\n        x = torch.sigmoid(self.linear1(x)) \n        x = torch.sigmoid(self.linear2(x))\n        x = self.linear3(x)\n        return x\n</pre> # Create the model class using sigmoid as the activation function  class Net(nn.Module):          # Constructor     def __init__(self, D_in, H1, H2, D_out):         super(Net, self).__init__()         self.linear1 = nn.Linear(D_in, H1)         self.linear2 = nn.Linear(H1, H2)         self.linear3 = nn.Linear(H2, D_out)          # Prediction     def forward(self,x):         x = torch.sigmoid(self.linear1(x))          x = torch.sigmoid(self.linear2(x))         x = self.linear3(x)         return x <p>Define the class with the Tanh activation function</p> In\u00a0[\u00a0]: Copied! <pre># Create the model class using Tanh as a activation function\n\nclass NetTanh(nn.Module):\n    \n    # Constructor\n    def __init__(self, D_in, H1, H2, D_out):\n        super(NetTanh, self).__init__()\n        self.linear1 = nn.Linear(D_in, H1)\n        self.linear2 = nn.Linear(H1, H2)\n        self.linear3 = nn.Linear(H2, D_out)\n    \n    # Prediction\n    def forward(self, x):\n        x = torch.tanh(self.linear1(x))\n        x = torch.tanh(self.linear2(x))\n        x = self.linear3(x)\n        return x\n</pre> # Create the model class using Tanh as a activation function  class NetTanh(nn.Module):          # Constructor     def __init__(self, D_in, H1, H2, D_out):         super(NetTanh, self).__init__()         self.linear1 = nn.Linear(D_in, H1)         self.linear2 = nn.Linear(H1, H2)         self.linear3 = nn.Linear(H2, D_out)          # Prediction     def forward(self, x):         x = torch.tanh(self.linear1(x))         x = torch.tanh(self.linear2(x))         x = self.linear3(x)         return x <p>Define the class for the Relu activation function</p> In\u00a0[\u00a0]: Copied! <pre># Create the model class using Relu as a activation function\n\nclass NetRelu(nn.Module):\n    \n    # Constructor\n    def __init__(self, D_in, H1, H2, D_out):\n        super(NetRelu, self).__init__()\n        self.linear1 = nn.Linear(D_in, H1)\n        self.linear2 = nn.Linear(H1, H2)\n        self.linear3 = nn.Linear(H2, D_out)\n    \n    # Prediction\n    def forward(self, x):\n        x = torch.relu(self.linear1(x))  \n        x = torch.relu(self.linear2(x))\n        x = self.linear3(x)\n        return x\n</pre> # Create the model class using Relu as a activation function  class NetRelu(nn.Module):          # Constructor     def __init__(self, D_in, H1, H2, D_out):         super(NetRelu, self).__init__()         self.linear1 = nn.Linear(D_in, H1)         self.linear2 = nn.Linear(H1, H2)         self.linear3 = nn.Linear(H2, D_out)          # Prediction     def forward(self, x):         x = torch.relu(self.linear1(x))           x = torch.relu(self.linear2(x))         x = self.linear3(x)         return x <p>Define a function to  train the model, in this case the function returns a Python dictionary to store the training loss and accuracy on the validation data</p> In\u00a0[\u00a0]: Copied! <pre># Train the model\n\ndef train(model, criterion, train_loader, validation_loader, optimizer, epochs=100):\n    i = 0\n    useful_stuff = {'training_loss': [], 'validation_accuracy': []}  \n    \n    for epoch in range(epochs):\n        for i, (x, y) in enumerate(train_loader):\n            optimizer.zero_grad()\n            z = model(x.view(-1, 28 * 28))\n            loss = criterion(z, y)\n            loss.backward()\n            optimizer.step()\n            useful_stuff['training_loss'].append(loss.data.item())\n        \n        correct = 0\n        for x, y in validation_loader:\n            z = model(x.view(-1, 28 * 28))\n            _, label = torch.max(z, 1)\n            correct += (label == y).sum().item()\n    \n        accuracy = 100 * (correct / len(validation_dataset))\n        useful_stuff['validation_accuracy'].append(accuracy)\n    \n    return useful_stuff\n</pre> # Train the model  def train(model, criterion, train_loader, validation_loader, optimizer, epochs=100):     i = 0     useful_stuff = {'training_loss': [], 'validation_accuracy': []}            for epoch in range(epochs):         for i, (x, y) in enumerate(train_loader):             optimizer.zero_grad()             z = model(x.view(-1, 28 * 28))             loss = criterion(z, y)             loss.backward()             optimizer.step()             useful_stuff['training_loss'].append(loss.data.item())                  correct = 0         for x, y in validation_loader:             z = model(x.view(-1, 28 * 28))             _, label = torch.max(z, 1)             correct += (label == y).sum().item()              accuracy = 100 * (correct / len(validation_dataset))         useful_stuff['validation_accuracy'].append(accuracy)          return useful_stuff <p>Load the training dataset by setting the parameters <code>train</code> to <code>True</code> and convert it to a tensor  by placing a transform object int the argument <code>transform</code></p> In\u00a0[\u00a0]: Copied! <pre># Create the training dataset\n\ntrain_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n</pre> # Create the training dataset  train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor()) <p>Load the testing dataset by setting the parameters <code>train</code> to <code>False</code> and convert it to a tensor  by placing a transform object int the argument <code>transform</code></p> In\u00a0[\u00a0]: Copied! <pre># Create the validating dataset\n\nvalidation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n</pre> # Create the validating dataset  validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor()) <p>Create the criterion function</p> In\u00a0[\u00a0]: Copied! <pre># Create the criterion function\n\ncriterion = nn.CrossEntropyLoss()\n</pre> # Create the criterion function  criterion = nn.CrossEntropyLoss() <p>Create the training-data loader and the validation-data loader object</p> In\u00a0[\u00a0]: Copied! <pre># Create the training data loader and validation data loader object\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)\n</pre> # Create the training data loader and validation data loader object  train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True) validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False) <p>Create  the model with 100 hidden layers</p> In\u00a0[\u00a0]: Copied! <pre># Set the parameters for create the model\n\ninput_dim = 28 * 28\nhidden_dim1 = 50\nhidden_dim2 = 50\noutput_dim = 10\n</pre> # Set the parameters for create the model  input_dim = 28 * 28 hidden_dim1 = 50 hidden_dim2 = 50 output_dim = 10 <p>The epoch number in the video is 35. You can try 10 for now. If you try 35, it may take a long time.</p> In\u00a0[\u00a0]: Copied! <pre># Set the number of iterations\n\ncust_epochs = 10\n</pre> # Set the number of iterations  cust_epochs = 10 <p>Train the network using the Sigmoid activation function</p> In\u00a0[\u00a0]: Copied! <pre># Train the model with sigmoid function\n\nlearning_rate = 0.01\nmodel = Net(input_dim, hidden_dim1, hidden_dim2, output_dim)\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ntraining_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=cust_epochs)\n</pre> # Train the model with sigmoid function  learning_rate = 0.01 model = Net(input_dim, hidden_dim1, hidden_dim2, output_dim) optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) training_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=cust_epochs) <p>Train the network using the Tanh activation function</p> In\u00a0[\u00a0]: Copied! <pre># Train the model with tanh function\n\nlearning_rate = 0.01\nmodel_Tanh = NetTanh(input_dim, hidden_dim1, hidden_dim2, output_dim)\noptimizer = torch.optim.SGD(model_Tanh.parameters(), lr=learning_rate)\ntraining_results_tanch = train(model_Tanh, criterion, train_loader, validation_loader, optimizer, epochs=cust_epochs)\n</pre> # Train the model with tanh function  learning_rate = 0.01 model_Tanh = NetTanh(input_dim, hidden_dim1, hidden_dim2, output_dim) optimizer = torch.optim.SGD(model_Tanh.parameters(), lr=learning_rate) training_results_tanch = train(model_Tanh, criterion, train_loader, validation_loader, optimizer, epochs=cust_epochs) <p>Train the network using the Relu activation function</p> In\u00a0[\u00a0]: Copied! <pre># Train the model with relu function\n\nlearning_rate = 0.01\nmodelRelu = NetRelu(input_dim, hidden_dim1, hidden_dim2, output_dim)\noptimizer = torch.optim.SGD(modelRelu.parameters(), lr=learning_rate)\ntraining_results_relu = train(modelRelu, criterion, train_loader, validation_loader, optimizer, epochs=cust_epochs)\n</pre> # Train the model with relu function  learning_rate = 0.01 modelRelu = NetRelu(input_dim, hidden_dim1, hidden_dim2, output_dim) optimizer = torch.optim.SGD(modelRelu.parameters(), lr=learning_rate) training_results_relu = train(modelRelu, criterion, train_loader, validation_loader, optimizer, epochs=cust_epochs) <p>Compare the training loss for each activation</p> In\u00a0[\u00a0]: Copied! <pre># Compare the training loss\n\nplt.plot(training_results_tanch['training_loss'], label='tanh')\nplt.plot(training_results['training_loss'], label='sigmoid')\nplt.plot(training_results_relu['training_loss'], label='relu')\nplt.ylabel('loss')\nplt.title('training loss iterations')\nplt.legend()\n</pre> # Compare the training loss  plt.plot(training_results_tanch['training_loss'], label='tanh') plt.plot(training_results['training_loss'], label='sigmoid') plt.plot(training_results_relu['training_loss'], label='relu') plt.ylabel('loss') plt.title('training loss iterations') plt.legend() <p>Compare the validation loss for each model</p> In\u00a0[\u00a0]: Copied! <pre># Compare the validation loss\n\nplt.plot(training_results_tanch['validation_accuracy'], label = 'tanh')\nplt.plot(training_results['validation_accuracy'], label = 'sigmoid')\nplt.plot(training_results_relu['validation_accuracy'], label = 'relu') \nplt.ylabel('validation accuracy')\nplt.xlabel('Iteration')   \nplt.legend()\n</pre> # Compare the validation loss  plt.plot(training_results_tanch['validation_accuracy'], label = 'tanh') plt.plot(training_results['validation_accuracy'], label = 'sigmoid') plt.plot(training_results_relu['validation_accuracy'], label = 'relu')  plt.ylabel('validation accuracy') plt.xlabel('Iteration')    plt.legend()"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.1mist2layer_v2/#author-juma-shafara-date-2024-08-08-title-multiple-linear-regression-keywords-training-two-parameter-mini-batch-gradient-decent-training-two-parameter-mini-batch-gradient-decent-description-in-this-lab-you-will-review-how-to-make-a-prediction-in-several-different-ways-by-using-pytorch","title":"author: Juma Shafara date: \"2024-08-08\" title: Multiple Linear Regression keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will review how to make a prediction in several different ways by using PyTorch.\u00b6","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.1mist2layer_v2/#Model","title":"Neural Network Module and Training Function","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.1mist2layer_v2/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.1mist2layer_v2/#Train","title":"Define Neural Network, Criterion function, Optimizer and Train the Model","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.1mist2layer_v2/#Test","title":"Test Sigmoid ,Tanh and Relu","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.1mist2layer_v2/#Result","title":"Analyze Results","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.2mulitclassspiralrulu_v2/","title":"Multiclass Spiral","text":"Deeper Neural Networks with nn.ModuleList() Objective for this Notebook  1. Create a Deeper Neural Network with <code>nn.ModuleList()</code>  2. Train and Validate the Model.  Table of Contents <p>In this lab, you will create a Deeper Neural Network with <code>nn.ModuleList()</code></p> <ul> <li>Neural Network Module and Function for Training</li> <li>Train and Validate the Model</li> </ul> <p>Estimated Time Needed: 25 min</p> Preparation <p>We'll need the following libraries</p> In\u00a0[1]: Copied! <pre># Import the libraries we need for this lab\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom matplotlib.colors import ListedColormap\nfrom torch.utils.data import Dataset, DataLoader\n\ntorch.manual_seed(1)\n</pre> # Import the libraries we need for this lab  import matplotlib.pyplot as plt  import numpy as np import torch import torch.nn as nn import torch.nn.functional as F from matplotlib.colors import ListedColormap from torch.utils.data import Dataset, DataLoader  torch.manual_seed(1) Out[1]: <pre>&lt;torch._C.Generator at 0x11c408610&gt;</pre> <p>Function used to plot:</p> In\u00a0[2]: Copied! <pre># Define the function to plot the diagram\n\ndef plot_decision_regions_3class(model, data_set):\n    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#00AAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])\n    X = data_set.x.numpy()\n    y = data_set.y.numpy()\n    h = .02\n    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1 \n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1 \n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    XX = torch.Tensor(np.c_[xx.ravel(), yy.ravel()])\n    _, yhat = torch.max(model(XX), 1)\n    yhat = yhat.numpy().reshape(xx.shape)\n    plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)\n    plt.plot(X[y[:] == 0, 0], X[y[:] == 0, 1], 'ro', label = 'y=0')\n    plt.plot(X[y[:] == 1, 0], X[y[:] == 1, 1], 'go', label = 'y=1')\n    plt.plot(X[y[:] == 2, 0], X[y[:] == 2, 1], 'o', label = 'y=2')\n    plt.title(\"decision region\")\n    plt.legend()\n</pre> # Define the function to plot the diagram  def plot_decision_regions_3class(model, data_set):     cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#00AAFF'])     cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])     X = data_set.x.numpy()     y = data_set.y.numpy()     h = .02     x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1      y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1      xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))     XX = torch.Tensor(np.c_[xx.ravel(), yy.ravel()])     _, yhat = torch.max(model(XX), 1)     yhat = yhat.numpy().reshape(xx.shape)     plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)     plt.plot(X[y[:] == 0, 0], X[y[:] == 0, 1], 'ro', label = 'y=0')     plt.plot(X[y[:] == 1, 0], X[y[:] == 1, 1], 'go', label = 'y=1')     plt.plot(X[y[:] == 2, 0], X[y[:] == 2, 1], 'o', label = 'y=2')     plt.title(\"decision region\")     plt.legend() <p>Create Dataset <code>Class</code></p> In\u00a0[3]: Copied! <pre># Create Data Class\n\nclass Data(Dataset):\n    \n    #  modified from: http://cs231n.github.io/neural-networks-case-study/\n    # Constructor\n    def __init__(self, K=3, N=500):\n        D = 2\n        X = np.zeros((N * K, D)) # data matrix (each row = single example)\n        y = np.zeros(N * K, dtype='uint8') # class labels\n        for j in range(K):\n          ix = range(N * j, N * (j + 1))\n          r = np.linspace(0.0, 1, N) # radius\n          t = np.linspace(j * 4, (j + 1) * 4, N) + np.random.randn(N) * 0.2 # theta\n          X[ix] = np.c_[r * np.sin(t), r*np.cos(t)]\n          y[ix] = j\n        self.y = torch.from_numpy(y).type(torch.LongTensor)\n        self.x = torch.from_numpy(X).type(torch.FloatTensor)\n        self.len = y.shape[0]\n    \n    # Getter\n    def __getitem__(self, index):    \n        return self.x[index], self.y[index]\n    \n    # Get Length\n    def __len__(self):\n        return self.len\n    \n    # Plot the diagram\n    def plot_stuff(self):\n        plt.plot(self.x[self.y[:] == 0, 0].numpy(), self.x[self.y[:] == 0, 1].numpy(), 'o', label=\"y = 0\")\n        plt.plot(self.x[self.y[:] == 1, 0].numpy(), self.x[self.y[:] == 1, 1].numpy(), 'ro', label=\"y = 1\")\n        plt.plot(self.x[self.y[:] == 2, 0].numpy(), self.x[self.y[:] == 2, 1].numpy(), 'go', label=\"y = 2\")\n        plt.legend()\n</pre> # Create Data Class  class Data(Dataset):          #  modified from: http://cs231n.github.io/neural-networks-case-study/     # Constructor     def __init__(self, K=3, N=500):         D = 2         X = np.zeros((N * K, D)) # data matrix (each row = single example)         y = np.zeros(N * K, dtype='uint8') # class labels         for j in range(K):           ix = range(N * j, N * (j + 1))           r = np.linspace(0.0, 1, N) # radius           t = np.linspace(j * 4, (j + 1) * 4, N) + np.random.randn(N) * 0.2 # theta           X[ix] = np.c_[r * np.sin(t), r*np.cos(t)]           y[ix] = j         self.y = torch.from_numpy(y).type(torch.LongTensor)         self.x = torch.from_numpy(X).type(torch.FloatTensor)         self.len = y.shape[0]          # Getter     def __getitem__(self, index):             return self.x[index], self.y[index]          # Get Length     def __len__(self):         return self.len          # Plot the diagram     def plot_stuff(self):         plt.plot(self.x[self.y[:] == 0, 0].numpy(), self.x[self.y[:] == 0, 1].numpy(), 'o', label=\"y = 0\")         plt.plot(self.x[self.y[:] == 1, 0].numpy(), self.x[self.y[:] == 1, 1].numpy(), 'ro', label=\"y = 1\")         plt.plot(self.x[self.y[:] == 2, 0].numpy(), self.x[self.y[:] == 2, 1].numpy(), 'go', label=\"y = 2\")         plt.legend() <p>Neural Network Module using <code>ModuleList()</code></p> In\u00a0[4]: Copied! <pre># Create Net model class\n\nclass Net(nn.Module):\n    \n    # Constructor\n    def __init__(self, Layers):\n        super(Net, self).__init__()\n        self.hidden = nn.ModuleList()\n        for input_size, output_size in zip(Layers, Layers[1:]):\n            self.hidden.append(nn.Linear(input_size, output_size))\n    \n    # Prediction\n    def forward(self, activation):\n        L = len(self.hidden)\n        for (l, linear_transform) in zip(range(L), self.hidden):\n            if l &lt; L - 1:\n                activation = F.relu(linear_transform(activation))\n            else:\n                activation = linear_transform(activation)\n        return activation\n</pre> # Create Net model class  class Net(nn.Module):          # Constructor     def __init__(self, Layers):         super(Net, self).__init__()         self.hidden = nn.ModuleList()         for input_size, output_size in zip(Layers, Layers[1:]):             self.hidden.append(nn.Linear(input_size, output_size))          # Prediction     def forward(self, activation):         L = len(self.hidden)         for (l, linear_transform) in zip(range(L), self.hidden):             if l &lt; L - 1:                 activation = F.relu(linear_transform(activation))             else:                 activation = linear_transform(activation)         return activation <p>A function used to train.</p> In\u00a0[5]: Copied! <pre># Define the function for training the model\n\ndef train(data_set, model, criterion, train_loader, optimizer, epochs=100):\n    LOSS = []\n    ACC = []\n    for epoch in range(epochs):\n        for x, y in train_loader:\n            optimizer.zero_grad()\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            LOSS.append(loss.item())\n        ACC.append(accuracy(model, data_set))\n    \n    fig, ax1 = plt.subplots()\n    color = 'tab:red'\n    ax1.plot(LOSS, color = color)\n    ax1.set_xlabel('Iteration', color = color)\n    ax1.set_ylabel('total loss', color = color)\n    ax1.tick_params(axis = 'y', color = color)\n    \n    ax2 = ax1.twinx()  \n    color = 'tab:blue'\n    ax2.set_ylabel('accuracy', color = color)  # we already handled the x-label with ax1\n    ax2.plot(ACC, color = color)\n    ax2.tick_params(axis = 'y', color = color)\n    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n    \n    plt.show()\n    return LOSS\n</pre> # Define the function for training the model  def train(data_set, model, criterion, train_loader, optimizer, epochs=100):     LOSS = []     ACC = []     for epoch in range(epochs):         for x, y in train_loader:             optimizer.zero_grad()             yhat = model(x)             loss = criterion(yhat, y)             optimizer.zero_grad()             loss.backward()             optimizer.step()             LOSS.append(loss.item())         ACC.append(accuracy(model, data_set))          fig, ax1 = plt.subplots()     color = 'tab:red'     ax1.plot(LOSS, color = color)     ax1.set_xlabel('Iteration', color = color)     ax1.set_ylabel('total loss', color = color)     ax1.tick_params(axis = 'y', color = color)          ax2 = ax1.twinx()       color = 'tab:blue'     ax2.set_ylabel('accuracy', color = color)  # we already handled the x-label with ax1     ax2.plot(ACC, color = color)     ax2.tick_params(axis = 'y', color = color)     fig.tight_layout()  # otherwise the right y-label is slightly clipped          plt.show()     return LOSS <p>A function used to calculate accuracy</p> In\u00a0[6]: Copied! <pre># The function to calculate the accuracy\n\ndef accuracy(model, data_set):\n    _, yhat = torch.max(model(data_set.x), 1)\n    return (yhat == data_set.y).numpy().mean()\n</pre> # The function to calculate the accuracy  def accuracy(model, data_set):     _, yhat = torch.max(model(data_set.x), 1)     return (yhat == data_set.y).numpy().mean() <p>Crate a dataset object:</p> In\u00a0[7]: Copied! <pre># Create a Dataset object\n\ndata_set = Data()\ndata_set.plot_stuff()\ndata_set.y = data_set.y.view(-1)\n</pre> # Create a Dataset object  data_set = Data() data_set.plot_stuff() data_set.y = data_set.y.view(-1) <p>Create a  network to classify three classes with 1 hidden layer with 50 neurons</p> In\u00a0[8]: Copied! <pre># Train the model with 1 hidden layer with 50 neurons\n\nLayers = [2, 50, 3]\nmodel = Net(Layers)\nlearning_rate = 0.10\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ntrain_loader = DataLoader(dataset=data_set, batch_size=20)\ncriterion = nn.CrossEntropyLoss()\nLOSS = train(data_set, model, criterion, train_loader, optimizer, epochs=100)\n\nplot_decision_regions_3class(model, data_set)\n</pre> # Train the model with 1 hidden layer with 50 neurons  Layers = [2, 50, 3] model = Net(Layers) learning_rate = 0.10 optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) train_loader = DataLoader(dataset=data_set, batch_size=20) criterion = nn.CrossEntropyLoss() LOSS = train(data_set, model, criterion, train_loader, optimizer, epochs=100)  plot_decision_regions_3class(model, data_set) <p>Create a  network to classify three classes with 2 hidden layers with 20 neurons in total</p> In\u00a0[15]: Copied! <pre>Net([3,3,4,3]).parameters\n</pre> Net([3,3,4,3]).parameters Out[15]: <pre>&lt;bound method Module.parameters of Net(\n  (hidden): ModuleList(\n    (0): Linear(in_features=3, out_features=3, bias=True)\n    (1): Linear(in_features=3, out_features=4, bias=True)\n    (2): Linear(in_features=4, out_features=3, bias=True)\n  )\n)&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Train the model with 2 hidden layers with 20 neurons\n\nLayers = [2, 10, 10, 3]\nmodel = Net(Layers)\nlearning_rate = 0.01\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ntrain_loader = DataLoader(dataset=data_set, batch_size=20)\ncriterion = nn.CrossEntropyLoss()\nLOSS = train(data_set, model, criterion, train_loader, optimizer, epochs=1000)\n\nplot_decision_regions_3class(model, data_set)\n</pre> # Train the model with 2 hidden layers with 20 neurons  Layers = [2, 10, 10, 3] model = Net(Layers) learning_rate = 0.01 optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) train_loader = DataLoader(dataset=data_set, batch_size=20) criterion = nn.CrossEntropyLoss() LOSS = train(data_set, model, criterion, train_loader, optimizer, epochs=1000)  plot_decision_regions_3class(model, data_set) Practice <p>Create a network with three hidden layers each with ten neurons, then train the network using the same process as above</p> In\u00a0[\u00a0]: Copied! <pre># Practice: Create a network with three hidden layers each with ten neurons.\n\n# Type your code here\n</pre> # Practice: Create a network with three hidden layers each with ten neurons.  # Type your code here <p>Double-click here for the solution.</p> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.2mulitclassspiralrulu_v2/#Model","title":"Neural Network Module and Function for Training","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.1.2mulitclassspiralrulu_v2/#Train","title":"Train and Validate the Model","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.1dropoutPredictin_v2/","title":"Dropout Prediction","text":"Using Dropout for Classification  Objective for this Notebook  1. Create the Model and Cost Function the PyTorch way.  2. Batch Gradient Descent  Table of Contents <p>In this lab, you will see how adding dropout to your model will decrease overfitting.</p> <ul> <li>Make Some Data</li> <li>Create the Model and Cost Function the PyTorch way</li> <li>Batch Gradient Descent</li> </ul> <p>Estimated Time Needed: 20 min</p> Preparation <p>We'll need the following libraries</p> In\u00a0[\u00a0]: Copied! <pre># Import the libraries we need for this lab\n\nimport torch\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom matplotlib.colors import ListedColormap\nfrom torch.utils.data import Dataset, DataLoader\n</pre> # Import the libraries we need for this lab  import torch import matplotlib.pyplot as plt import torch.nn as nn import torch.nn.functional as F import numpy as np from matplotlib.colors import ListedColormap from torch.utils.data import Dataset, DataLoader <p>Use this function only for plotting:</p> In\u00a0[\u00a0]: Copied! <pre># The function for plotting the diagram\n\ndef plot_decision_regions_3class(data_set, model=None):\n    cmap_light = ListedColormap([ '#0000FF','#FF0000'])\n    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])\n    X = data_set.x.numpy()\n    y = data_set.y.numpy()\n    h = .02\n    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1 \n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1 \n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    newdata = np.c_[xx.ravel(), yy.ravel()]\n    \n    Z = data_set.multi_dim_poly(newdata).flatten()\n    f = np.zeros(Z.shape)\n    f[Z &gt; 0] = 1\n    f = f.reshape(xx.shape)\n    if model != None:\n        model.eval()\n        XX = torch.Tensor(newdata)\n        _, yhat = torch.max(model(XX), 1)\n        yhat = yhat.numpy().reshape(xx.shape)\n        plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)\n        plt.contour(xx, yy, f, cmap=plt.cm.Paired)\n    else:\n        plt.contour(xx, yy, f, cmap=plt.cm.Paired)\n        plt.pcolormesh(xx, yy, f, cmap=cmap_light) \n\n    plt.title(\"decision region vs True decision boundary\")\n</pre> # The function for plotting the diagram  def plot_decision_regions_3class(data_set, model=None):     cmap_light = ListedColormap([ '#0000FF','#FF0000'])     cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])     X = data_set.x.numpy()     y = data_set.y.numpy()     h = .02     x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1      y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1      xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))     newdata = np.c_[xx.ravel(), yy.ravel()]          Z = data_set.multi_dim_poly(newdata).flatten()     f = np.zeros(Z.shape)     f[Z &gt; 0] = 1     f = f.reshape(xx.shape)     if model != None:         model.eval()         XX = torch.Tensor(newdata)         _, yhat = torch.max(model(XX), 1)         yhat = yhat.numpy().reshape(xx.shape)         plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)         plt.contour(xx, yy, f, cmap=plt.cm.Paired)     else:         plt.contour(xx, yy, f, cmap=plt.cm.Paired)         plt.pcolormesh(xx, yy, f, cmap=cmap_light)       plt.title(\"decision region vs True decision boundary\") <p>Use this function to calculate accuracy:</p> In\u00a0[\u00a0]: Copied! <pre># The function for calculating accuracy\n\ndef accuracy(model, data_set):\n    _, yhat = torch.max(model(data_set.x), 1)\n    return (yhat == data_set.y).numpy().mean()\n</pre> # The function for calculating accuracy  def accuracy(model, data_set):     _, yhat = torch.max(model(data_set.x), 1)     return (yhat == data_set.y).numpy().mean() <p>Create a nonlinearly separable dataset:</p> In\u00a0[\u00a0]: Copied! <pre># Create data class for creating dataset object\n\nclass Data(Dataset):\n    \n    # Constructor\n    def __init__(self, N_SAMPLES=1000, noise_std=0.15, train=True):\n        a = np.matrix([-1, 1, 2, 1, 1, -3, 1]).T\n        self.x = np.matrix(np.random.rand(N_SAMPLES, 2))\n        self.f = np.array(a[0] + (self.x) * a[1:3] + np.multiply(self.x[:, 0], self.x[:, 1]) * a[4] + np.multiply(self.x, self.x) * a[5:7]).flatten()\n        self.a = a\n       \n        self.y = np.zeros(N_SAMPLES)\n        self.y[self.f &gt; 0] = 1\n        self.y = torch.from_numpy(self.y).type(torch.LongTensor)\n        self.x = torch.from_numpy(self.x).type(torch.FloatTensor)\n        self.x = self.x + noise_std * torch.randn(self.x.size())\n        self.f = torch.from_numpy(self.f)\n        self.a = a\n        if train == True:\n            torch.manual_seed(1)\n            self.x = self.x + noise_std * torch.randn(self.x.size())\n            torch.manual_seed(0)\n        \n    # Getter        \n    def __getitem__(self, index):    \n        return self.x[index], self.y[index]\n    \n    # Get Length\n    def __len__(self):\n        return self.len\n    \n    # Plot the diagram\n    def plot(self):\n        X = data_set.x.numpy()\n        y = data_set.y.numpy()\n        h = .02\n        x_min, x_max = X[:, 0].min(), X[:, 0].max()\n        y_min, y_max = X[:, 1].min(), X[:, 1].max() \n        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n        Z = data_set.multi_dim_poly(np.c_[xx.ravel(), yy.ravel()]).flatten()\n        f = np.zeros(Z.shape)\n        f[Z &gt; 0] = 1\n        f = f.reshape(xx.shape)\n        \n        plt.title('True decision boundary  and sample points with noise ')\n        plt.plot(self.x[self.y == 0, 0].numpy(), self.x[self.y == 0,1].numpy(), 'bo', label='y=0') \n        plt.plot(self.x[self.y == 1, 0].numpy(), self.x[self.y == 1,1].numpy(), 'ro', label='y=1')\n        plt.contour(xx, yy, f,cmap=plt.cm.Paired)\n        plt.xlim(0,1)\n        plt.ylim(0,1)\n        plt.legend()\n    \n    # Make a multidimension ploynomial function\n    def multi_dim_poly(self, x):\n        x = np.matrix(x)\n        out = np.array(self.a[0] + (x) * self.a[1:3] + np.multiply(x[:, 0], x[:, 1]) * self.a[4] + np.multiply(x, x) * self.a[5:7])\n        out = np.array(out)\n        return out\n</pre> # Create data class for creating dataset object  class Data(Dataset):          # Constructor     def __init__(self, N_SAMPLES=1000, noise_std=0.15, train=True):         a = np.matrix([-1, 1, 2, 1, 1, -3, 1]).T         self.x = np.matrix(np.random.rand(N_SAMPLES, 2))         self.f = np.array(a[0] + (self.x) * a[1:3] + np.multiply(self.x[:, 0], self.x[:, 1]) * a[4] + np.multiply(self.x, self.x) * a[5:7]).flatten()         self.a = a                 self.y = np.zeros(N_SAMPLES)         self.y[self.f &gt; 0] = 1         self.y = torch.from_numpy(self.y).type(torch.LongTensor)         self.x = torch.from_numpy(self.x).type(torch.FloatTensor)         self.x = self.x + noise_std * torch.randn(self.x.size())         self.f = torch.from_numpy(self.f)         self.a = a         if train == True:             torch.manual_seed(1)             self.x = self.x + noise_std * torch.randn(self.x.size())             torch.manual_seed(0)              # Getter             def __getitem__(self, index):             return self.x[index], self.y[index]          # Get Length     def __len__(self):         return self.len          # Plot the diagram     def plot(self):         X = data_set.x.numpy()         y = data_set.y.numpy()         h = .02         x_min, x_max = X[:, 0].min(), X[:, 0].max()         y_min, y_max = X[:, 1].min(), X[:, 1].max()          xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))         Z = data_set.multi_dim_poly(np.c_[xx.ravel(), yy.ravel()]).flatten()         f = np.zeros(Z.shape)         f[Z &gt; 0] = 1         f = f.reshape(xx.shape)                  plt.title('True decision boundary  and sample points with noise ')         plt.plot(self.x[self.y == 0, 0].numpy(), self.x[self.y == 0,1].numpy(), 'bo', label='y=0')          plt.plot(self.x[self.y == 1, 0].numpy(), self.x[self.y == 1,1].numpy(), 'ro', label='y=1')         plt.contour(xx, yy, f,cmap=plt.cm.Paired)         plt.xlim(0,1)         plt.ylim(0,1)         plt.legend()          # Make a multidimension ploynomial function     def multi_dim_poly(self, x):         x = np.matrix(x)         out = np.array(self.a[0] + (x) * self.a[1:3] + np.multiply(x[:, 0], x[:, 1]) * self.a[4] + np.multiply(x, x) * self.a[5:7])         out = np.array(out)         return out <p>Create a dataset object:</p> In\u00a0[\u00a0]: Copied! <pre># Create a dataset object\n\ndata_set = Data(noise_std=0.2)\ndata_set.plot()\n</pre> # Create a dataset object  data_set = Data(noise_std=0.2) data_set.plot() <p>Validation data:</p> In\u00a0[\u00a0]: Copied! <pre># Get some validation data\n\ntorch.manual_seed(0) \nvalidation_set = Data(train=False)\n</pre> # Get some validation data  torch.manual_seed(0)  validation_set = Data(train=False) <p>Create a custom module with three layers. <code>in_size</code> is the size of the input features, <code>n_hidden</code> is the size of the layers, and <code>out_size</code> is the size. <code>p</code> is the dropout probability. The default is 0, that is, no dropout.</p> In\u00a0[\u00a0]: Copied! <pre># Create Net Class\n\nclass Net(nn.Module):\n    \n    # Constructor\n    def __init__(self, in_size, n_hidden, out_size, p=0):\n        super(Net, self).__init__()\n        self.drop = nn.Dropout(p=p)\n        self.linear1 = nn.Linear(in_size, n_hidden)\n        self.linear2 = nn.Linear(n_hidden, n_hidden)\n        self.linear3 = nn.Linear(n_hidden, out_size)\n    \n    # Prediction function\n    def forward(self, x):\n        x = F.relu(self.drop(self.linear1(x)))\n        x = F.relu(self.drop(self.linear2(x)))\n        x = self.linear3(x)\n        return x\n</pre> # Create Net Class  class Net(nn.Module):          # Constructor     def __init__(self, in_size, n_hidden, out_size, p=0):         super(Net, self).__init__()         self.drop = nn.Dropout(p=p)         self.linear1 = nn.Linear(in_size, n_hidden)         self.linear2 = nn.Linear(n_hidden, n_hidden)         self.linear3 = nn.Linear(n_hidden, out_size)          # Prediction function     def forward(self, x):         x = F.relu(self.drop(self.linear1(x)))         x = F.relu(self.drop(self.linear2(x)))         x = self.linear3(x)         return x <p>Create two model objects: <code>model</code> had no dropout and <code>model_drop</code> has a dropout probability of 0.5:</p> In\u00a0[\u00a0]: Copied! <pre># Create two model objects: model without dropout and model with dropout\n\nmodel = Net(2, 300, 2)\nmodel_drop = Net(2, 300, 2, p=0.5)\n</pre> # Create two model objects: model without dropout and model with dropout  model = Net(2, 300, 2) model_drop = Net(2, 300, 2, p=0.5) <p>Set the model using dropout to training mode; this is the default mode, but it's  good practice to write this in your code :</p> In\u00a0[\u00a0]: Copied! <pre># Set the model to training mode\n\nmodel_drop.train()\n</pre> # Set the model to training mode  model_drop.train() <p>Train the model by using the Adam optimizer. See the unit on other optimizers. Use the Cross Entropy Loss:</p> In\u00a0[\u00a0]: Copied! <pre># Set optimizer functions and criterion functions\n\noptimizer_ofit = torch.optim.Adam(model.parameters(), lr=0.01)\noptimizer_drop = torch.optim.Adam(model_drop.parameters(), lr=0.01)\ncriterion = torch.nn.CrossEntropyLoss()\n</pre> # Set optimizer functions and criterion functions  optimizer_ofit = torch.optim.Adam(model.parameters(), lr=0.01) optimizer_drop = torch.optim.Adam(model_drop.parameters(), lr=0.01) criterion = torch.nn.CrossEntropyLoss() <p>Initialize a dictionary that stores the training and validation loss for each model:</p> In\u00a0[\u00a0]: Copied! <pre># Initialize the LOSS dictionary to store the loss\n\nLOSS = {}\nLOSS['training data no dropout'] = []\nLOSS['validation data no dropout'] = []\nLOSS['training data dropout'] = []\nLOSS['validation data dropout'] = []\n</pre> # Initialize the LOSS dictionary to store the loss  LOSS = {} LOSS['training data no dropout'] = [] LOSS['validation data no dropout'] = [] LOSS['training data dropout'] = [] LOSS['validation data dropout'] = [] <p>Run 500 iterations of batch gradient gradient descent:</p> In\u00a0[\u00a0]: Copied! <pre># Train the model\n\nepochs = 500\n\ndef train_model(epochs):\n    \n    for epoch in range(epochs):\n        #all the samples are used for training \n        yhat = model(data_set.x)\n        yhat_drop = model_drop(data_set.x)\n        loss = criterion(yhat, data_set.y)\n        loss_drop = criterion(yhat_drop, data_set.y)\n\n        #store the loss for both the training and validation data for both models \n        LOSS['training data no dropout'].append(loss.item())\n        LOSS['validation data no dropout'].append(criterion(model(validation_set.x), validation_set.y).item())\n        LOSS['training data dropout'].append(loss_drop.item())\n        model_drop.eval()\n        LOSS['validation data dropout'].append(criterion(model_drop(validation_set.x), validation_set.y).item())\n        model_drop.train()\n\n        optimizer_ofit.zero_grad()\n        optimizer_drop.zero_grad()\n        loss.backward()\n        loss_drop.backward()\n        optimizer_ofit.step()\n        optimizer_drop.step()\n        \ntrain_model(epochs)\n</pre> # Train the model  epochs = 500  def train_model(epochs):          for epoch in range(epochs):         #all the samples are used for training          yhat = model(data_set.x)         yhat_drop = model_drop(data_set.x)         loss = criterion(yhat, data_set.y)         loss_drop = criterion(yhat_drop, data_set.y)          #store the loss for both the training and validation data for both models          LOSS['training data no dropout'].append(loss.item())         LOSS['validation data no dropout'].append(criterion(model(validation_set.x), validation_set.y).item())         LOSS['training data dropout'].append(loss_drop.item())         model_drop.eval()         LOSS['validation data dropout'].append(criterion(model_drop(validation_set.x), validation_set.y).item())         model_drop.train()          optimizer_ofit.zero_grad()         optimizer_drop.zero_grad()         loss.backward()         loss_drop.backward()         optimizer_ofit.step()         optimizer_drop.step()          train_model(epochs) <p>Set the model with dropout to evaluation mode:</p> In\u00a0[\u00a0]: Copied! <pre># Set the model to evaluation model\n\nmodel_drop.eval()\n</pre> # Set the model to evaluation model  model_drop.eval() <p>Test the model without dropout on the validation data:</p> In\u00a0[\u00a0]: Copied! <pre># Print out the accuracy of the model without dropout\n\nprint(\"The accuracy of the model without dropout: \", accuracy(model, validation_set))\n</pre> # Print out the accuracy of the model without dropout  print(\"The accuracy of the model without dropout: \", accuracy(model, validation_set)) <p>Test the model with dropout on the validation data:</p> In\u00a0[\u00a0]: Copied! <pre># Print out the accuracy of the model with dropout\n\nprint(\"The accuracy of the model with dropout: \", accuracy(model_drop, validation_set))\n</pre> # Print out the accuracy of the model with dropout  print(\"The accuracy of the model with dropout: \", accuracy(model_drop, validation_set)) <p>You see that the model with dropout performs better on the validation data.</p> True Function <p>Plot the decision boundary and the prediction of the networks in different colors.</p> In\u00a0[\u00a0]: Copied! <pre># Plot the decision boundary and the prediction\n\nplot_decision_regions_3class(data_set)\n</pre> # Plot the decision boundary and the prediction  plot_decision_regions_3class(data_set) <p>Model without Dropout:</p> In\u00a0[\u00a0]: Copied! <pre># The model without dropout\n\nplot_decision_regions_3class(data_set, model)\n</pre> # The model without dropout  plot_decision_regions_3class(data_set, model) <p>Model with Dropout:</p> In\u00a0[\u00a0]: Copied! <pre># The model with dropout\n\nplot_decision_regions_3class(data_set, model_drop)\n</pre> # The model with dropout  plot_decision_regions_3class(data_set, model_drop) <p>You can see that the model using dropout does better at tracking the function that generated the data.</p> <p>Plot out the loss for the training and validation data on both models, we use the log to make the difference more apparent</p> In\u00a0[\u00a0]: Copied! <pre># Plot the LOSS\n\nplt.figure(figsize=(6.1, 10))\ndef plot_LOSS():\n    for key, value in LOSS.items():\n        plt.plot(np.log(np.array(value)), label=key)\n        plt.legend()\n        plt.xlabel(\"iterations\")\n        plt.ylabel(\"Log of cost or total loss\")\n\nplot_LOSS()\n</pre> # Plot the LOSS  plt.figure(figsize=(6.1, 10)) def plot_LOSS():     for key, value in LOSS.items():         plt.plot(np.log(np.array(value)), label=key)         plt.legend()         plt.xlabel(\"iterations\")         plt.ylabel(\"Log of cost or total loss\")  plot_LOSS() <p>You see that the model without dropout performs better on the training data, but it performs worse on the validation data. This suggests overfitting.  However, the model using dropout performed better on the validation data, but worse on the training data.</p> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.1dropoutPredictin_v2/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.1dropoutPredictin_v2/#Model_Cost","title":"Create the Model, Optimizer, and Total Loss Function (Cost)","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.1dropoutPredictin_v2/#BGD","title":"Train the Model via Mini-Batch Gradient Descent","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.2dropoutRegression_v2/","title":"Dropout Regression","text":"Using Dropout in Regression Objective for this Notebook  1. Create the Model and Cost Function the PyTorch way.  2. Learn Batch Gradient Descent  Table of Contents <p>In this lab, you will see how adding dropout to your model will decrease overfitting.</p> <ul> <li>Make Some Data</li> <li>Create the Model and Cost Function the PyTorch way</li> <li>Batch Gradient Descent</li> </ul> <p>Estimated Time Needed: 20 min</p> Preparation <p>We'll need the following libraries</p> In\u00a0[\u00a0]: Copied! <pre># Import the libraries we need for the lab\n\nimport torch\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\n\ntorch.manual_seed(0) \n</pre> # Import the libraries we need for the lab  import torch import matplotlib.pyplot as plt import torch.nn as nn import torch.nn.functional as F import numpy as np from torch.utils.data import Dataset, DataLoader  torch.manual_seed(0)  <p>Create polynomial dataset class:</p> In\u00a0[\u00a0]: Copied! <pre># Create Data object\n\nclass Data(Dataset):\n    \n    # Constructor\n    def __init__(self, N_SAMPLES=40, noise_std=1, train=True):\n        self.x = torch.linspace(-1, 1, N_SAMPLES).view(-1, 1)\n        self.f = self.x ** 2\n        if train != True:\n            torch.manual_seed(1)\n            self.y = self.f + noise_std * torch.randn(self.f.size())\n            self.y = self.y.view(-1, 1)\n            torch.manual_seed(0)\n        else:\n            self.y = self.f + noise_std * torch.randn(self.f.size())\n            self.y = self.y.view(-1, 1)\n            \n    # Getter\n    def __getitem__(self, index):    \n        return self.x[index], self.y[index]\n    \n    # Get Length\n    def __len__(self):\n        return self.len\n    \n    # Plot the data\n    def plot(self):\n        plt.figure(figsize = (6.1, 10))\n        plt.scatter(self.x.numpy(), self.y.numpy(), label=\"Samples\")\n        plt.plot(self.x.numpy(), self.f.numpy() ,label=\"True Function\", color='orange')\n        plt.xlabel(\"x\")\n        plt.ylabel(\"y\")\n        plt.xlim((-1, 1))\n        plt.ylim((-2, 2.5))\n        plt.legend(loc=\"best\")\n        plt.show()\n</pre> # Create Data object  class Data(Dataset):          # Constructor     def __init__(self, N_SAMPLES=40, noise_std=1, train=True):         self.x = torch.linspace(-1, 1, N_SAMPLES).view(-1, 1)         self.f = self.x ** 2         if train != True:             torch.manual_seed(1)             self.y = self.f + noise_std * torch.randn(self.f.size())             self.y = self.y.view(-1, 1)             torch.manual_seed(0)         else:             self.y = self.f + noise_std * torch.randn(self.f.size())             self.y = self.y.view(-1, 1)                  # Getter     def __getitem__(self, index):             return self.x[index], self.y[index]          # Get Length     def __len__(self):         return self.len          # Plot the data     def plot(self):         plt.figure(figsize = (6.1, 10))         plt.scatter(self.x.numpy(), self.y.numpy(), label=\"Samples\")         plt.plot(self.x.numpy(), self.f.numpy() ,label=\"True Function\", color='orange')         plt.xlabel(\"x\")         plt.ylabel(\"y\")         plt.xlim((-1, 1))         plt.ylim((-2, 2.5))         plt.legend(loc=\"best\")         plt.show() <p>Create a dataset object:</p> In\u00a0[\u00a0]: Copied! <pre># Create the dataset object and plot the dataset\n\ndata_set = Data()\ndata_set.plot()\n</pre> # Create the dataset object and plot the dataset  data_set = Data() data_set.plot() <p>Get some validation data:</p> In\u00a0[\u00a0]: Copied! <pre># Create validation dataset object\n\nvalidation_set = Data(train=False)\n</pre> # Create validation dataset object  validation_set = Data(train=False) <p>Create a custom module with three layers. <code>in_size</code> is the size of the input features, <code>n_hidden</code> is the size of the layers, and <code>out_size</code> is the size. <code>p</code> is dropout probability. The default is 0 which is no dropout.</p> In\u00a0[\u00a0]: Copied! <pre># Create the class for model\n\nclass Net(nn.Module):\n    \n    # Constructor\n    def __init__(self, in_size, n_hidden, out_size, p=0):\n        super(Net, self).__init__()\n        self.drop = nn.Dropout(p=p)\n        self.linear1 = nn.Linear(in_size, n_hidden)\n        self.linear2 = nn.Linear(n_hidden, n_hidden)\n        self.linear3 = nn.Linear(n_hidden, out_size)\n        \n    def forward(self, x):\n        x = F.relu(self.drop(self.linear1(x)))\n        x = F.relu(self.drop(self.linear2(x)))\n        x = self.linear3(x)\n        return x\n</pre> # Create the class for model  class Net(nn.Module):          # Constructor     def __init__(self, in_size, n_hidden, out_size, p=0):         super(Net, self).__init__()         self.drop = nn.Dropout(p=p)         self.linear1 = nn.Linear(in_size, n_hidden)         self.linear2 = nn.Linear(n_hidden, n_hidden)         self.linear3 = nn.Linear(n_hidden, out_size)              def forward(self, x):         x = F.relu(self.drop(self.linear1(x)))         x = F.relu(self.drop(self.linear2(x)))         x = self.linear3(x)         return x <p>Create two model objects: <code>model</code> had no dropout, and <code>model_drop  has a dropout probability of 0.5:</code></p> In\u00a0[\u00a0]: Copied! <pre># Create the model objects\n\nmodel = Net(1, 300, 1)\nmodel_drop = Net(1, 300, 1, p=0.5)\n</pre> # Create the model objects  model = Net(1, 300, 1) model_drop = Net(1, 300, 1, p=0.5) <p>Set the model using dropout to training mode; this is the default mode, but it's good practice.</p> In\u00a0[\u00a0]: Copied! <pre># Set the model to train mode\n\nmodel_drop.train()\n</pre> # Set the model to train mode  model_drop.train() <p>Train the model by using the Adam optimizer. See the unit on other optimizers. Use the mean square loss:</p> In\u00a0[\u00a0]: Copied! <pre># Set the optimizer and criterion function\n\noptimizer_ofit = torch.optim.Adam(model.parameters(), lr=0.01)\noptimizer_drop = torch.optim.Adam(model_drop.parameters(), lr=0.01)\ncriterion = torch.nn.MSELoss()\n</pre> # Set the optimizer and criterion function  optimizer_ofit = torch.optim.Adam(model.parameters(), lr=0.01) optimizer_drop = torch.optim.Adam(model_drop.parameters(), lr=0.01) criterion = torch.nn.MSELoss() <p>Initialize a dictionary that stores the training and validation loss for each model:</p> In\u00a0[\u00a0]: Copied! <pre># Initialize the dict to contain the loss results\n\nLOSS={}\nLOSS['training data no dropout']=[]\nLOSS['validation data no dropout']=[]\nLOSS['training data dropout']=[]\nLOSS['validation data dropout']=[]\n</pre> # Initialize the dict to contain the loss results  LOSS={} LOSS['training data no dropout']=[] LOSS['validation data no dropout']=[] LOSS['training data dropout']=[] LOSS['validation data dropout']=[] <p>Run 500 iterations of batch gradient descent:</p> In\u00a0[\u00a0]: Copied! <pre># Train the model\n\nepochs = 500\n\ndef train_model(epochs):\n    for epoch in range(epochs):\n        yhat = model(data_set.x)\n        yhat_drop = model_drop(data_set.x)\n        loss = criterion(yhat, data_set.y)\n        loss_drop = criterion(yhat_drop, data_set.y)\n\n        #store the loss for  both the training and validation  data for both models \n        LOSS['training data no dropout'].append(loss.item())\n        LOSS['validation data no dropout'].append(criterion(model(validation_set.x), validation_set.y).item())\n        LOSS['training data dropout'].append(loss_drop.item())\n        model_drop.eval()\n        LOSS['validation data dropout'].append(criterion(model_drop(validation_set.x), validation_set.y).item())\n        model_drop.train()\n\n        optimizer_ofit.zero_grad()\n        optimizer_drop.zero_grad()\n        loss.backward()\n        loss_drop.backward()\n        optimizer_ofit.step()\n        optimizer_drop.step()\n        \ntrain_model(epochs)\n</pre> # Train the model  epochs = 500  def train_model(epochs):     for epoch in range(epochs):         yhat = model(data_set.x)         yhat_drop = model_drop(data_set.x)         loss = criterion(yhat, data_set.y)         loss_drop = criterion(yhat_drop, data_set.y)          #store the loss for  both the training and validation  data for both models          LOSS['training data no dropout'].append(loss.item())         LOSS['validation data no dropout'].append(criterion(model(validation_set.x), validation_set.y).item())         LOSS['training data dropout'].append(loss_drop.item())         model_drop.eval()         LOSS['validation data dropout'].append(criterion(model_drop(validation_set.x), validation_set.y).item())         model_drop.train()          optimizer_ofit.zero_grad()         optimizer_drop.zero_grad()         loss.backward()         loss_drop.backward()         optimizer_ofit.step()         optimizer_drop.step()          train_model(epochs) <p>Set the model with dropout to evaluation mode:</p> In\u00a0[\u00a0]: Copied! <pre># Set the model with dropout to evaluation mode\n\nmodel_drop.eval()\n</pre> # Set the model with dropout to evaluation mode  model_drop.eval() <p>Make a prediction by using both models:</p> In\u00a0[\u00a0]: Copied! <pre># Make the prediction\n\nyhat = model(data_set.x)\nyhat_drop = model_drop(data_set.x)\n</pre> # Make the prediction  yhat = model(data_set.x) yhat_drop = model_drop(data_set.x) <p>Plot predictions of both models. Compare them to the training points and the true function:</p> In\u00a0[\u00a0]: Copied! <pre># Plot the predictions for both models\n\nplt.figure(figsize=(6.1, 10))\n\nplt.scatter(data_set.x.numpy(), data_set.y.numpy(), label=\"Samples\")\nplt.plot(data_set.x.numpy(), data_set.f.numpy(), label=\"True function\", color='orange')\nplt.plot(data_set.x.numpy(), yhat.detach().numpy(), label='no dropout', c='r')\nplt.plot(data_set.x.numpy(), yhat_drop.detach().numpy(), label=\"dropout\", c ='g')\n\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xlim((-1, 1))\nplt.ylim((-2, 2.5))\nplt.legend(loc = \"best\")\nplt.show()\n</pre> # Plot the predictions for both models  plt.figure(figsize=(6.1, 10))  plt.scatter(data_set.x.numpy(), data_set.y.numpy(), label=\"Samples\") plt.plot(data_set.x.numpy(), data_set.f.numpy(), label=\"True function\", color='orange') plt.plot(data_set.x.numpy(), yhat.detach().numpy(), label='no dropout', c='r') plt.plot(data_set.x.numpy(), yhat_drop.detach().numpy(), label=\"dropout\", c ='g')  plt.xlabel(\"x\") plt.ylabel(\"y\") plt.xlim((-1, 1)) plt.ylim((-2, 2.5)) plt.legend(loc = \"best\") plt.show() <p>You can see that the model using dropout does better at tracking the function that generated the data. We use the log to make the difference more apparent</p> <p>Plot out the loss for training and validation data on both models:</p> In\u00a0[\u00a0]: Copied! <pre># Plot the loss\n\nplt.figure(figsize=(6.1, 10))\nfor key, value in LOSS.items():\n    plt.plot(np.log(np.array(value)), label=key)\n    plt.legend()\n    plt.xlabel(\"iterations\")\n    plt.ylabel(\"Log of cost or total loss\")\n</pre> # Plot the loss  plt.figure(figsize=(6.1, 10)) for key, value in LOSS.items():     plt.plot(np.log(np.array(value)), label=key)     plt.legend()     plt.xlabel(\"iterations\")     plt.ylabel(\"Log of cost or total loss\") <p>You see that the model without dropout performs better on the training data, but it performs worse on the validation data. This suggests overfitting.  However, the model using dropout performs better on the validation data, but worse on the training data.</p> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.2dropoutRegression_v2/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.2dropoutRegression_v2/#Model_Cost","title":"Create the Model, Optimizer, and Total Loss Function (Cost)","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.2.2dropoutRegression_v2/#Train","title":"Train the Model via Mini-Batch Gradient Descent","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.1.initializationsame/","title":"Initialization","text":"Initialization with Same Weights  Objective for this Notebook  1. Learn hw to Define the Neural Network with Same Weights Initialization define  Criterion Function, Optimizer, and Train the Model  2.Define the Neural Network with defult Weights Initialization define  Criterion Function, Optimizer  3. Train the Model  Table of Contents <p>In this lab, we will see the problem of initializing the weights with the same value. We will see that even for a simple network, our model will not train properly. .</p> <ul> <li>Neural Network Module and Training Function</li> <li>Make Some Data</li> <li>Define the Neural Network with Same Weights Initialization define  Criterion Function, Optimizer, and Train the Model</li> <li>Define the Neural Network with defult Weights Initialization define  Criterion Function, Optimizer, and Train the Model</li> </ul> <p>Estimated Time Needed: 25 min</p> Preparation <p>We'll need the following libraries</p> In\u00a0[\u00a0]: Copied! <pre># Import the libraries we need for this lab\n\nimport torch \nimport torch.nn as nn\nfrom torch import sigmoid\nimport matplotlib.pylab as plt\nimport numpy as np\ntorch.manual_seed(0)\n</pre> # Import the libraries we need for this lab  import torch  import torch.nn as nn from torch import sigmoid import matplotlib.pylab as plt import numpy as np torch.manual_seed(0) <p>Used for plotting the model</p> In\u00a0[\u00a0]: Copied! <pre># The function for plotting the model\n\ndef PlotStuff(X, Y, model, epoch, leg=True):\n    \n    plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch)))\n    plt.plot(X.numpy(), Y.numpy(), 'r')\n    plt.xlabel('x')\n    if leg == True:\n        plt.legend()\n    else:\n        pass\n</pre> # The function for plotting the model  def PlotStuff(X, Y, model, epoch, leg=True):          plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch)))     plt.plot(X.numpy(), Y.numpy(), 'r')     plt.xlabel('x')     if leg == True:         plt.legend()     else:         pass <p>Define the activations and the output of the first linear layer as an attribute. Note that this is not good practice.</p> In\u00a0[\u00a0]: Copied! <pre># Define the class Net\n\nclass Net(nn.Module):\n    \n    # Constructor\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n        # hidden layer \n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n        # Define the first linear layer as an attribute, this is not good practice\n        self.a1 = None\n        self.l1 = None\n        self.l2=None\n    \n    # Prediction\n    def forward(self, x):\n        self.l1 = self.linear1(x)\n        self.a1 = sigmoid(self.l1)\n        self.l2=self.linear2(self.a1)\n        yhat = sigmoid(self.linear2(self.a1))\n        return yhat\n</pre> # Define the class Net  class Net(nn.Module):          # Constructor     def __init__(self, D_in, H, D_out):         super(Net, self).__init__()         # hidden layer          self.linear1 = nn.Linear(D_in, H)         self.linear2 = nn.Linear(H, D_out)         # Define the first linear layer as an attribute, this is not good practice         self.a1 = None         self.l1 = None         self.l2=None          # Prediction     def forward(self, x):         self.l1 = self.linear1(x)         self.a1 = sigmoid(self.l1)         self.l2=self.linear2(self.a1)         yhat = sigmoid(self.linear2(self.a1))         return yhat <p>Define the training function:</p> In\u00a0[\u00a0]: Copied! <pre># Define the training function\n\ndef train(Y, X, model, optimizer, criterion, epochs=1000):\n    cost = []\n    total=0\n    for epoch in range(epochs):\n        total=0\n        for y, x in zip(Y, X):\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            #cumulative loss \n            total+=loss.item() \n        cost.append(total)\n        if epoch % 300 == 0:    \n            PlotStuff(X, Y, model, epoch, leg=True)\n            plt.show()\n            model(X)\n            plt.scatter(model.a1.detach().numpy()[:, 0], model.a1.detach().numpy()[:, 1], c=Y.numpy().reshape(-1))\n            plt.title('activations')\n            plt.show()\n    return cost\n</pre> # Define the training function  def train(Y, X, model, optimizer, criterion, epochs=1000):     cost = []     total=0     for epoch in range(epochs):         total=0         for y, x in zip(Y, X):             yhat = model(x)             loss = criterion(yhat, y)             loss.backward()             optimizer.step()             optimizer.zero_grad()             #cumulative loss              total+=loss.item()          cost.append(total)         if epoch % 300 == 0:                 PlotStuff(X, Y, model, epoch, leg=True)             plt.show()             model(X)             plt.scatter(model.a1.detach().numpy()[:, 0], model.a1.detach().numpy()[:, 1], c=Y.numpy().reshape(-1))             plt.title('activations')             plt.show()     return cost In\u00a0[\u00a0]: Copied! <pre># Make some data\n\nX = torch.arange(-20, 20, 1).view(-1, 1).type(torch.FloatTensor)\nY = torch.zeros(X.shape[0])\nY[(X[:, 0] &gt; -4) &amp; (X[:, 0] &lt; 4)] = 1.0\n</pre> # Make some data  X = torch.arange(-20, 20, 1).view(-1, 1).type(torch.FloatTensor) Y = torch.zeros(X.shape[0]) Y[(X[:, 0] &gt; -4) &amp; (X[:, 0] &lt; 4)] = 1.0 <p>Create the Cross-Entropy loss function:</p> In\u00a0[\u00a0]: Copied! <pre># The loss function\n\ndef criterion_cross(outputs, labels):\n    out = -1 * torch.mean(labels * torch.log(outputs) + (1 - labels) * torch.log(1 - outputs))\n    return out\n</pre> # The loss function  def criterion_cross(outputs, labels):     out = -1 * torch.mean(labels * torch.log(outputs) + (1 - labels) * torch.log(1 - outputs))     return out <p>Define the Neural Network</p> In\u00a0[\u00a0]: Copied! <pre># Train the model\n# size of input \nD_in = 1\n# size of hidden layer \nH = 2\n# number of outputs \nD_out = 1\n# learning rate \nlearning_rate = 0.1\n# create the model \nmodel = Net(D_in, H, D_out)\n</pre> # Train the model # size of input  D_in = 1 # size of hidden layer  H = 2 # number of outputs  D_out = 1 # learning rate  learning_rate = 0.1 # create the model  model = Net(D_in, H, D_out)  <p>This is the PyTorch default installation</p> In\u00a0[\u00a0]: Copied! <pre>model.state_dict()\n</pre> model.state_dict() <p>Same Weights Initialization with all ones for weights and zeros for the bias.</p> In\u00a0[\u00a0]: Copied! <pre>model.state_dict()['linear1.weight'][0]=1.0\nmodel.state_dict()['linear1.weight'][1]=1.0\nmodel.state_dict()['linear1.bias'][0]=0.0\nmodel.state_dict()['linear1.bias'][1]=0.0\nmodel.state_dict()['linear2.weight'][0]=1.0\nmodel.state_dict()['linear2.bias'][0]=0.0\nmodel.state_dict()\n</pre> model.state_dict()['linear1.weight'][0]=1.0 model.state_dict()['linear1.weight'][1]=1.0 model.state_dict()['linear1.bias'][0]=0.0 model.state_dict()['linear1.bias'][1]=0.0 model.state_dict()['linear2.weight'][0]=1.0 model.state_dict()['linear2.bias'][0]=0.0 model.state_dict() <p>Optimizer, and Train the Model:</p> In\u00a0[\u00a0]: Copied! <pre>#optimizer \noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n#train the model usein\ncost_cross = train(Y, X, model, optimizer, criterion_cross, epochs=1000)\n#plot the loss\nplt.plot(cost_cross)\nplt.xlabel('epoch')\nplt.title('cross entropy loss')\n</pre> #optimizer  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) #train the model usein cost_cross = train(Y, X, model, optimizer, criterion_cross, epochs=1000) #plot the loss plt.plot(cost_cross) plt.xlabel('epoch') plt.title('cross entropy loss') <p>By examining the output of the  paramters all thought they have changed they are identical.</p> In\u00a0[\u00a0]: Copied! <pre>model.state_dict()\n</pre> model.state_dict() In\u00a0[\u00a0]: Copied! <pre>yhat=model(torch.tensor([[-2.0],[0.0],[2.0]]))\nyhat\n</pre> yhat=model(torch.tensor([[-2.0],[0.0],[2.0]])) yhat In\u00a0[\u00a0]: Copied! <pre># Train the model\n# size of input \nD_in = 1\n# size of hidden layer \nH = 2\n# number of outputs \nD_out = 1\n# learning rate \nlearning_rate = 0.1\n# create the model \nmodel = Net(D_in, H, D_out)\n</pre> # Train the model # size of input  D_in = 1 # size of hidden layer  H = 2 # number of outputs  D_out = 1 # learning rate  learning_rate = 0.1 # create the model  model = Net(D_in, H, D_out) <p>Repeat the previous steps above by using the MSE cost or total loss:</p> In\u00a0[\u00a0]: Copied! <pre>#optimizer \noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n#train the model usein\ncost_cross = train(Y, X, model, optimizer, criterion_cross, epochs=1000)\n#plot the loss\nplt.plot(cost_cross)\nplt.xlabel('epoch')\nplt.title('cross entropy loss')\n</pre> #optimizer  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) #train the model usein cost_cross = train(Y, X, model, optimizer, criterion_cross, epochs=1000) #plot the loss plt.plot(cost_cross) plt.xlabel('epoch') plt.title('cross entropy loss') <p>Double-click here for the solution.</p> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.1.initializationsame/#Model","title":"Neural Network Module and Training Function","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.1.initializationsame/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.1.initializationsame/#Train","title":"Define the Neural Network with Same Weights Initialization define, Criterion Function, Optimizer and Train the Model","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.1.initializationsame/#Train2","title":"Define the Neural Network, Criterion Function, Optimizer and Train the Model","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.2Xaviermist1layer_v2/","title":"Xavier Init","text":"Test Uniform, Default and Xavier Uniform Initialization on MNIST dataset with tanh activation Objective for this Notebook  1. Define Several Neural Network, Criterion function, Optimizer  2. Test Uniform, Default and Xavier Initialization  Table of Contents In this lab, you will test PyTroch Default Initialization, Xavier Initialization and Uniform Initialization on the MNIST dataset.   <ul> <li>Neural Network Module and Training Function</li> <li>Make Some Data</li> <li>Define Several Neural Network, Criterion function, Optimizer</li> <li>Test Uniform, Default and Xavier Initialization</li> <li>Analyze Results</li> </ul> <p>Estimated Time Needed: 25 min</p> Preparation <p>We'll need the following libraries:</p> In\u00a0[\u00a0]: Copied! <pre># Import the libraries we need to use in this lab\n\n# Using the following line code to install the torchvision library\n# !mamba install -y torchvision\n\nimport torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport matplotlib.pylab as plt\nimport numpy as np\n\ntorch.manual_seed(0)\n</pre> # Import the libraries we need to use in this lab  # Using the following line code to install the torchvision library # !mamba install -y torchvision  import torch  import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets import matplotlib.pylab as plt import numpy as np  torch.manual_seed(0) <p>Define the neural network module or class with Xavier Initialization</p> In\u00a0[\u00a0]: Copied! <pre># Define the neural network with Xavier initialization\n\nclass Net_Xavier(nn.Module):\n    \n    # Constructor\n    def __init__(self, Layers):\n        super(Net_Xavier, self).__init__()\n        self.hidden = nn.ModuleList()\n\n        for input_size, output_size in zip(Layers, Layers[1:]):\n            linear = nn.Linear(input_size, output_size)\n            torch.nn.init.xavier_uniform_(linear.weight)\n            self.hidden.append(linear)\n    \n    # Prediction\n    def forward(self, x):\n        L = len(self.hidden)\n        for (l, linear_transform) in zip(range(L), self.hidden):\n            if l &lt; L - 1:\n                x = torch.tanh(linear_transform(x))\n            else:\n                x = linear_transform(x)\n        return x\n</pre> # Define the neural network with Xavier initialization  class Net_Xavier(nn.Module):          # Constructor     def __init__(self, Layers):         super(Net_Xavier, self).__init__()         self.hidden = nn.ModuleList()          for input_size, output_size in zip(Layers, Layers[1:]):             linear = nn.Linear(input_size, output_size)             torch.nn.init.xavier_uniform_(linear.weight)             self.hidden.append(linear)          # Prediction     def forward(self, x):         L = len(self.hidden)         for (l, linear_transform) in zip(range(L), self.hidden):             if l &lt; L - 1:                 x = torch.tanh(linear_transform(x))             else:                 x = linear_transform(x)         return x <p>Define the neural network module with Uniform Initialization:</p> In\u00a0[\u00a0]: Copied! <pre># Define the neural network with Uniform initialization\n\nclass Net_Uniform(nn.Module):\n    \n    # Constructor\n    def __init__(self, Layers):\n        super(Net_Uniform, self).__init__()\n        self.hidden = nn.ModuleList()\n\n        for input_size, output_size in zip(Layers, Layers[1:]):\n            linear = nn.Linear(input_size, output_size)\n            linear.weight.data.uniform_(0, 1)\n            self.hidden.append(linear)\n    \n    # Prediction\n    def forward(self, x):\n        L = len(self.hidden)\n        for (l, linear_transform) in zip(range(L), self.hidden):\n            if l &lt; L - 1:\n                x = torch.tanh(linear_transform(x))\n            else:\n                x = linear_transform(x)\n        return x\n</pre> # Define the neural network with Uniform initialization  class Net_Uniform(nn.Module):          # Constructor     def __init__(self, Layers):         super(Net_Uniform, self).__init__()         self.hidden = nn.ModuleList()          for input_size, output_size in zip(Layers, Layers[1:]):             linear = nn.Linear(input_size, output_size)             linear.weight.data.uniform_(0, 1)             self.hidden.append(linear)          # Prediction     def forward(self, x):         L = len(self.hidden)         for (l, linear_transform) in zip(range(L), self.hidden):             if l &lt; L - 1:                 x = torch.tanh(linear_transform(x))             else:                 x = linear_transform(x)         return x <p>Define the neural network module with PyTroch Default Initialization</p> In\u00a0[\u00a0]: Copied! <pre># Define the neural network with Default initialization\n\nclass Net(nn.Module):\n    \n    # Constructor\n    def __init__(self, Layers):\n        super(Net, self).__init__()\n        self.hidden = nn.ModuleList()\n\n        for input_size, output_size in zip(Layers, Layers[1:]):\n            linear = nn.Linear(input_size, output_size)\n            self.hidden.append(linear)\n    \n    # Prediction\n    def forward(self, x):\n        L = len(self.hidden)\n        for (l, linear_transform) in zip(range(L), self.hidden):\n            if l &lt; L - 1:\n                x = torch.tanh(linear_transform(x))\n            else:\n                x = linear_transform(x)\n        return x\n</pre> # Define the neural network with Default initialization  class Net(nn.Module):          # Constructor     def __init__(self, Layers):         super(Net, self).__init__()         self.hidden = nn.ModuleList()          for input_size, output_size in zip(Layers, Layers[1:]):             linear = nn.Linear(input_size, output_size)             self.hidden.append(linear)          # Prediction     def forward(self, x):         L = len(self.hidden)         for (l, linear_transform) in zip(range(L), self.hidden):             if l &lt; L - 1:                 x = torch.tanh(linear_transform(x))             else:                 x = linear_transform(x)         return x <p>Define a function to train the model, in this case the function returns a Python dictionary to store the training loss and accuracy on the validation data</p> In\u00a0[\u00a0]: Copied! <pre># function to Train the model\n\ndef train(model, criterion, train_loader, validation_loader, optimizer, epochs = 100):\n    i = 0\n    loss_accuracy = {'training_loss':[], 'validation_accuracy':[]}  \n    \n    for epoch in range(epochs):\n        for i,(x, y) in enumerate(train_loader):\n            optimizer.zero_grad()\n            z = model(x.view(-1, 28 * 28))\n            loss = criterion(z, y)\n            loss.backward()\n            optimizer.step()\n            loss_accuracy['training_loss'].append(loss.data.item())\n            \n        correct = 0\n        for x, y in validation_loader:\n            yhat = model(x.view(-1, 28 * 28))\n            _, label = torch.max(yhat, 1)\n            correct += (label==y).sum().item()\n        accuracy = 100 * (correct / len(validation_dataset))\n        loss_accuracy['validation_accuracy'].append(accuracy)\n        \n    return loss_accuracy\n</pre> # function to Train the model  def train(model, criterion, train_loader, validation_loader, optimizer, epochs = 100):     i = 0     loss_accuracy = {'training_loss':[], 'validation_accuracy':[]}            for epoch in range(epochs):         for i,(x, y) in enumerate(train_loader):             optimizer.zero_grad()             z = model(x.view(-1, 28 * 28))             loss = criterion(z, y)             loss.backward()             optimizer.step()             loss_accuracy['training_loss'].append(loss.data.item())                      correct = 0         for x, y in validation_loader:             yhat = model(x.view(-1, 28 * 28))             _, label = torch.max(yhat, 1)             correct += (label==y).sum().item()         accuracy = 100 * (correct / len(validation_dataset))         loss_accuracy['validation_accuracy'].append(accuracy)              return loss_accuracy <p>Load the training dataset by setting the parameters <code>train </code> to <code>True</code> and convert it to a tensor  by placing a transform object int the argument <code>transform</code></p> In\u00a0[\u00a0]: Copied! <pre># Create the train dataset\n\ntrain_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n</pre> # Create the train dataset  train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor()) <p>Load the testing dataset by setting the parameters <code>train</code> to <code>False</code> and convert it to a tensor  by placing a transform object int the argument <code>transform</code></p> In\u00a0[\u00a0]: Copied! <pre># Create the validation dataset\n\nvalidation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n</pre> # Create the validation dataset  validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor()) <p>Create the training-data loader and the validation-data loader object</p> In\u00a0[\u00a0]: Copied! <pre># Create Dataloader for both train dataset and validation dataset\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)\n</pre> # Create Dataloader for both train dataset and validation dataset  train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True) validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False) <p>Create the criterion function</p> In\u00a0[\u00a0]: Copied! <pre># Define criterion function\n\ncriterion = nn.CrossEntropyLoss()\n</pre> # Define criterion function  criterion = nn.CrossEntropyLoss() <p>Create the model with 100 hidden layers</p> In\u00a0[\u00a0]: Copied! <pre># Set the parameters\n\ninput_dim = 28 * 28\noutput_dim = 10\nlayers = [input_dim, 100, 10, 100, 10, 100, output_dim]\nepochs = 15\n</pre> # Set the parameters  input_dim = 28 * 28 output_dim = 10 layers = [input_dim, 100, 10, 100, 10, 100, output_dim] epochs = 15 <p>Train the network using PyTorch Default Initialization</p> In\u00a0[\u00a0]: Copied! <pre># Train the model with default initialization\n\nmodel = Net(layers)\nlearning_rate = 0.01\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ntraining_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=epochs)\n</pre> # Train the model with default initialization  model = Net(layers) learning_rate = 0.01 optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) training_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=epochs) <p>Train the network using Xavier Initialization function</p> In\u00a0[\u00a0]: Copied! <pre># Train the model with Xavier initialization\n\nmodel_Xavier = Net_Xavier(layers)\noptimizer = torch.optim.SGD(model_Xavier.parameters(), lr=learning_rate)\ntraining_results_Xavier = train(model_Xavier, criterion, train_loader, validation_loader, optimizer, epochs=epochs)\n</pre> # Train the model with Xavier initialization  model_Xavier = Net_Xavier(layers) optimizer = torch.optim.SGD(model_Xavier.parameters(), lr=learning_rate) training_results_Xavier = train(model_Xavier, criterion, train_loader, validation_loader, optimizer, epochs=epochs) <p>Train the network using Uniform Initialization</p> In\u00a0[\u00a0]: Copied! <pre># Train the model with Uniform initialization\n\nmodel_Uniform = Net_Uniform(layers)\noptimizer = torch.optim.SGD(model_Uniform.parameters(), lr=learning_rate)\ntraining_results_Uniform = train(model_Uniform, criterion, train_loader, validation_loader, optimizer, epochs=epochs)\n</pre> # Train the model with Uniform initialization  model_Uniform = Net_Uniform(layers) optimizer = torch.optim.SGD(model_Uniform.parameters(), lr=learning_rate) training_results_Uniform = train(model_Uniform, criterion, train_loader, validation_loader, optimizer, epochs=epochs) <p>Compare the training loss for each initialization</p> In\u00a0[\u00a0]: Copied! <pre># Plot the loss\n\nplt.plot(training_results_Xavier['training_loss'], label='Xavier')\nplt.plot(training_results['training_loss'], label='Default')\nplt.plot(training_results_Uniform['training_loss'], label='Uniform')\nplt.ylabel('loss')\nplt.xlabel('iteration ')  \nplt.title('training loss iterations')\nplt.legend()\n</pre> # Plot the loss  plt.plot(training_results_Xavier['training_loss'], label='Xavier') plt.plot(training_results['training_loss'], label='Default') plt.plot(training_results_Uniform['training_loss'], label='Uniform') plt.ylabel('loss') plt.xlabel('iteration ')   plt.title('training loss iterations') plt.legend() <p>compare the validation loss for each model</p> In\u00a0[\u00a0]: Copied! <pre># Plot the accuracy\n\nplt.plot(training_results_Xavier['validation_accuracy'], label='Xavier')\nplt.plot(training_results['validation_accuracy'], label='Default')\nplt.plot(training_results_Uniform['validation_accuracy'], label='Uniform') \nplt.ylabel('validation accuracy')\nplt.xlabel('epochs')   \nplt.legend()\n</pre> # Plot the accuracy  plt.plot(training_results_Xavier['validation_accuracy'], label='Xavier') plt.plot(training_results['validation_accuracy'], label='Default') plt.plot(training_results_Uniform['validation_accuracy'], label='Uniform')  plt.ylabel('validation accuracy') plt.xlabel('epochs')    plt.legend() What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.2Xaviermist1layer_v2/#Model","title":"Neural Network Module and Training Function","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.2Xaviermist1layer_v2/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.2Xaviermist1layer_v2/#Cost","title":"Define Neural Network, Criterion function, Optimizer and Train the Model","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.2Xaviermist1layer_v2/#Train","title":"Test PyTorch Default Initialization, Xavier Initialization, Uniform Initialization","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.2Xaviermist1layer_v2/#Result","title":"Analyse Results","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.3.He_Initialization_v2/","title":"He Init","text":"Test Uniform, Default and He Initialization on MNIST Dataset with Relu Activation Objective for this Notebook  1. Learn how to Define Several Neural Network, Criterion function, Optimizer.  2. Test Uniform, Default and He Initialization  Table of Contents <p>In this lab, you will test the Uniform Initialization, Default Initialization and He Initialization on the MNIST dataset with Relu Activation</p> <ul> <li>Neural Network Module and Training Function</li> <li>Make Some Data</li> <li>Define Several Neural Network, Criterion function, Optimizer</li> <li>Test Uniform, Default and He Initialization</li> <li>Analyze Results</li> </ul> <p>Estimated Time Needed: 25 min</p> Preparation <p>We'll need the following libraries:</p> In\u00a0[\u00a0]: Copied! <pre># Import the libraries we need to use in this lab\n\n# Using the following line code to install the torchvision library\n# !mamba install -y torchvision\n\nimport torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport torch.nn.functional as F\nimport matplotlib.pylab as plt\nimport numpy as np\n\ntorch.manual_seed(0)\n</pre> # Import the libraries we need to use in this lab  # Using the following line code to install the torchvision library # !mamba install -y torchvision  import torch  import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets import torch.nn.functional as F import matplotlib.pylab as plt import numpy as np  torch.manual_seed(0) <p>Define the neural network module or class with He Initialization</p> In\u00a0[\u00a0]: Copied! <pre># Define the class for neural network model with He Initialization\n\nclass Net_He(nn.Module):\n    \n    # Constructor\n    def __init__(self, Layers):\n        super(Net_He, self).__init__()\n        self.hidden = nn.ModuleList()\n\n        for input_size, output_size in zip(Layers, Layers[1:]):\n            linear = nn.Linear(input_size, output_size)\n            torch.nn.init.kaiming_uniform_(linear.weight, nonlinearity='relu')\n            self.hidden.append(linear)\n\n    # Prediction\n    def forward(self, x):\n        L = len(self.hidden)\n        for (l, linear_transform) in zip(range(L), self.hidden):\n            if l &lt; L - 1:\n                x = F.relu(linear_transform(x))\n            else:\n                x = linear_transform(x)\n        return x\n</pre> # Define the class for neural network model with He Initialization  class Net_He(nn.Module):          # Constructor     def __init__(self, Layers):         super(Net_He, self).__init__()         self.hidden = nn.ModuleList()          for input_size, output_size in zip(Layers, Layers[1:]):             linear = nn.Linear(input_size, output_size)             torch.nn.init.kaiming_uniform_(linear.weight, nonlinearity='relu')             self.hidden.append(linear)      # Prediction     def forward(self, x):         L = len(self.hidden)         for (l, linear_transform) in zip(range(L), self.hidden):             if l &lt; L - 1:                 x = F.relu(linear_transform(x))             else:                 x = linear_transform(x)         return x <p>Define the class or neural network with Uniform Initialization</p> In\u00a0[\u00a0]: Copied! <pre># Define the class for neural network model with Uniform Initialization\n\nclass Net_Uniform(nn.Module):\n    \n    # Constructor\n    def __init__(self, Layers):\n        super(Net_Uniform, self).__init__()\n        self.hidden = nn.ModuleList()\n\n        for input_size, output_size in zip(Layers, Layers[1:]):\n            linear = nn.Linear(input_size,output_size)\n            linear.weight.data.uniform_(0, 1)\n            self.hidden.append(linear)\n    \n    # Prediction\n    def forward(self, x):\n        L = len(self.hidden)\n        for (l, linear_transform) in zip(range(L), self.hidden):\n            if l &lt; L - 1:\n                x = F.relu(linear_transform(x))\n            else:\n                x = linear_transform(x)\n                \n        return x\n</pre> # Define the class for neural network model with Uniform Initialization  class Net_Uniform(nn.Module):          # Constructor     def __init__(self, Layers):         super(Net_Uniform, self).__init__()         self.hidden = nn.ModuleList()          for input_size, output_size in zip(Layers, Layers[1:]):             linear = nn.Linear(input_size,output_size)             linear.weight.data.uniform_(0, 1)             self.hidden.append(linear)          # Prediction     def forward(self, x):         L = len(self.hidden)         for (l, linear_transform) in zip(range(L), self.hidden):             if l &lt; L - 1:                 x = F.relu(linear_transform(x))             else:                 x = linear_transform(x)                          return x <p>Class or Neural Network with PyTorch Default Initialization</p> In\u00a0[\u00a0]: Copied! <pre># Define the class for neural network model with PyTorch Default Initialization\n\nclass Net(nn.Module):\n    \n    # Constructor\n    def __init__(self, Layers):\n        super(Net, self).__init__()\n        self.hidden = nn.ModuleList()\n\n        for input_size, output_size in zip(Layers, Layers[1:]):\n            linear = nn.Linear(input_size, output_size)\n            self.hidden.append(linear)\n        \n    def forward(self, x):\n        L=len(self.hidden)\n        for (l, linear_transform) in zip(range(L), self.hidden):\n            if l &lt; L - 1:\n                x = F.relu(linear_transform(x))\n            else:\n                x = linear_transform(x)\n                \n        return x\n</pre> # Define the class for neural network model with PyTorch Default Initialization  class Net(nn.Module):          # Constructor     def __init__(self, Layers):         super(Net, self).__init__()         self.hidden = nn.ModuleList()          for input_size, output_size in zip(Layers, Layers[1:]):             linear = nn.Linear(input_size, output_size)             self.hidden.append(linear)              def forward(self, x):         L=len(self.hidden)         for (l, linear_transform) in zip(range(L), self.hidden):             if l &lt; L - 1:                 x = F.relu(linear_transform(x))             else:                 x = linear_transform(x)                          return x <p>Define a function to train the model, in this case the function returns a Python dictionary to store the training loss and accuracy on the validation data</p> In\u00a0[\u00a0]: Copied! <pre># Define function to  train model\n\ndef train(model, criterion, train_loader, validation_loader, optimizer, epochs = 100):\n    i = 0\n    loss_accuracy = {'training_loss': [], 'validation_accuracy': []}  \n    \n    #n_epochs\n    for epoch in range(epochs):\n        for i, (x, y) in enumerate(train_loader):\n            optimizer.zero_grad()\n            z = model(x.view(-1, 28 * 28))\n            loss = criterion(z, y)\n            loss.backward()\n            optimizer.step()\n            loss_accuracy['training_loss'].append(loss.data.item())\n        \n        correct = 0\n        for x, y in validation_loader:\n            yhat = model(x.view(-1, 28 * 28))\n            _, label = torch.max(yhat, 1)\n            correct += (label == y).sum().item()\n        accuracy = 100 * (correct / len(validation_dataset))\n        loss_accuracy['validation_accuracy'].append(accuracy)\n    \n    return loss_accuracy\n</pre> # Define function to  train model  def train(model, criterion, train_loader, validation_loader, optimizer, epochs = 100):     i = 0     loss_accuracy = {'training_loss': [], 'validation_accuracy': []}            #n_epochs     for epoch in range(epochs):         for i, (x, y) in enumerate(train_loader):             optimizer.zero_grad()             z = model(x.view(-1, 28 * 28))             loss = criterion(z, y)             loss.backward()             optimizer.step()             loss_accuracy['training_loss'].append(loss.data.item())                  correct = 0         for x, y in validation_loader:             yhat = model(x.view(-1, 28 * 28))             _, label = torch.max(yhat, 1)             correct += (label == y).sum().item()         accuracy = 100 * (correct / len(validation_dataset))         loss_accuracy['validation_accuracy'].append(accuracy)          return loss_accuracy <p>Load the training dataset by setting the parameters <code>train </code> to <code>True</code> and convert it to a tensor  by placing a transform object int the argument <code>transform</code></p> In\u00a0[\u00a0]: Copied! <pre># Create the training dataset\n\ntrain_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n</pre> # Create the training dataset  train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor()) <p>Load the testing dataset by setting the parameters train  <code>False</code> and convert it to a tensor  by placing a transform object int the argument <code>transform</code></p> In\u00a0[\u00a0]: Copied! <pre># Create the validation dataset\n\nvalidation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n</pre> # Create the validation dataset  validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor()) <p>Create the training-data loader and the validation-data loader object</p> In\u00a0[\u00a0]: Copied! <pre># Create the data loader for training and validation\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)\n</pre> # Create the data loader for training and validation  train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True) validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False) <p>Create the criterion function</p> In\u00a0[\u00a0]: Copied! <pre># Create the criterion function\n\ncriterion = nn.CrossEntropyLoss()\n</pre> # Create the criterion function  criterion = nn.CrossEntropyLoss() <p>Create a list that contains layer size</p> In\u00a0[\u00a0]: Copied! <pre># Create the parameters\n\ninput_dim = 28 * 28\noutput_dim = 10\nlayers = [input_dim, 100, 200, 100, output_dim]\n</pre> # Create the parameters  input_dim = 28 * 28 output_dim = 10 layers = [input_dim, 100, 200, 100, output_dim] <p>Train the network using PyTorch Default Initialization</p> In\u00a0[\u00a0]: Copied! <pre># Train the model with the default initialization\n\nmodel = Net(layers)\nlearning_rate = 0.01\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ntraining_results = train(model, criterion, train_loader,validation_loader, optimizer, epochs=30)\n</pre> # Train the model with the default initialization  model = Net(layers) learning_rate = 0.01 optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) training_results = train(model, criterion, train_loader,validation_loader, optimizer, epochs=30) <p>Train the network using He Initialization function</p> In\u00a0[\u00a0]: Copied! <pre># Train the model with the He initialization\n\nmodel_He = Net_He(layers)\noptimizer = torch.optim.SGD(model_He.parameters(), lr=learning_rate)\ntraining_results_He = train(model_He, criterion, train_loader, validation_loader, optimizer, epochs=30)\n</pre> # Train the model with the He initialization  model_He = Net_He(layers) optimizer = torch.optim.SGD(model_He.parameters(), lr=learning_rate) training_results_He = train(model_He, criterion, train_loader, validation_loader, optimizer, epochs=30) <p>Train the network using Uniform Initialization function</p> In\u00a0[\u00a0]: Copied! <pre># Train the model with the Uniform initialization\n\nmodel_Uniform = Net_Uniform(layers)\noptimizer = torch.optim.SGD(model_Uniform.parameters(), lr=learning_rate)\ntraining_results_Uniform = train(model_Uniform, criterion, train_loader, validation_loader, optimizer, epochs=30)\n</pre> # Train the model with the Uniform initialization  model_Uniform = Net_Uniform(layers) optimizer = torch.optim.SGD(model_Uniform.parameters(), lr=learning_rate) training_results_Uniform = train(model_Uniform, criterion, train_loader, validation_loader, optimizer, epochs=30) <p>Compare the training loss for each activation</p> In\u00a0[\u00a0]: Copied! <pre># Plot the loss\n\nplt.plot(training_results_He['training_loss'], label='He')\nplt.plot(training_results['training_loss'], label='Default')\nplt.plot(training_results_Uniform['training_loss'], label='Uniform')\nplt.ylabel('loss')\nplt.xlabel('iteration ') \nplt.title('training loss iterations')\nplt.legend()\n</pre> # Plot the loss  plt.plot(training_results_He['training_loss'], label='He') plt.plot(training_results['training_loss'], label='Default') plt.plot(training_results_Uniform['training_loss'], label='Uniform') plt.ylabel('loss') plt.xlabel('iteration ')  plt.title('training loss iterations') plt.legend() <p>Compare the validation loss for each model</p> In\u00a0[\u00a0]: Copied! <pre># Plot the accuracy\n\nplt.plot(training_results_He['validation_accuracy'], label='He')\nplt.plot(training_results['validation_accuracy'], label='Default')\nplt.plot(training_results_Uniform['validation_accuracy'], label='Uniform') \nplt.ylabel('validation accuracy')\nplt.xlabel('epochs ')   \nplt.legend()\nplt.show()\n</pre> # Plot the accuracy  plt.plot(training_results_He['validation_accuracy'], label='He') plt.plot(training_results['validation_accuracy'], label='Default') plt.plot(training_results_Uniform['validation_accuracy'], label='Uniform')  plt.ylabel('validation accuracy') plt.xlabel('epochs ')    plt.legend() plt.show() What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.3.He_Initialization_v2/#Model","title":"Neural Network Module and Training Function","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.3.He_Initialization_v2/#Makeup_Data","title":"Make some Data","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.3.He_Initialization_v2/#Cost","title":"Define Neural Network, Criterion function, Optimizer and Train the Model","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.3.He_Initialization_v2/#Train","title":"Test PyTorch Default Initialization, Xavier Initialization and Uniform Initialization","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.3.3.He_Initialization_v2/#Result","title":"Analyze Results","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.1_MomentumwithPolynomialFunctions_v2/","title":"Momentum","text":"Momentum Objective for this Notebook  1. Learn Saddle Points, Local Minima, and Noise Table of Contents <p>In this lab, you will deal with several problems associated with optimization and see how momentum can improve your results.</p> <ul> <li>Saddle Points</li> <li>Local Minima</li> <li> Noise </li> </ul> <p>Estimated Time Needed: 25 min</p> Preparation <p>Import the following libraries that you'll use for this lab:</p> In\u00a0[\u00a0]: Copied! <pre># These are the libraries that will be used for this lab.\n\nimport torch \nimport torch.nn as nn\nimport matplotlib.pylab as plt\nimport numpy as np\n\ntorch.manual_seed(0)\n</pre> # These are the libraries that will be used for this lab.  import torch  import torch.nn as nn import matplotlib.pylab as plt import numpy as np  torch.manual_seed(0) <p>This function will plot a cubic function and the parameter values obtained via Gradient Descent.</p> In\u00a0[\u00a0]: Copied! <pre># Plot the cubic\n\ndef plot_cubic(w, optimizer):\n    LOSS = []\n    # parameter values \n    W = torch.arange(-4, 4, 0.1)\n    # plot the loss fuction \n    for w.state_dict()['linear.weight'][0] in W:\n        LOSS.append(cubic(w(torch.tensor([[1.0]]))).item())\n    w.state_dict()['linear.weight'][0] = 4.0\n    n_epochs = 10\n    parameter = []\n    loss_list = []\n\n    # n_epochs\n    # Use PyTorch custom module to implement a ploynomial function\n    for n in range(n_epochs):\n        optimizer.zero_grad() \n        loss = cubic(w(torch.tensor([[1.0]])))\n        loss_list.append(loss)\n        parameter.append(w.state_dict()['linear.weight'][0].detach().data.item())\n        loss.backward()\n        optimizer.step()\n    plt.plot(parameter, loss_list, 'ro', label='parameter values')\n    plt.plot(W.numpy(), LOSS, label='objective function')\n    plt.xlabel('w')\n    plt.ylabel('l(w)')\n    plt.legend()\n</pre> # Plot the cubic  def plot_cubic(w, optimizer):     LOSS = []     # parameter values      W = torch.arange(-4, 4, 0.1)     # plot the loss fuction      for w.state_dict()['linear.weight'][0] in W:         LOSS.append(cubic(w(torch.tensor([[1.0]]))).item())     w.state_dict()['linear.weight'][0] = 4.0     n_epochs = 10     parameter = []     loss_list = []      # n_epochs     # Use PyTorch custom module to implement a ploynomial function     for n in range(n_epochs):         optimizer.zero_grad()          loss = cubic(w(torch.tensor([[1.0]])))         loss_list.append(loss)         parameter.append(w.state_dict()['linear.weight'][0].detach().data.item())         loss.backward()         optimizer.step()     plt.plot(parameter, loss_list, 'ro', label='parameter values')     plt.plot(W.numpy(), LOSS, label='objective function')     plt.xlabel('w')     plt.ylabel('l(w)')     plt.legend() <p>This function will plot a 4th order function and the parameter values obtained via Gradient Descent. You can also add Gaussian noise with a standard deviation determined by the parameter <code>std</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Plot the fourth order function and the parameter values\n\ndef plot_fourth_order(w, optimizer, std=0, color='r', paramlabel='parameter values', objfun=True):\n    W = torch.arange(-4, 6, 0.1)\n    LOSS = []\n    for w.state_dict()['linear.weight'][0] in W:\n        LOSS.append(fourth_order(w(torch.tensor([[1.0]]))).item())\n    w.state_dict()['linear.weight'][0] = 6\n    n_epochs = 100\n    parameter = []\n    loss_list = []\n\n    #n_epochs\n    for n in range(n_epochs):\n        optimizer.zero_grad()\n        loss = fourth_order(w(torch.tensor([[1.0]]))) + std * torch.randn(1, 1)\n        loss_list.append(loss)\n        parameter.append(w.state_dict()['linear.weight'][0].detach().data.item())\n        loss.backward()\n        optimizer.step()\n    \n    # Plotting\n    if objfun:\n        plt.plot(W.numpy(), LOSS, label='objective function')\n    plt.plot(parameter, loss_list, 'ro',label=paramlabel, color=color)\n    plt.xlabel('w')\n    plt.ylabel('l(w)')\n    plt.legend()\n</pre> # Plot the fourth order function and the parameter values  def plot_fourth_order(w, optimizer, std=0, color='r', paramlabel='parameter values', objfun=True):     W = torch.arange(-4, 6, 0.1)     LOSS = []     for w.state_dict()['linear.weight'][0] in W:         LOSS.append(fourth_order(w(torch.tensor([[1.0]]))).item())     w.state_dict()['linear.weight'][0] = 6     n_epochs = 100     parameter = []     loss_list = []      #n_epochs     for n in range(n_epochs):         optimizer.zero_grad()         loss = fourth_order(w(torch.tensor([[1.0]]))) + std * torch.randn(1, 1)         loss_list.append(loss)         parameter.append(w.state_dict()['linear.weight'][0].detach().data.item())         loss.backward()         optimizer.step()          # Plotting     if objfun:         plt.plot(W.numpy(), LOSS, label='objective function')     plt.plot(parameter, loss_list, 'ro',label=paramlabel, color=color)     plt.xlabel('w')     plt.ylabel('l(w)')     plt.legend() <p>This is a custom module. It will behave like a single parameter value. We do it this way so we can use PyTorch's build-in optimizers .</p> In\u00a0[\u00a0]: Copied! <pre># Create a linear model\n\nclass one_param(nn.Module):\n    \n    # Constructor\n    def __init__(self, input_size, output_size):\n        super(one_param, self).__init__()\n        self.linear = nn.Linear(input_size, output_size, bias=False)\n        \n    # Prediction\n    def forward(self, x):\n        yhat = self.linear(x)\n        return yhat\n</pre> # Create a linear model  class one_param(nn.Module):          # Constructor     def __init__(self, input_size, output_size):         super(one_param, self).__init__()         self.linear = nn.Linear(input_size, output_size, bias=False)              # Prediction     def forward(self, x):         yhat = self.linear(x)         return yhat <p>We create an object <code>w</code>, when we call the object with an input of one, it will behave like an individual parameter value. i.e <code>w(1)</code> is analogous to $w$</p> In\u00a0[\u00a0]: Copied! <pre># Create a one_param object\n\nw = one_param(1, 1)\n</pre> # Create a one_param object  w = one_param(1, 1) <p>Let's create a cubic function with Saddle points</p> In\u00a0[\u00a0]: Copied! <pre># Define a function to output a cubic \n\ndef cubic(yhat):\n    out = yhat ** 3\n    return out\n</pre> # Define a function to output a cubic   def cubic(yhat):     out = yhat ** 3     return out <p>We create an optimizer with no momentum term</p> In\u00a0[\u00a0]: Copied! <pre># Create a optimizer without momentum\n\noptimizer = torch.optim.SGD(w.parameters(), lr=0.01, momentum=0)\n</pre> # Create a optimizer without momentum  optimizer = torch.optim.SGD(w.parameters(), lr=0.01, momentum=0) <p>We run several iterations of stochastic gradient descent and plot the results. We see the parameter values get stuck in the saddle point.</p> In\u00a0[\u00a0]: Copied! <pre># Plot the model\n\nplot_cubic(w, optimizer)\n</pre> # Plot the model  plot_cubic(w, optimizer) <p>we create an optimizer with momentum term of 0.9</p> In\u00a0[\u00a0]: Copied! <pre># Create a optimizer with momentum\n\noptimizer = torch.optim.SGD(w.parameters(), lr=0.01, momentum=0.9)\n</pre> # Create a optimizer with momentum  optimizer = torch.optim.SGD(w.parameters(), lr=0.01, momentum=0.9) <p>We run several iterations of stochastic gradient descent with momentum and plot the results. We see the parameter values do not get stuck in the saddle point.</p> In\u00a0[\u00a0]: Copied! <pre># Plot the model\n\nplot_cubic(w, optimizer)\n</pre> # Plot the model  plot_cubic(w, optimizer) <p>In this section, we will create a fourth order polynomial with a local minimum at 4 and a global minimum a -2. We will then see how the momentum parameter affects convergence to a global minimum. The fourth order polynomial is given by:</p> In\u00a0[\u00a0]: Copied! <pre># Create a function to calculate the fourth order polynomial \n\ndef fourth_order(yhat): \n    out = torch.mean(2 * (yhat ** 4) - 9 * (yhat ** 3) - 21 * (yhat ** 2) + 88 * yhat + 48)\n    return out\n</pre> # Create a function to calculate the fourth order polynomial   def fourth_order(yhat):      out = torch.mean(2 * (yhat ** 4) - 9 * (yhat ** 3) - 21 * (yhat ** 2) + 88 * yhat + 48)     return out <p>We create an optimizer with no momentum term. We run several iterations of stochastic gradient descent and plot the results. We see the parameter values get stuck in the local minimum.</p> In\u00a0[\u00a0]: Copied! <pre># Make the prediction without momentum\n\noptimizer = torch.optim.SGD(w.parameters(), lr=0.001)\nplot_fourth_order(w, optimizer)\n</pre> # Make the prediction without momentum  optimizer = torch.optim.SGD(w.parameters(), lr=0.001) plot_fourth_order(w, optimizer) <p>We create an optimizer with a  momentum term of 0.9. We run several iterations of stochastic gradient descent and plot the results. We see the parameter values reach a global minimum.</p> In\u00a0[\u00a0]: Copied! <pre># Make the prediction with momentum\n\noptimizer = torch.optim.SGD(w.parameters(), lr=0.001, momentum=0.9)\nplot_fourth_order(w, optimizer)\n</pre> # Make the prediction with momentum  optimizer = torch.optim.SGD(w.parameters(), lr=0.001, momentum=0.9) plot_fourth_order(w, optimizer) <p>In this section, we will create a fourth order polynomial with a local minimum at 4 and a global minimum a -2, but we will add noise to the function when the Gradient is calculated. We will then see how the momentum parameter affects convergence to a global minimum.</p> <p>with no momentum, we get stuck in a local minimum</p> In\u00a0[\u00a0]: Copied! <pre># Make the prediction without momentum when there is noise\n\noptimizer = torch.optim.SGD(w.parameters(), lr=0.001)\nplot_fourth_order(w, optimizer, std=10)\n</pre> # Make the prediction without momentum when there is noise  optimizer = torch.optim.SGD(w.parameters(), lr=0.001) plot_fourth_order(w, optimizer, std=10) <p>with  momentum, we get to the global  minimum</p> In\u00a0[\u00a0]: Copied! <pre># Make the prediction with momentum when there is noise\n\noptimizer = torch.optim.SGD(w.parameters(), lr=0.001,momentum=0.9)\nplot_fourth_order(w, optimizer, std=10)\n</pre> # Make the prediction with momentum when there is noise  optimizer = torch.optim.SGD(w.parameters(), lr=0.001,momentum=0.9) plot_fourth_order(w, optimizer, std=10) Practice <p>Create two <code> SGD</code>  objects with a learning rate of <code> 0.001</code>. Use the default momentum parameter value  for one and a value of <code> 0.9</code> for the second. Use the function <code>plot_fourth_order</code> with an <code>std=100</code>, to plot the different steps of each. Make sure you run the function on two independent cells.</p> In\u00a0[\u00a0]: Copied! <pre># Practice: Create two SGD optimizer with lr = 0.001, and one without momentum and the other with momentum = 0.9. Plot the result out.\n\n# Type your code here\n</pre> # Practice: Create two SGD optimizer with lr = 0.001, and one without momentum and the other with momentum = 0.9. Plot the result out.  # Type your code here <p>Double-click here for the solution.</p> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.1_MomentumwithPolynomialFunctions_v2/#Saddle","title":"Saddle Points","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.1_MomentumwithPolynomialFunctions_v2/#Minima","title":"Local Minima","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.1_MomentumwithPolynomialFunctions_v2/#Noise","title":"Noise","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.2_NeuralNetworkswithMomentum_v2/","title":"NN with Momentum","text":"Neural Networks with Momentum Objective for this Notebook  1. Train Different Neural Networks Model different values for the Momentum Parameter.  2. Compare Results of Different Momentum Terms.  Table of Contents <p>In this lab, you will see how different values for the momentum parameters affect the convergence rate of a neural network.</p> <ul> <li>Neural Network Module and Function for Training</li> <li>Train Different Neural Networks Model different values for the Momentum Parameter</li> <li>Compare Results of Different Momentum Terms</li> </ul> <p>Estimated Time Needed: 25 min</p> Preparation <p>We'll need the following libraries:</p> In\u00a0[\u00a0]: Copied! <pre># Import the libraries for this lab\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom matplotlib.colors import ListedColormap\nfrom torch.utils.data import Dataset, DataLoader\n\ntorch.manual_seed(1)\nnp.random.seed(1)\n</pre> # Import the libraries for this lab  import matplotlib.pyplot as plt  import numpy as np import torch import torch.nn as nn import torch.nn.functional as F from matplotlib.colors import ListedColormap from torch.utils.data import Dataset, DataLoader  torch.manual_seed(1) np.random.seed(1) <p>Functions used to plot:</p> In\u00a0[\u00a0]: Copied! <pre># Define a function for plot the decision region\n\ndef plot_decision_regions_3class(model, data_set):\n    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA','#00AAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#00FF00','#00AAFF'])\n    X=data_set.x.numpy()\n    y=data_set.y.numpy()\n    h = .02\n    x_min, x_max = X[:, 0].min() - 0.1 , X[:, 0].max() + 0.1 \n    y_min, y_max = X[:, 1].min() - 0.1 , X[:, 1].max() + 0.1 \n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n    XX=torch.torch.Tensor(np.c_[xx.ravel(), yy.ravel()])\n    _,yhat=torch.max(model(XX),1)\n    yhat=yhat.numpy().reshape(xx.shape)\n    plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)\n    plt.plot(X[y[:]==0,0], X[y[:]==0,1], 'ro', label='y=0')\n    plt.plot(X[y[:]==1,0], X[y[:]==1,1], 'go', label='y=1')\n    plt.plot(X[y[:]==2,0], X[y[:]==2,1], 'o', label='y=2')\n    plt.title(\"decision region\")\n    plt.legend()\n</pre> # Define a function for plot the decision region  def plot_decision_regions_3class(model, data_set):     cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA','#00AAFF'])     cmap_bold = ListedColormap(['#FF0000', '#00FF00','#00AAFF'])     X=data_set.x.numpy()     y=data_set.y.numpy()     h = .02     x_min, x_max = X[:, 0].min() - 0.1 , X[:, 0].max() + 0.1      y_min, y_max = X[:, 1].min() - 0.1 , X[:, 1].max() + 0.1      xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))     XX=torch.torch.Tensor(np.c_[xx.ravel(), yy.ravel()])     _,yhat=torch.max(model(XX),1)     yhat=yhat.numpy().reshape(xx.shape)     plt.pcolormesh(xx, yy, yhat, cmap=cmap_light)     plt.plot(X[y[:]==0,0], X[y[:]==0,1], 'ro', label='y=0')     plt.plot(X[y[:]==1,0], X[y[:]==1,1], 'go', label='y=1')     plt.plot(X[y[:]==2,0], X[y[:]==2,1], 'o', label='y=2')     plt.title(\"decision region\")     plt.legend() <p>Create the dataset class</p> In\u00a0[\u00a0]: Copied! <pre># Create the dataset class\n\nclass Data(Dataset):\n    \n    #  modified from: http://cs231n.github.io/neural-networks-case-study/\n    # Constructor\n    def __init__(self, K=3, N=500):\n        D = 2\n        X = np.zeros((N * K, D)) # data matrix (each row = single example)\n        y = np.zeros(N * K, dtype='uint8') # class labels\n        for j in range(K):\n          ix = range(N * j, N * (j + 1))\n          r = np.linspace(0.0, 1, N) # radius\n          t = np.linspace(j * 4, (j + 1) * 4, N) + np.random.randn(N) * 0.2 # theta\n          X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]\n          y[ix] = j\n    \n        self.y = torch.from_numpy(y).type(torch.LongTensor)\n        self.x = torch.from_numpy(X).type(torch.FloatTensor)\n        self.len = y.shape[0]\n            \n    # Getter\n    def __getitem__(self, index):    \n        return self.x[index], self.y[index]\n    \n    # Get Length\n    def __len__(self):\n        return self.len\n    \n    # Plot the diagram\n    def plot_data(self):\n        plt.plot(self.x[self.y[:] == 0, 0].numpy(), self.x[self.y[:] == 0, 1].numpy(), 'o', label=\"y=0\")\n        plt.plot(self.x[self.y[:] == 1, 0].numpy(), self.x[self.y[:] == 1, 1].numpy(), 'ro', label=\"y=1\")\n        plt.plot(self.x[self.y[:] == 2, 0].numpy(),self.x[self.y[:] == 2, 1].numpy(), 'go',label=\"y=2\")\n        plt.legend()\n</pre> # Create the dataset class  class Data(Dataset):          #  modified from: http://cs231n.github.io/neural-networks-case-study/     # Constructor     def __init__(self, K=3, N=500):         D = 2         X = np.zeros((N * K, D)) # data matrix (each row = single example)         y = np.zeros(N * K, dtype='uint8') # class labels         for j in range(K):           ix = range(N * j, N * (j + 1))           r = np.linspace(0.0, 1, N) # radius           t = np.linspace(j * 4, (j + 1) * 4, N) + np.random.randn(N) * 0.2 # theta           X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]           y[ix] = j              self.y = torch.from_numpy(y).type(torch.LongTensor)         self.x = torch.from_numpy(X).type(torch.FloatTensor)         self.len = y.shape[0]                  # Getter     def __getitem__(self, index):             return self.x[index], self.y[index]          # Get Length     def __len__(self):         return self.len          # Plot the diagram     def plot_data(self):         plt.plot(self.x[self.y[:] == 0, 0].numpy(), self.x[self.y[:] == 0, 1].numpy(), 'o', label=\"y=0\")         plt.plot(self.x[self.y[:] == 1, 0].numpy(), self.x[self.y[:] == 1, 1].numpy(), 'ro', label=\"y=1\")         plt.plot(self.x[self.y[:] == 2, 0].numpy(),self.x[self.y[:] == 2, 1].numpy(), 'go',label=\"y=2\")         plt.legend() <p>Create Neural Network Module using <code>ModuleList()</code></p> In\u00a0[\u00a0]: Copied! <pre># Create dataset object\n\nclass Net(nn.Module):\n    \n    # Constructor\n    def __init__(self, Layers):\n        super(Net, self).__init__()\n        self.hidden = nn.ModuleList()\n        for input_size, output_size in zip(Layers, Layers[1:]):\n            self.hidden.append(nn.Linear(input_size, output_size))\n    \n    # Prediction\n    def forward(self, activation):\n        L = len(self.hidden)\n        for (l, linear_transform) in zip(range(L), self.hidden):\n            if l &lt; L - 1:\n                activation = F.relu(linear_transform(activation))    \n            else:\n                activation = linear_transform(activation)\n        return activation\n</pre> # Create dataset object  class Net(nn.Module):          # Constructor     def __init__(self, Layers):         super(Net, self).__init__()         self.hidden = nn.ModuleList()         for input_size, output_size in zip(Layers, Layers[1:]):             self.hidden.append(nn.Linear(input_size, output_size))          # Prediction     def forward(self, activation):         L = len(self.hidden)         for (l, linear_transform) in zip(range(L), self.hidden):             if l &lt; L - 1:                 activation = F.relu(linear_transform(activation))                 else:                 activation = linear_transform(activation)         return activation <p>Create the function for training the model.</p> In\u00a0[\u00a0]: Copied! <pre># Define the function for training the model\n\ndef train(data_set, model, criterion, train_loader, optimizer, epochs=100):\n    LOSS = []\n    ACC = []\n    for epoch in range(epochs):\n        for x, y in train_loader:\n            optimizer.zero_grad()\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        LOSS.append(loss.item())\n        ACC.append(accuracy(model,data_set))\n        \n    results ={\"Loss\":LOSS, \"Accuracy\":ACC}\n    fig, ax1 = plt.subplots()\n    color = 'tab:red'\n    ax1.plot(LOSS,color=color)\n    ax1.set_xlabel('epoch', color=color)\n    ax1.set_ylabel('total loss', color=color)\n    ax1.tick_params(axis = 'y', color=color)\n    \n    ax2 = ax1.twinx()  \n    color = 'tab:blue'\n    ax2.set_ylabel('accuracy', color=color)  # we already handled the x-label with ax1\n    ax2.plot(ACC, color=color)\n    ax2.tick_params(axis='y', color=color)\n    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n    \n    plt.show()\n    return results\n</pre> # Define the function for training the model  def train(data_set, model, criterion, train_loader, optimizer, epochs=100):     LOSS = []     ACC = []     for epoch in range(epochs):         for x, y in train_loader:             optimizer.zero_grad()             yhat = model(x)             loss = criterion(yhat, y)             optimizer.zero_grad()             loss.backward()             optimizer.step()         LOSS.append(loss.item())         ACC.append(accuracy(model,data_set))              results ={\"Loss\":LOSS, \"Accuracy\":ACC}     fig, ax1 = plt.subplots()     color = 'tab:red'     ax1.plot(LOSS,color=color)     ax1.set_xlabel('epoch', color=color)     ax1.set_ylabel('total loss', color=color)     ax1.tick_params(axis = 'y', color=color)          ax2 = ax1.twinx()       color = 'tab:blue'     ax2.set_ylabel('accuracy', color=color)  # we already handled the x-label with ax1     ax2.plot(ACC, color=color)     ax2.tick_params(axis='y', color=color)     fig.tight_layout()  # otherwise the right y-label is slightly clipped          plt.show()     return results <p>Define a function used to calculate accuracy.</p> In\u00a0[\u00a0]: Copied! <pre># Define a function for calculating accuracy\n\ndef accuracy(model, data_set):\n    _, yhat = torch.max(model(data_set.x), 1)\n    return (yhat == data_set.y).numpy().mean()\n</pre> # Define a function for calculating accuracy  def accuracy(model, data_set):     _, yhat = torch.max(model(data_set.x), 1)     return (yhat == data_set.y).numpy().mean() <p>Crate a dataset object using <code>Data</code></p> In\u00a0[\u00a0]: Copied! <pre># Create the dataset and plot it\n\ndata_set = Data()\ndata_set.plot_data()\ndata_set.y = data_set.y.view(-1)\n</pre> # Create the dataset and plot it  data_set = Data() data_set.plot_data() data_set.y = data_set.y.view(-1) <p>Dictionary to contain different cost and  accuracy values for each epoch  for different values of the momentum parameter.</p> In\u00a0[\u00a0]: Copied! <pre># Initialize a dictionary to contain the cost and accuracy\n\nResults = {\"momentum 0\": {\"Loss\": 0, \"Accuracy:\": 0}, \"momentum 0.1\": {\"Loss\": 0, \"Accuracy:\": 0}}\n</pre> # Initialize a dictionary to contain the cost and accuracy  Results = {\"momentum 0\": {\"Loss\": 0, \"Accuracy:\": 0}, \"momentum 0.1\": {\"Loss\": 0, \"Accuracy:\": 0}} <p>Create a  network to classify three classes with 1 hidden layer with 50 neurons and a momentum value of zero.</p> In\u00a0[\u00a0]: Copied! <pre># Train a model with 1 hidden layer and 50 neurons\n\nLayers = [2, 50, 3]\nmodel = Net(Layers)\nlearning_rate = 0.10\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ntrain_loader = DataLoader(dataset=data_set, batch_size=20)\ncriterion = nn.CrossEntropyLoss()\nResults[\"momentum 0\"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100)\nplot_decision_regions_3class(model, data_set)\n</pre> # Train a model with 1 hidden layer and 50 neurons  Layers = [2, 50, 3] model = Net(Layers) learning_rate = 0.10 optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) train_loader = DataLoader(dataset=data_set, batch_size=20) criterion = nn.CrossEntropyLoss() Results[\"momentum 0\"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100) plot_decision_regions_3class(model, data_set) <p>Create a network to classify three classes with 1 hidden layer with 50 neurons and a momentum value of 0.1.</p> In\u00a0[\u00a0]: Copied! <pre># Train a model with 1 hidden layer and 50 neurons with 0.1 momentum\n\nLayers = [2, 50, 3]\nmodel = Net(Layers)\nlearning_rate = 0.10\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.1)\ntrain_loader = DataLoader(dataset=data_set, batch_size=20)\ncriterion = nn.CrossEntropyLoss()\nResults[\"momentum 0.1\"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100)\nplot_decision_regions_3class(model, data_set)\n</pre> # Train a model with 1 hidden layer and 50 neurons with 0.1 momentum  Layers = [2, 50, 3] model = Net(Layers) learning_rate = 0.10 optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.1) train_loader = DataLoader(dataset=data_set, batch_size=20) criterion = nn.CrossEntropyLoss() Results[\"momentum 0.1\"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100) plot_decision_regions_3class(model, data_set) <p>Create a network to classify three classes with 1 hidden layer with 50 neurons and a momentum value of 0.2.</p> In\u00a0[\u00a0]: Copied! <pre># Train a model with 1 hidden layer and 50 neurons with 0.2 momentum\n\nLayers = [2, 50, 3]\nmodel = Net(Layers)\nlearning_rate = 0.10\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.2)\ntrain_loader = DataLoader(dataset=data_set, batch_size=20)\ncriterion = nn.CrossEntropyLoss()\nResults[\"momentum 0.2\"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100)\nplot_decision_regions_3class(model, data_set)\n</pre> # Train a model with 1 hidden layer and 50 neurons with 0.2 momentum  Layers = [2, 50, 3] model = Net(Layers) learning_rate = 0.10 optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.2) train_loader = DataLoader(dataset=data_set, batch_size=20) criterion = nn.CrossEntropyLoss() Results[\"momentum 0.2\"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100) plot_decision_regions_3class(model, data_set) <p>Create a network to classify three classes with 1 hidden layer with 50 neurons and a momentum value of 0.4.</p> In\u00a0[\u00a0]: Copied! <pre># Train a model with 1 hidden layer and 50 neurons with 0.4 momentum\n\nLayers = [2, 50, 3]\nmodel = Net(Layers)\nlearning_rate = 0.10\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.4)\ntrain_loader = DataLoader(dataset=data_set, batch_size=20)\ncriterion = nn.CrossEntropyLoss()\nResults[\"momentum 0.4\"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100)\nplot_decision_regions_3class(model, data_set)\n</pre> # Train a model with 1 hidden layer and 50 neurons with 0.4 momentum  Layers = [2, 50, 3] model = Net(Layers) learning_rate = 0.10 optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.4) train_loader = DataLoader(dataset=data_set, batch_size=20) criterion = nn.CrossEntropyLoss() Results[\"momentum 0.4\"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100) plot_decision_regions_3class(model, data_set) <p>Create a network to classify three classes with 1 hidden layer with 50 neurons and a momentum value of 0.5.</p> In\u00a0[\u00a0]: Copied! <pre># Train a model with 1 hidden layer and 50 neurons with 0.5 momentum\n\nLayers = [2, 50, 3]\nmodel = Net(Layers)\nlearning_rate = 0.10\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.5)\ntrain_loader = DataLoader(dataset=data_set, batch_size=20)\ncriterion = nn.CrossEntropyLoss()\nResults[\"momentum 0.5\"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100)\nplot_decision_regions_3class(model,data_set)\n</pre> # Train a model with 1 hidden layer and 50 neurons with 0.5 momentum  Layers = [2, 50, 3] model = Net(Layers) learning_rate = 0.10 optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.5) train_loader = DataLoader(dataset=data_set, batch_size=20) criterion = nn.CrossEntropyLoss() Results[\"momentum 0.5\"] = train(data_set, model, criterion, train_loader, optimizer, epochs=100) plot_decision_regions_3class(model,data_set) <p>The plot below compares results of different momentum terms. We see that in general. The Cost decreases proportionally to the momentum term, but larger momentum terms lead to larger oscillations. While the momentum term decreases faster, it seems that a momentum term of 0.2 reaches the smallest value for the cost.</p> In\u00a0[\u00a0]: Copied! <pre># Plot the Loss result for each term\n\nfor key, value in Results.items():\n    plt.plot(value['Loss'],label=key)\n    plt.legend()\n    plt.xlabel('epoch')\n    plt.ylabel('Total Loss or Cost')\n</pre> # Plot the Loss result for each term  for key, value in Results.items():     plt.plot(value['Loss'],label=key)     plt.legend()     plt.xlabel('epoch')     plt.ylabel('Total Loss or Cost') <p>The  accuracy seems to be proportional to the momentum term.</p> In\u00a0[\u00a0]: Copied! <pre># Plot the Accuracy result for each term\n\nfor key, value in Results.items():\n    plt.plot(value['Accuracy'],label=key)\n    plt.legend()\n    plt.xlabel('epoch')\n    plt.ylabel('Accuracy')\n</pre> # Plot the Accuracy result for each term  for key, value in Results.items():     plt.plot(value['Accuracy'],label=key)     plt.legend()     plt.xlabel('epoch')     plt.ylabel('Accuracy') What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.2_NeuralNetworkswithMomentum_v2/#Model","title":"Neural Network Module and Function for Training","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.2_NeuralNetworkswithMomentum_v2/#Train","title":"Train Different Networks Model different values for the Momentum Parameter","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.4.2_NeuralNetworkswithMomentum_v2/#Result","title":"Compare Results of Different Momentum Terms","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.5.1BachNorm_v2/","title":"Batch Normalization","text":"Batch Normalization with the MNIST Dataset Objective for this Notebook  1. Define Several Neural Networks, Criterion function, Optimizer.  2. Train Neural Network using Batch Normalization and no Batch Normalization  Table of Contents In this lab, you will build a Neural Network using Batch Normalization and compare it to a Neural Network that does not use Batch Normalization. You will use the MNIST dataset to test your network.   <ul> <li>Neural Network Module and Training Function</li> <li>Load Data </li> <li>Define Several Neural Networks, Criterion function, Optimizer</li> <li>Train Neural Network using Batch Normalization and no Batch Normalization</li> <li>Analyze Results</li> </ul> <p>Estimated Time Needed: 25 min</p> Preparation <p>We'll need the following libraries:</p> In\u00a0[1]: Copied! <pre># These are the libraries will be used for this lab.\n\n# Using the following line code to install the torchvision library\n# !mamba install -y torchvision\n\nimport torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport torch.nn.functional as F\nimport matplotlib.pylab as plt\nimport numpy as np\ntorch.manual_seed(0)\n</pre> # These are the libraries will be used for this lab.  # Using the following line code to install the torchvision library # !mamba install -y torchvision  import torch  import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets import torch.nn.functional as F import matplotlib.pylab as plt import numpy as np torch.manual_seed(0) Out[1]: <pre>&lt;torch._C.Generator at 0x7fe38026e6b0&gt;</pre> <p>Define the neural network module or class</p> <p>Neural Network Module with two hidden layers using Batch Normalization</p> In\u00a0[2]: Copied! <pre># Define the Neural Network Model using Batch Normalization\n\nclass NetBatchNorm(nn.Module):\n    \n    # Constructor\n    def __init__(self, in_size, n_hidden1, n_hidden2, out_size):\n        super(NetBatchNorm, self).__init__()\n        self.linear1 = nn.Linear(in_size, n_hidden1)\n        self.linear2 = nn.Linear(n_hidden1, n_hidden2)\n        self.linear3 = nn.Linear(n_hidden2, out_size)\n        self.bn1 = nn.BatchNorm1d(n_hidden1)\n        self.bn2 = nn.BatchNorm1d(n_hidden2)\n        \n    # Prediction\n    def forward(self, x):\n        x = self.bn1(torch.sigmoid(self.linear1(x)))\n        x = self.bn2(torch.sigmoid(self.linear2(x)))\n        x = self.linear3(x)\n        return x\n    \n    # Activations, to analyze results \n    def activation(self, x):\n        out = []\n        z1 = self.bn1(self.linear1(x))\n        out.append(z1.detach().numpy().reshape(-1))\n        a1 = torch.sigmoid(z1)\n        out.append(a1.detach().numpy().reshape(-1).reshape(-1))\n        z2 = self.bn2(self.linear2(a1))\n        out.append(z2.detach().numpy().reshape(-1))\n        a2 = torch.sigmoid(z2)\n        out.append(a2.detach().numpy().reshape(-1))\n        return out\n</pre> # Define the Neural Network Model using Batch Normalization  class NetBatchNorm(nn.Module):          # Constructor     def __init__(self, in_size, n_hidden1, n_hidden2, out_size):         super(NetBatchNorm, self).__init__()         self.linear1 = nn.Linear(in_size, n_hidden1)         self.linear2 = nn.Linear(n_hidden1, n_hidden2)         self.linear3 = nn.Linear(n_hidden2, out_size)         self.bn1 = nn.BatchNorm1d(n_hidden1)         self.bn2 = nn.BatchNorm1d(n_hidden2)              # Prediction     def forward(self, x):         x = self.bn1(torch.sigmoid(self.linear1(x)))         x = self.bn2(torch.sigmoid(self.linear2(x)))         x = self.linear3(x)         return x          # Activations, to analyze results      def activation(self, x):         out = []         z1 = self.bn1(self.linear1(x))         out.append(z1.detach().numpy().reshape(-1))         a1 = torch.sigmoid(z1)         out.append(a1.detach().numpy().reshape(-1).reshape(-1))         z2 = self.bn2(self.linear2(a1))         out.append(z2.detach().numpy().reshape(-1))         a2 = torch.sigmoid(z2)         out.append(a2.detach().numpy().reshape(-1))         return out <p>Neural Network Module with two hidden layers with out Batch Normalization</p> In\u00a0[3]: Copied! <pre># Class Net for Neural Network Model\n\nclass Net(nn.Module):\n    \n    # Constructor\n    def __init__(self, in_size, n_hidden1, n_hidden2, out_size):\n\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(in_size, n_hidden1)\n        self.linear2 = nn.Linear(n_hidden1, n_hidden2)\n        self.linear3 = nn.Linear(n_hidden2, out_size)\n    \n    # Prediction\n    def forward(self, x):\n        x = torch.sigmoid(self.linear1(x))\n        x = torch.sigmoid(self.linear2(x))\n        x = self.linear3(x)\n        return x\n    \n    # Activations, to analyze results \n    def activation(self, x):\n        out = []\n        z1 = self.linear1(x)\n        out.append(z1.detach().numpy().reshape(-1))\n        a1 = torch.sigmoid(z1)\n        out.append(a1.detach().numpy().reshape(-1).reshape(-1))\n        z2 = self.linear2(a1)\n        out.append(z2.detach().numpy().reshape(-1))\n        a2 = torch.sigmoid(z2)\n        out.append(a2.detach().numpy().reshape(-1))\n        return out \n</pre> # Class Net for Neural Network Model  class Net(nn.Module):          # Constructor     def __init__(self, in_size, n_hidden1, n_hidden2, out_size):          super(Net, self).__init__()         self.linear1 = nn.Linear(in_size, n_hidden1)         self.linear2 = nn.Linear(n_hidden1, n_hidden2)         self.linear3 = nn.Linear(n_hidden2, out_size)          # Prediction     def forward(self, x):         x = torch.sigmoid(self.linear1(x))         x = torch.sigmoid(self.linear2(x))         x = self.linear3(x)         return x          # Activations, to analyze results      def activation(self, x):         out = []         z1 = self.linear1(x)         out.append(z1.detach().numpy().reshape(-1))         a1 = torch.sigmoid(z1)         out.append(a1.detach().numpy().reshape(-1).reshape(-1))         z2 = self.linear2(a1)         out.append(z2.detach().numpy().reshape(-1))         a2 = torch.sigmoid(z2)         out.append(a2.detach().numpy().reshape(-1))         return out   <p>Define a function to train the model. In this case the function returns a Python dictionary to store the training loss and accuracy on the validation data</p> In\u00a0[4]: Copied! <pre># Define the function to train model\n\ndef train(model, criterion, train_loader, validation_loader, optimizer, epochs=100):\n    i = 0\n    useful_stuff = {'training_loss':[], 'validation_accuracy':[]}  \n\n    for epoch in range(epochs):\n        for i, (x, y) in enumerate(train_loader):\n            model.train()\n            optimizer.zero_grad()\n            z = model(x.view(-1, 28 * 28))\n            loss = criterion(z, y)\n            loss.backward()\n            optimizer.step()\n            useful_stuff['training_loss'].append(loss.data.item())\n            \n        correct = 0\n        for x, y in validation_loader:\n            model.eval()\n            yhat = model(x.view(-1, 28 * 28))\n            _, label = torch.max(yhat, 1)\n            correct += (label == y).sum().item()\n            \n        accuracy = 100 * (correct / len(validation_dataset))\n        useful_stuff['validation_accuracy'].append(accuracy)\n    \n    return useful_stuff\n</pre> # Define the function to train model  def train(model, criterion, train_loader, validation_loader, optimizer, epochs=100):     i = 0     useful_stuff = {'training_loss':[], 'validation_accuracy':[]}        for epoch in range(epochs):         for i, (x, y) in enumerate(train_loader):             model.train()             optimizer.zero_grad()             z = model(x.view(-1, 28 * 28))             loss = criterion(z, y)             loss.backward()             optimizer.step()             useful_stuff['training_loss'].append(loss.data.item())                      correct = 0         for x, y in validation_loader:             model.eval()             yhat = model(x.view(-1, 28 * 28))             _, label = torch.max(yhat, 1)             correct += (label == y).sum().item()                      accuracy = 100 * (correct / len(validation_dataset))         useful_stuff['validation_accuracy'].append(accuracy)          return useful_stuff <p>Load the training dataset by setting the parameters <code>train </code> to <code>True</code> and convert it to a tensor  by placing a transform object int the argument <code>transform</code></p> In\u00a0[5]: Copied! <pre># load the train dataset\n\ntrain_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n</pre> # load the train dataset  train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor()) <p>Load the validating dataset by setting the parameters train  <code>False</code> and convert it to a tensor by placing a transform object into the argument <code>transform</code></p> In\u00a0[6]: Copied! <pre># load the train dataset\n\nvalidation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n</pre> # load the train dataset  validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor()) <p>create the training-data loader and the validation-data loader object</p> In\u00a0[7]: Copied! <pre># Create Data Loader for both train and validating\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)\n</pre> # Create Data Loader for both train and validating  train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True) validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False) <p></p> Define Neural Network, Criterion function, Optimizer and Train the  Model   <p>Create the criterion function</p> In\u00a0[8]: Copied! <pre># Create the criterion function\n\ncriterion = nn.CrossEntropyLoss()\n</pre> # Create the criterion function  criterion = nn.CrossEntropyLoss() <p>Variables for Neural Network Shape <code> hidden_dim</code> used for number of neurons in both hidden layers.</p> In\u00a0[9]: Copied! <pre># Set the parameters\n\ninput_dim = 28 * 28\nhidden_dim = 100\noutput_dim = 10\n</pre> # Set the parameters  input_dim = 28 * 28 hidden_dim = 100 output_dim = 10 <p>Train Neural Network using  Batch Normalization :</p> In\u00a0[10]: Copied! <pre># Create model, optimizer and train the model\n\nmodel_norm  = NetBatchNorm(input_dim, hidden_dim, hidden_dim, output_dim)\noptimizer = torch.optim.Adam(model_norm.parameters(), lr = 0.1)\ntraining_results_Norm=train(model_norm , criterion, train_loader, validation_loader, optimizer, epochs=5)\n</pre> # Create model, optimizer and train the model  model_norm  = NetBatchNorm(input_dim, hidden_dim, hidden_dim, output_dim) optimizer = torch.optim.Adam(model_norm.parameters(), lr = 0.1) training_results_Norm=train(model_norm , criterion, train_loader, validation_loader, optimizer, epochs=5) <p>Train Neural Network with no Batch Normalization:</p> In\u00a0[\u00a0]: Copied! <pre># Create model without Batch Normalization, optimizer and train the model\n\nmodel = Net(input_dim, hidden_dim, hidden_dim, output_dim)\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\ntraining_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=5)\n</pre> # Create model without Batch Normalization, optimizer and train the model  model = Net(input_dim, hidden_dim, hidden_dim, output_dim) optimizer = torch.optim.Adam(model.parameters(), lr = 0.1) training_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=5) <p>Compare the histograms of the activation for the first layer of the first sample, for both models.</p> In\u00a0[\u00a0]: Copied! <pre>model.eval()\nmodel_norm.eval()\nout=model.activation(validation_dataset[0][0].reshape(-1,28*28))\nplt.hist(out[2],label='model with no batch normalization' )\nout_norm=model_norm.activation(validation_dataset[0][0].reshape(-1,28*28))\nplt.hist(out_norm[2],label='model with normalization')\nplt.xlabel(\"activation \")\nplt.legend()\nplt.show()\n</pre> model.eval() model_norm.eval() out=model.activation(validation_dataset[0][0].reshape(-1,28*28)) plt.hist(out[2],label='model with no batch normalization' ) out_norm=model_norm.activation(validation_dataset[0][0].reshape(-1,28*28)) plt.hist(out_norm[2],label='model with normalization') plt.xlabel(\"activation \") plt.legend() plt.show() <p>We see the activations with Batch Normalization are zero centred and have a smaller variance.</p> <p>Compare the training loss for each iteration</p> In\u00a0[\u00a0]: Copied! <pre># Plot the diagram to show the loss\n\nplt.plot(training_results['training_loss'], label='No Batch Normalization')\nplt.plot(training_results_Norm['training_loss'], label='Batch Normalization')\nplt.ylabel('Cost')\nplt.xlabel('iterations ')   \nplt.legend()\nplt.show()\n</pre> # Plot the diagram to show the loss  plt.plot(training_results['training_loss'], label='No Batch Normalization') plt.plot(training_results_Norm['training_loss'], label='Batch Normalization') plt.ylabel('Cost') plt.xlabel('iterations ')    plt.legend() plt.show() <p>Compare the validating accuracy for each iteration</p> In\u00a0[\u00a0]: Copied! <pre># Plot the diagram to show the accuracy\n\nplt.plot(training_results['validation_accuracy'],label='No Batch Normalization')\nplt.plot(training_results_Norm['validation_accuracy'],label='Batch Normalization')\nplt.ylabel('validation accuracy')\nplt.xlabel('epochs ')   \nplt.legend()\nplt.show()\n</pre> # Plot the diagram to show the accuracy  plt.plot(training_results['validation_accuracy'],label='No Batch Normalization') plt.plot(training_results_Norm['validation_accuracy'],label='Batch Normalization') plt.ylabel('validation accuracy') plt.xlabel('epochs ')    plt.legend() plt.show() What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week8-Deep-Networks/8.5.1BachNorm_v2/#Train_Func","title":"Neural Network Module and Training Function","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.5.1BachNorm_v2/#Makeup_Data","title":"Make Some Data","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.5.1BachNorm_v2/#Train","title":"Train Neural Network using Batch Normalization and no Batch Normalization","text":""},{"location":"Deep%20Learning/Week8-Deep-Networks/8.5.1BachNorm_v2/#Result","title":"Analyze Results","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.1What_is_Convolution/","title":"Convolution Basics","text":"Objective for this Notebook <ul> <li>Learn about Convolution.</li> <li>Leran Determining  the Size of Output. </li> <li>Learn Stride, Zero Padding</li> </ul>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>Import the following libraries:</p> In\u00a0[1]: Copied! <pre>import torch \nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import ndimage, misc\n</pre> import torch  import torch.nn as nn import matplotlib.pyplot as plt import numpy as np from scipy import ndimage, misc <p></p> What is Convolution? <p>Convolution is a linear operation similar to a linear equation, dot product, or matrix multiplication. Convolution has several advantages for analyzing images. As discussed in the video, convolution preserves the relationship between elements, and it requires fewer parameters than other methods.</p> <p>You can see the relationship between the different methods that you learned:</p> <p>$$linear \\ equation :y=wx+b$$ $$linear\\ equation\\ with\\ multiple \\ variables \\ where \\ \\mathbf{x} \\ is \\ a \\ vector \\ \\mathbf{y}=\\mathbf{wx}+b$$ $$ \\ matrix\\ multiplication \\ where \\ \\mathbf{X} \\ in \\ a \\ matrix \\ \\mathbf{y}=\\mathbf{wX}+\\mathbf{b} $$ $$\\ convolution \\ where \\ \\mathbf{X} \\ and \\ \\mathbf{Y} \\ is \\ a \\ tensor \\  \\mathbf{Y}=\\mathbf{w}*\\mathbf{X}+\\mathbf{b}$$</p> <p>In convolution, the parameter w is called a kernel. You can perform convolution on images where you let the variable image denote the variable X and w denote the parameter.</p> <p>Create a two-dimensional convolution object by using the constructor Conv2d, the parameter <code>in_channels</code> and <code>out_channels</code> will be used for this section, and the parameter kernel_size will be three.</p> In\u00a0[2]: Copied! <pre>conv = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\nconv\n</pre> conv = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3) conv Out[2]: <pre>Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))</pre> <p>Because the parameters in <code>nn.Conv2d</code> are randomly initialized and learned through training, give them some values.</p> In\u00a0[3]: Copied! <pre>conv.state_dict()['weight'][0][0]=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])\nconv.state_dict()['bias'][0]=0.0\nconv.state_dict()\n</pre> conv.state_dict()['weight'][0][0]=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]]) conv.state_dict()['bias'][0]=0.0 conv.state_dict() Out[3]: <pre>OrderedDict([('weight',\n              tensor([[[[ 1.,  0., -1.],\n                        [ 2.,  0., -2.],\n                        [ 1.,  0., -1.]]]])),\n             ('bias', tensor([0.]))])</pre> <p>Create a dummy tensor to represent an image. The shape of the image is (1,1,5,5) where:</p> <p>(number of inputs, number of channels, number of rows, number of columns )</p> <p>Set the third column to 1:</p> In\u00a0[4]: Copied! <pre>image=torch.zeros(1,1,5,5)\nimage[0,0,:,2]=1\nimage\n</pre> image=torch.zeros(1,1,5,5) image[0,0,:,2]=1 image Out[4]: <pre>tensor([[[[0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.]]]])</pre> <p>Call the object <code>conv</code> on the tensor <code>image</code> as an input to perform the convolution and assign the result to the tensor <code>z</code>.</p> In\u00a0[5]: Copied! <pre>z=conv(image)\nz\n</pre> z=conv(image) z Out[5]: <pre>tensor([[[[-4.,  0.,  4.],\n          [-4.,  0.,  4.],\n          [-4.,  0.,  4.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)</pre> <p>The following animation illustrates the process, the kernel performs at the element-level multiplication on every element in the image in the corresponding region. The values are then added together. The kernel is then shifted and the process is repeated.</p> <p></p> Determining  the Size of the Output <p>The size of the output is an important parameter. In this lab, you will assume square images. For rectangular images, the same formula can be used in for each dimension independently.</p> <p>Let M be the size of the input and K be the size of the kernel. The size of the output is given by the following formula:</p> <p>$$M_{new}=M-K+1$$</p> <p>Create a kernel of size 2:</p> In\u00a0[6]: Copied! <pre>K=2\nconv1 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=K)\nconv1.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\nconv1.state_dict()['bias'][0]=0.0\nconv1.state_dict()\nconv1\n</pre> K=2 conv1 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=K) conv1.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]]) conv1.state_dict()['bias'][0]=0.0 conv1.state_dict() conv1 Out[6]: <pre>Conv2d(1, 1, kernel_size=(2, 2), stride=(1, 1))</pre> <p>Create an image of size 2:</p> In\u00a0[7]: Copied! <pre>M=4\nimage1=torch.ones(1,1,M,M)\n</pre> M=4 image1=torch.ones(1,1,M,M) <p>The following equation provides the output:</p> <p>$$M_{new}=M-K+1$$ $$M_{new}=4-2+1$$ $$M_{new}=3$$</p> <p>The following animation illustrates the process: The first iteration of the kernel overlay of the images produces one output. As the kernel is of size K, there are M-K  elements for the kernel to move in the horizontal direction. The same logic applies to the vertical direction.</p> <p>Perform the convolution and verify the size is correct:</p> In\u00a0[8]: Copied! <pre>z1=conv1(image1)\nprint(\"z1:\",z1)\nprint(\"shape:\",z1.shape[2:4])\n</pre> z1=conv1(image1) print(\"z1:\",z1) print(\"shape:\",z1.shape[2:4]) <pre>z1: tensor([[[[4., 4., 4.],\n          [4., 4., 4.],\n          [4., 4., 4.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\nshape: torch.Size([3, 3])\n</pre> <p></p> Stride parameter <p>The parameter stride changes the number of shifts the kernel moves per iteration. As a result, the output size also changes and is given by the following formula:</p> <p>$$M_{new}=\\dfrac{M-K}{stride}+1$$</p> <p>Create a convolution object with a stride of 2:</p> In\u00a0[9]: Copied! <pre>conv3 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=2)\n\nconv3.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\nconv3.state_dict()['bias'][0]=0.0\nconv3.state_dict()\n</pre> conv3 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=2)  conv3.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]]) conv3.state_dict()['bias'][0]=0.0 conv3.state_dict() Out[9]: <pre>OrderedDict([('weight',\n              tensor([[[[1., 1.],\n                        [1., 1.]]]])),\n             ('bias', tensor([0.]))])</pre> <p>For an image with a size of 4, calculate the output size:</p> <p>$$M_{new}=\\dfrac{M-K}{stride}+1$$ $$M_{new}=\\dfrac{4-2}{2}+1$$ $$M_{new}=2$$</p> <p>The following animation illustrates the process: The first iteration of the kernel overlay of the images produces one output. Because the kernel is of size K, there are M-K=2 elements. The stride is 2 because it will move 2 elements at a time. As a result, you divide M-K by the stride value 2:</p> <p>Perform the convolution and verify the size is correct:</p> In\u00a0[10]: Copied! <pre>z3=conv3(image1)\n\nprint(\"z3:\",z3)\nprint(\"shape:\",z3.shape[2:4])\n</pre> z3=conv3(image1)  print(\"z3:\",z3) print(\"shape:\",z3.shape[2:4]) <pre>z3: tensor([[[[4., 4.],\n          [4., 4.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\nshape: torch.Size([2, 2])\n</pre> <p></p> Zero Padding  <p>As you apply successive convolutions, the image will shrink. You can apply zero padding to keep the image at a reasonable size, which also holds information at the borders.</p> <p>In addition, you might not get integer values for the size of the kernel. Consider the following image:</p> In\u00a0[11]: Copied! <pre>image1\n</pre> image1 Out[11]: <pre>tensor([[[[1., 1., 1., 1.],\n          [1., 1., 1., 1.],\n          [1., 1., 1., 1.],\n          [1., 1., 1., 1.]]]])</pre> <p>Try performing convolutions with the <code>kernel_size=2</code> and a <code>stride=3</code>. Use these values:</p> <p>$$M_{new}=\\dfrac{M-K}{stride}+1$$ $$M_{new}=\\dfrac{4-2}{3}+1$$ $$M_{new}=1.666$$</p> In\u00a0[12]: Copied! <pre>conv4 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=3)\nconv4.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\nconv4.state_dict()['bias'][0]=0.0\nconv4.state_dict()\nz4=conv4(image1)\nprint(\"z4:\",z4)\nprint(\"z4:\",z4.shape[2:4])\n</pre> conv4 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=3) conv4.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]]) conv4.state_dict()['bias'][0]=0.0 conv4.state_dict() z4=conv4(image1) print(\"z4:\",z4) print(\"z4:\",z4.shape[2:4]) <pre>z4: tensor([[[[4.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\nz4: torch.Size([1, 1])\n</pre> <p>You can add rows and columns of zeros around the image. This is called padding. In the constructor <code>Conv2d</code>, you specify the number of rows or columns of zeros that you want to add with the parameter padding.</p> <p>For a square image, you merely pad an extra column of zeros to the first column and the last column. Repeat the process for the rows. As a result, for a square image, the width and height is the original size plus 2 x the number of padding elements specified. You can then determine the size of the output after subsequent operations accordingly as shown in the following equation where you determine the size of an image after padding and then applying a convolutions kernel of size K.</p> <p>$$M'=M+2 \\times padding$$ $$M_{new}=M'-K+1$$</p> <p>Consider the following example:</p> In\u00a0[13]: Copied! <pre>conv5 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=3,padding=1)\n\nconv5.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]])\nconv5.state_dict()['bias'][0]=0.0\nconv5.state_dict()\nz5=conv5(image1)\nprint(\"z5:\",z5)\nprint(\"z5:\",z4.shape[2:4])\n</pre> conv5 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=2,stride=3,padding=1)  conv5.state_dict()['weight'][0][0]=torch.tensor([[1.0,1.0],[1.0,1.0]]) conv5.state_dict()['bias'][0]=0.0 conv5.state_dict() z5=conv5(image1) print(\"z5:\",z5) print(\"z5:\",z4.shape[2:4]) <pre>z5: tensor([[[[1., 2.],\n          [2., 4.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\nz5: torch.Size([1, 1])\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>The process is summarized in the following  animation:</p> <p></p> Practice Question  <p>A kernel of zeros with a kernel size=3  is applied to the following image:</p> In\u00a0[14]: Copied! <pre>Image=torch.randn((1,1,4,4))\nImage\n</pre> Image=torch.randn((1,1,4,4)) Image Out[14]: <pre>tensor([[[[-0.4460, -0.1425,  1.0888,  0.8292],\n          [ 1.0301, -0.4119, -1.0132, -0.4925],\n          [-1.1662, -0.5480,  1.7078,  0.0230],\n          [-0.1644,  1.8086, -1.1509, -0.2585]]]])</pre> <p>Question: Without using the function, determine what the outputs values are as each element:</p> <p>Double-click here for the solution.</p> <p>Question: Use the following convolution object to perform convolution on the tensor   <code>Image</code>:</p> In\u00a0[15]: Copied! <pre>conv = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\nconv.state_dict()['weight'][0][0]=torch.tensor([[0,0,0],[0,0,0],[0,0.0,0]])\nconv.state_dict()['bias'][0]=0.0\n</pre> conv = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3) conv.state_dict()['weight'][0][0]=torch.tensor([[0,0,0],[0,0,0],[0,0.0,0]]) conv.state_dict()['bias'][0]=0.0 <p>Double-click here for the solution.</p> <p>Question: You have an image of size 4. The parameters are as follows  kernel_size=2,stride=2. What is the size of the output?</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week9-CNNs/9.1What_is_Convolution/#author-juma-shafara-date-2024-08-08-title-convolution-neural-networks-keywords-training-two-parameter-mini-batch-gradient-decent-training-two-parameter-mini-batch-gradient-decent-description-in-this-lab-you-will-review-how-to-make-a-prediction-in-several-different-ways-by-using-pytorch","title":"author: Juma Shafara date: \"2024-08-08\" title: Convolution Neural Networks keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will review how to make a prediction in several different ways by using PyTorch.\u00b6","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.1What_is_Convolution/#table-of-contents","title":"Table of Contents\u00b6","text":"<p>In this lab, you will study convolution and review how the different operations change the relationship between input and output.</p> <li>What is Convolution  </li> <li>Determining  the Size of Output</li> <li>Stride</li> <li>Zero Padding </li> <li>Practice Questions </li> <p></p> Estimated Time Needed: 25 min"},{"location":"Deep%20Learning/Week9-CNNs/9.1What_is_Convolution/#preparation","title":"Preparation\u00b6","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.2Activation_max_pooling/","title":"Activation & Pooling","text":"Objective for this Notebook  1. Learn how to apply an activation function.  2. Learn about max pooling   Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>Import the following libraries:</p> In\u00a0[1]: Copied! <pre>import torch \nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import ndimage, misc\n</pre> import torch  import torch.nn as nn import matplotlib.pyplot as plt import numpy as np from scipy import ndimage, misc  <p></p> Activation Functions   <p>Just like a neural network, you apply an activation function to the activation map as shown in the following image:</p> <p>Create a kernel and image as usual. Set the bias to zero:</p> In\u00a0[2]: Copied! <pre>conv = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\nGx=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0,-1.0]])\nconv.state_dict()['weight'][0][0]=Gx\nconv.state_dict()['bias'][0]=0.0\nconv.state_dict()\n</pre> conv = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3) Gx=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0,-1.0]]) conv.state_dict()['weight'][0][0]=Gx conv.state_dict()['bias'][0]=0.0 conv.state_dict() Out[2]: <pre>OrderedDict([('weight',\n              tensor([[[[ 1.,  0., -1.],\n                        [ 2.,  0., -2.],\n                        [ 1.,  0., -1.]]]])),\n             ('bias', tensor([0.]))])</pre> In\u00a0[3]: Copied! <pre>image=torch.zeros(1,1,5,5)\nimage[0,0,:,2]=1\nimage\n</pre> image=torch.zeros(1,1,5,5) image[0,0,:,2]=1 image Out[3]: <pre>tensor([[[[0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.]]]])</pre> <p>The following image shows the image and kernel:</p> <p>Apply convolution to the image:</p> In\u00a0[4]: Copied! <pre>Z=conv(image)\nZ\n</pre> Z=conv(image) Z Out[4]: <pre>tensor([[[[-4.,  0.,  4.],\n          [-4.,  0.,  4.],\n          [-4.,  0.,  4.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)</pre> <p>Apply the activation function to the activation map. This will apply the activation function to each element in the activation map.</p> In\u00a0[5]: Copied! <pre>A=torch.relu(Z)\nA\n</pre> A=torch.relu(Z) A Out[5]: <pre>tensor([[[[0., 0., 4.],\n          [0., 0., 4.],\n          [0., 0., 4.]]]], grad_fn=&lt;ReluBackward0&gt;)</pre> In\u00a0[6]: Copied! <pre>relu = nn.ReLU()\nrelu(Z)\n</pre> relu = nn.ReLU() relu(Z) Out[6]: <pre>tensor([[[[0., 0., 4.],\n          [0., 0., 4.],\n          [0., 0., 4.]]]], grad_fn=&lt;ReluBackward0&gt;)</pre> <p>The process is summarized in the the following figure. The Relu function is applied to each element. All the elements less than zero are mapped to zero. The remaining components do not change.</p> <p></p> Max Pooling  <p>Consider the following image:</p> In\u00a0[7]: Copied! <pre>image1=torch.zeros(1,1,4,4)\nimage1[0,0,0,:]=torch.tensor([1.0,2.0,3.0,-4.0])\nimage1[0,0,1,:]=torch.tensor([0.0,2.0,-3.0,0.0])\nimage1[0,0,2,:]=torch.tensor([0.0,2.0,3.0,1.0])\n\nimage1\n</pre> image1=torch.zeros(1,1,4,4) image1[0,0,0,:]=torch.tensor([1.0,2.0,3.0,-4.0]) image1[0,0,1,:]=torch.tensor([0.0,2.0,-3.0,0.0]) image1[0,0,2,:]=torch.tensor([0.0,2.0,3.0,1.0])  image1 Out[7]: <pre>tensor([[[[ 1.,  2.,  3., -4.],\n          [ 0.,  2., -3.,  0.],\n          [ 0.,  2.,  3.,  1.],\n          [ 0.,  0.,  0.,  0.]]]])</pre> <p>Max pooling simply takes the maximum value in each region. Consider the following image. For the first region, max pooling simply takes the largest element in a yellow region.</p> <p>The region shifts, and the process is repeated. The process is similar to convolution and is demonstrated in the following figure:</p> <p>Create a maxpooling object in 2d as follows and perform max pooling as follows:</p> In\u00a0[8]: Copied! <pre>max1=torch.nn.MaxPool2d(2,stride=1)\nmax1(image1)\n</pre> max1=torch.nn.MaxPool2d(2,stride=1) max1(image1) Out[8]: <pre>tensor([[[[2., 3., 3.],\n          [2., 3., 3.],\n          [2., 3., 3.]]]])</pre> <p>If the stride is set to None (its defaults setting), the process will simply take the maximum in a prescribed area and shift over accordingly as shown in the following figure:</p> <p>Here's the code in Pytorch:</p> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week9-CNNs/9.2Activation_max_pooling/#author-juma-shafara-date-2024-08-12-title-activation-function-and-maxpooling-keywords-training-two-parameter-mini-batch-gradient-decent-training-two-parameter-mini-batch-gradient-decent-description-in-this-lab-you-will-learn-two-important-components-in-building-a-convolutional-neural-network","title":"author: Juma Shafara date: \"2024-08-12\" title: Activation function and Maxpooling keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will learn two important components in building a convolutional neural network.\u00b6","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.2Activation_max_pooling/#table-of-contents","title":"Table of Contents\u00b6","text":"<p>In this lab, you will learn two important components in building a convolutional neural network. The first is applying an activation function, which is analogous to building a regular network. You will also learn about max pooling. Max pooling reduces the number of parameters and makes the network less susceptible to changes in the image.</p> <li>Activation Functions</li> <li>Max Pooling</li> <p></p> Estimated Time Needed: 25 min"},{"location":"Deep%20Learning/Week9-CNNs/9.3Multiple_Channel_Convolution/","title":"Multiple Channels","text":"Objective for this Notebook  1. Learn on Multiple Input and Multiple Output Channels.  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>Import the following libraries:</p> In\u00a0[1]: Copied! <pre>import torch \nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import ndimage, misc\n</pre> import torch  import torch.nn as nn import matplotlib.pyplot as plt import numpy as np from scipy import ndimage, misc <p></p> Multiple Output Channels  <p>In Pytroch, you can create a <code>Conv2d</code> object with multiple outputs. For each channel, a kernel is created, and each kernel performs a convolution independently. As a result, the number of outputs is equal to the number of channels. This is demonstrated in the following figure. The number 9 is convolved with three kernels: each of a different color. There are three different activation maps represented by the different colors.</p> <p>Symbolically, this can be represented as follows:</p> <p>Create a <code>Conv2d</code> with three channels:</p> In\u00a0[2]: Copied! <pre>conv1 = nn.Conv2d(in_channels=1, out_channels=3,kernel_size=3)\n</pre> conv1 = nn.Conv2d(in_channels=1, out_channels=3,kernel_size=3) <p>Pytorch randomly assigns values to each kernel. However, use kernels that have  been developed to detect edges:</p> In\u00a0[3]: Copied! <pre>Gx=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])\nGy=torch.tensor([[1.0,2.0,1.0],[0.0,0.0,0.0],[-1.0,-2.0,-1.0]])\n\nconv1.state_dict()['weight'][0][0]=Gx\nconv1.state_dict()['weight'][1][0]=Gy\nconv1.state_dict()['weight'][2][0]=torch.ones(3,3)\n</pre> Gx=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]]) Gy=torch.tensor([[1.0,2.0,1.0],[0.0,0.0,0.0],[-1.0,-2.0,-1.0]])  conv1.state_dict()['weight'][0][0]=Gx conv1.state_dict()['weight'][1][0]=Gy conv1.state_dict()['weight'][2][0]=torch.ones(3,3) <p>Each kernel has its own bias, so set them all to zero:</p> In\u00a0[4]: Copied! <pre>conv1.state_dict()['bias'][:]=torch.tensor([0.0,0.0,0.0])\nconv1.state_dict()['bias']\n</pre> conv1.state_dict()['bias'][:]=torch.tensor([0.0,0.0,0.0]) conv1.state_dict()['bias'] Out[4]: <pre>tensor([0., 0., 0.])</pre> <p>Print out each kernel:</p> In\u00a0[5]: Copied! <pre>for x in conv1.state_dict()['weight']:\n    print(x)\n</pre> for x in conv1.state_dict()['weight']:     print(x) <pre>tensor([[[ 1.,  0., -1.],\n         [ 2.,  0., -2.],\n         [ 1.,  0., -1.]]])\ntensor([[[ 1.,  2.,  1.],\n         [ 0.,  0.,  0.],\n         [-1., -2., -1.]]])\ntensor([[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]])\n</pre> <p>Create an input <code>image</code> to represent the input X:</p> In\u00a0[6]: Copied! <pre>image=torch.zeros(1,1,5,5)\nimage[0,0,:,2]=1\nimage\n</pre> image=torch.zeros(1,1,5,5) image[0,0,:,2]=1 image Out[6]: <pre>tensor([[[[0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.]]]])</pre> <p>Plot it as an image:</p> In\u00a0[7]: Copied! <pre>plt.imshow(image[0,0,:,:].numpy(), interpolation='nearest', cmap=plt.cm.gray)\nplt.colorbar()\nplt.show()\n</pre> plt.imshow(image[0,0,:,:].numpy(), interpolation='nearest', cmap=plt.cm.gray) plt.colorbar() plt.show() <p>Perform convolution using each channel:</p> In\u00a0[8]: Copied! <pre>out=conv1(image)\n</pre> out=conv1(image) <p>The result is a 1x3x3x3 tensor. This represents one sample with three channels, and each channel contains a 3x3 image.  The same rules that govern the shape of each image were discussed in the last section.</p> In\u00a0[9]: Copied! <pre>out.shape\n</pre> out.shape Out[9]: <pre>torch.Size([1, 3, 3, 3])</pre> <p>Print out each channel as a tensor or an image:</p> In\u00a0[10]: Copied! <pre>for channel,image in enumerate(out[0]):\n    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n    print(image)\n    plt.title(\"channel {}\".format(channel))\n    plt.colorbar()\n    plt.show()\n</pre> for channel,image in enumerate(out[0]):     plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)     print(image)     plt.title(\"channel {}\".format(channel))     plt.colorbar()     plt.show() <pre>tensor([[-4.,  0.,  4.],\n        [-4.,  0.,  4.],\n        [-4.,  0.,  4.]], grad_fn=&lt;UnbindBackward0&gt;)\n</pre> <pre>tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]], grad_fn=&lt;UnbindBackward0&gt;)\n</pre> <pre>tensor([[3., 3., 3.],\n        [3., 3., 3.],\n        [3., 3., 3.]], grad_fn=&lt;UnbindBackward0&gt;)\n</pre> <p>Different kernels can be used to detect various features in an image. You can see that the first channel fluctuates, and the second two channels produce a constant value. The following figure summarizes the process:</p> <p>If you use a different image, the result will be different:</p> In\u00a0[11]: Copied! <pre>image1=torch.zeros(1,1,5,5)\nimage1[0,0,2,:]=1\nprint(image1)\nplt.imshow(image1[0,0,:,:].detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\nplt.show()\n</pre> image1=torch.zeros(1,1,5,5) image1[0,0,2,:]=1 print(image1) plt.imshow(image1[0,0,:,:].detach().numpy(), interpolation='nearest', cmap=plt.cm.gray) plt.show() <pre>tensor([[[[0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0.],\n          [1., 1., 1., 1., 1.],\n          [0., 0., 0., 0., 0.],\n          [0., 0., 0., 0., 0.]]]])\n</pre> <p>In this case, the second channel fluctuates, and the first and the third channels produce a constant value.</p> In\u00a0[12]: Copied! <pre>out1=conv1(image1)\nfor channel,image in enumerate(out1[0]):\n    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n    print(image)\n    plt.title(\"channel {}\".format(channel))\n    plt.colorbar()\n    plt.show()\n</pre> out1=conv1(image1) for channel,image in enumerate(out1[0]):     plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)     print(image)     plt.title(\"channel {}\".format(channel))     plt.colorbar()     plt.show() <pre>tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]], grad_fn=&lt;UnbindBackward0&gt;)\n</pre> <pre>tensor([[-4., -4., -4.],\n        [ 0.,  0.,  0.],\n        [ 4.,  4.,  4.]], grad_fn=&lt;UnbindBackward0&gt;)\n</pre> <pre>tensor([[3., 3., 3.],\n        [3., 3., 3.],\n        [3., 3., 3.]], grad_fn=&lt;UnbindBackward0&gt;)\n</pre> <p>The following figure summarizes the process:</p> <p></p> Multiple Input Channels  <p>For two inputs, you can create two kernels. Each kernel performs a convolution on its associated input channel. The resulting output is added together as shown:</p> <p>Create an input with two channels:</p> In\u00a0[13]: Copied! <pre>image2=torch.zeros(1,2,5,5)\nimage2[0,0,2,:]=-2\nimage2[0,1,2,:]=1\nimage2\n</pre> image2=torch.zeros(1,2,5,5) image2[0,0,2,:]=-2 image2[0,1,2,:]=1 image2 Out[13]: <pre>tensor([[[[ 0.,  0.,  0.,  0.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.],\n          [-2., -2., -2., -2., -2.],\n          [ 0.,  0.,  0.,  0.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.]],\n\n         [[ 0.,  0.,  0.,  0.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.],\n          [ 1.,  1.,  1.,  1.,  1.],\n          [ 0.,  0.,  0.,  0.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.]]]])</pre> <p>Plot out each image:</p> In\u00a0[14]: Copied! <pre>for channel,image in enumerate(image2[0]):\n    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n    print(image)\n    plt.title(\"channel {}\".format(channel))\n    plt.colorbar()\n    plt.show()\n</pre> for channel,image in enumerate(image2[0]):     plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)     print(image)     plt.title(\"channel {}\".format(channel))     plt.colorbar()     plt.show() <pre>tensor([[ 0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.],\n        [-2., -2., -2., -2., -2.],\n        [ 0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.]])\n</pre> <pre>tensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 1.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n</pre> <p>Create a <code>Conv2d</code> object with two inputs:</p> In\u00a0[15]: Copied! <pre>conv3 = nn.Conv2d(in_channels=2, out_channels=1,kernel_size=3)\n</pre> conv3 = nn.Conv2d(in_channels=2, out_channels=1,kernel_size=3) <p>Assign kernel values to make the math a little easier:</p> In\u00a0[16]: Copied! <pre>Gx1=torch.tensor([[0.0,0.0,0.0],[0,1.0,0],[0.0,0.0,0.0]])\nconv3.state_dict()['weight'][0][0]=1*Gx1\nconv3.state_dict()['weight'][0][1]=-2*Gx1\nconv3.state_dict()['bias'][:]=torch.tensor([0.0])\n</pre> Gx1=torch.tensor([[0.0,0.0,0.0],[0,1.0,0],[0.0,0.0,0.0]]) conv3.state_dict()['weight'][0][0]=1*Gx1 conv3.state_dict()['weight'][0][1]=-2*Gx1 conv3.state_dict()['bias'][:]=torch.tensor([0.0]) In\u00a0[17]: Copied! <pre>conv3.state_dict()['weight']\n</pre> conv3.state_dict()['weight'] Out[17]: <pre>tensor([[[[ 0.,  0.,  0.],\n          [ 0.,  1.,  0.],\n          [ 0.,  0.,  0.]],\n\n         [[-0., -0., -0.],\n          [-0., -2., -0.],\n          [-0., -0., -0.]]]])</pre> <p>Perform the convolution:</p> In\u00a0[18]: Copied! <pre>conv3(image2)\n</pre> conv3(image2) Out[18]: <pre>tensor([[[[ 0.,  0.,  0.],\n          [-4., -4., -4.],\n          [ 0.,  0.,  0.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)</pre> <p>The following images summarize the process. The object performs Convolution.</p> <p>Then, it adds the result:</p> <p></p> Multiple Input and Multiple Output Channels <p>When using multiple inputs and outputs, a kernel is created for each input, and the process is repeated for each output. The process is summarized in the following image.</p> <p>There are two input channels and 3 output channels. For each channel, the input in red and purple is convolved with an individual kernel that is colored differently. As a result, there are three outputs.</p> <p>Create an example with two inputs and three outputs and assign the kernel values to make the math a little easier:</p> In\u00a0[19]: Copied! <pre>conv4 = nn.Conv2d(in_channels=2, out_channels=3,kernel_size=3)\nconv4.state_dict()['weight'][0][0]=torch.tensor([[0.0,0.0,0.0],[0,0.5,0],[0.0,0.0,0.0]])\nconv4.state_dict()['weight'][0][1]=torch.tensor([[0.0,0.0,0.0],[0,0.5,0],[0.0,0.0,0.0]])\n\n\nconv4.state_dict()['weight'][1][0]=torch.tensor([[0.0,0.0,0.0],[0,1,0],[0.0,0.0,0.0]])\nconv4.state_dict()['weight'][1][1]=torch.tensor([[0.0,0.0,0.0],[0,-1,0],[0.0,0.0,0.0]])\n\nconv4.state_dict()['weight'][2][0]=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]])\nconv4.state_dict()['weight'][2][1]=torch.tensor([[1.0,2.0,1.0],[0.0,0.0,0.0],[-1.0,-2.0,-1.0]])\n</pre> conv4 = nn.Conv2d(in_channels=2, out_channels=3,kernel_size=3) conv4.state_dict()['weight'][0][0]=torch.tensor([[0.0,0.0,0.0],[0,0.5,0],[0.0,0.0,0.0]]) conv4.state_dict()['weight'][0][1]=torch.tensor([[0.0,0.0,0.0],[0,0.5,0],[0.0,0.0,0.0]])   conv4.state_dict()['weight'][1][0]=torch.tensor([[0.0,0.0,0.0],[0,1,0],[0.0,0.0,0.0]]) conv4.state_dict()['weight'][1][1]=torch.tensor([[0.0,0.0,0.0],[0,-1,0],[0.0,0.0,0.0]])  conv4.state_dict()['weight'][2][0]=torch.tensor([[1.0,0,-1.0],[2.0,0,-2.0],[1.0,0.0,-1.0]]) conv4.state_dict()['weight'][2][1]=torch.tensor([[1.0,2.0,1.0],[0.0,0.0,0.0],[-1.0,-2.0,-1.0]]) <p>For each output, there is a bias, so set them all to zero:</p> In\u00a0[20]: Copied! <pre>conv4.state_dict()['bias'][:]=torch.tensor([0.0,0.0,0.0])\n</pre> conv4.state_dict()['bias'][:]=torch.tensor([0.0,0.0,0.0]) <p>Create a two-channel image and plot the results:</p> In\u00a0[21]: Copied! <pre>image4=torch.zeros(1,2,5,5)\nimage4[0][0]=torch.ones(5,5)\nimage4[0][1][2][2]=1\nfor channel,image in enumerate(image4[0]):\n    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n    print(image)\n    plt.title(\"channel {}\".format(channel))\n    plt.colorbar()\n    plt.show()\n</pre> image4=torch.zeros(1,2,5,5) image4[0][0]=torch.ones(5,5) image4[0][1][2][2]=1 for channel,image in enumerate(image4[0]):     plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)     print(image)     plt.title(\"channel {}\".format(channel))     plt.colorbar()     plt.show() <pre>tensor([[1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.]])\n</pre> <pre>tensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n</pre> <p>Perform the convolution:</p> In\u00a0[22]: Copied! <pre>z=conv4(image4)\nz\n</pre> z=conv4(image4) z Out[22]: <pre>tensor([[[[ 0.5000,  0.5000,  0.5000],\n          [ 0.5000,  1.0000,  0.5000],\n          [ 0.5000,  0.5000,  0.5000]],\n\n         [[ 1.0000,  1.0000,  1.0000],\n          [ 1.0000,  0.0000,  1.0000],\n          [ 1.0000,  1.0000,  1.0000]],\n\n         [[-1.0000, -2.0000, -1.0000],\n          [ 0.0000,  0.0000,  0.0000],\n          [ 1.0000,  2.0000,  1.0000]]]], grad_fn=&lt;ConvolutionBackward0&gt;)</pre> <p>The output of the first channel is given by:</p> <p>The output of the second channel is given by:</p> <p>The output of the third channel is given by:</p> <p></p> Practice Questions  <p>Use the following two convolution objects to produce the same result as two input channel convolution on imageA and imageB as shown in the following image:</p> In\u00a0[23]: Copied! <pre>imageA=torch.zeros(1,1,5,5)\nimageB=torch.zeros(1,1,5,5)\nimageA[0,0,2,:]=-2\nimageB[0,0,2,:]=1\n\n\nconv5 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\nconv6 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)\n\n\nGx1=torch.tensor([[0.0,0.0,0.0],[0,1.0,0],[0.0,0.0,0.0]])\nconv5.state_dict()['weight'][0][0]=1*Gx1\nconv6.state_dict()['weight'][0][0]=-2*Gx1\nconv5.state_dict()['bias'][:]=torch.tensor([0.0])\nconv6.state_dict()['bias'][:]=torch.tensor([0.0])\n</pre> imageA=torch.zeros(1,1,5,5) imageB=torch.zeros(1,1,5,5) imageA[0,0,2,:]=-2 imageB[0,0,2,:]=1   conv5 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3) conv6 = nn.Conv2d(in_channels=1, out_channels=1,kernel_size=3)   Gx1=torch.tensor([[0.0,0.0,0.0],[0,1.0,0],[0.0,0.0,0.0]]) conv5.state_dict()['weight'][0][0]=1*Gx1 conv6.state_dict()['weight'][0][0]=-2*Gx1 conv5.state_dict()['bias'][:]=torch.tensor([0.0]) conv6.state_dict()['bias'][:]=torch.tensor([0.0]) <p>Double-click here for the solution.</p> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week9-CNNs/9.3Multiple_Channel_Convolution/#author-juma-shafara-date-2024-08-12-title-multiple-channel-convolutional-network-keywords-training-two-parameter-mini-batch-gradient-decent-training-two-parameter-mini-batch-gradient-decent-description-in-this-lab-you-will-learn-two-important-components-in-building-a-convolutional-neural-network","title":"author: Juma Shafara date: \"2024-08-12\" title: Multiple Channel Convolutional Network keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, you will learn two important components in building a convolutional neural network.\u00b6","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.3Multiple_Channel_Convolution/#table-of-contents","title":"Table of Contents\u00b6","text":"<p>In this lab, you will study convolution and review how the different operations change the relationship between input and output.</p> <li>Multiple Output Channels </li> <li>Multiple Inputs</li> <li>Multiple Input and Multiple Output Channels </li> <li>Practice Questions </li> <p></p> Estimated Time Needed: 25 min"},{"location":"Deep%20Learning/Week9-CNNs/9.4.1ConvolutionalNeralNetworkSimple%20example/","title":"Simple CNN","text":"Objective for this Notebook  1. Learn Convolutional Neral Network  2. Define Softmax , Criterion function, Optimizer and Train the  Model  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p></p> Helper functions  In\u00a0[1]: Copied! <pre>import torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport matplotlib.pylab as plt\nimport numpy as np\nimport pandas as pd\n</pre> import torch  import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets import matplotlib.pylab as plt import numpy as np import pandas as pd In\u00a0[2]: Copied! <pre>torch.manual_seed(4)\n</pre> torch.manual_seed(4) Out[2]: <pre>&lt;torch._C.Generator at 0x70243022eeb0&gt;</pre> <p>function to plot out the parameters of the Convolutional layers</p> In\u00a0[3]: Copied! <pre>def plot_channels(W):\n    #number of output channels \n    n_out=W.shape[0]\n    #number of input channels \n    n_in=W.shape[1]\n    w_min=W.min().item()\n    w_max=W.max().item()\n    fig, axes = plt.subplots(n_out,n_in)\n    fig.subplots_adjust(hspace = 0.1)\n    out_index=0\n    in_index=0\n    #plot outputs as rows inputs as columns \n    for ax in axes.flat:\n    \n        if in_index&gt;n_in-1:\n            out_index=out_index+1\n            in_index=0\n              \n        ax.imshow(W[out_index,in_index,:,:], vmin=w_min, vmax=w_max, cmap='seismic')\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n        in_index=in_index+1\n\n    plt.show()\n</pre> def plot_channels(W):     #number of output channels      n_out=W.shape[0]     #number of input channels      n_in=W.shape[1]     w_min=W.min().item()     w_max=W.max().item()     fig, axes = plt.subplots(n_out,n_in)     fig.subplots_adjust(hspace = 0.1)     out_index=0     in_index=0     #plot outputs as rows inputs as columns      for ax in axes.flat:              if in_index&gt;n_in-1:             out_index=out_index+1             in_index=0                        ax.imshow(W[out_index,in_index,:,:], vmin=w_min, vmax=w_max, cmap='seismic')         ax.set_yticklabels([])         ax.set_xticklabels([])         in_index=in_index+1      plt.show() <p><code>show_data</code>: plot out data sample</p> In\u00a0[4]: Copied! <pre>def show_data(dataset,sample):\n\n    plt.imshow(dataset.x[sample,0,:,:].numpy(),cmap='gray')\n    plt.title('y='+str(dataset.y[sample].item()))\n    plt.show()\n</pre> def show_data(dataset,sample):      plt.imshow(dataset.x[sample,0,:,:].numpy(),cmap='gray')     plt.title('y='+str(dataset.y[sample].item()))     plt.show() <p>create some toy data</p> In\u00a0[5]: Copied! <pre>from torch.utils.data import Dataset, DataLoader\nclass Data(Dataset):\n    def __init__(self,N_images=100,offset=0,p=0.9, train=False):\n        \"\"\"\n        p:portability that pixel is wight  \n        N_images:number of images \n        offset:set a random vertical and horizontal offset images by a sample should be less than 3 \n        \"\"\"\n        if train==True:\n            np.random.seed(1)  \n        \n        #make images multiple of 3 \n        N_images=2*(N_images//2)\n        images=np.zeros((N_images,1,11,11))\n        start1=3\n        start2=1\n        self.y=torch.zeros(N_images).type(torch.long)\n\n        for n in range(N_images):\n            if offset&gt;0:\n        \n                low=int(np.random.randint(low=start1, high=start1+offset, size=1))\n                high=int(np.random.randint(low=start2, high=start2+offset, size=1))\n            else:\n                low=4\n                high=1\n        \n            if n&lt;=N_images//2:\n                self.y[n]=0\n                images[n,0,high:high+9,low:low+3]= np.random.binomial(1, p, (9,3))\n            elif  n&gt;N_images//2:\n                self.y[n]=1\n                images[n,0,low:low+3,high:high+9] = np.random.binomial(1, p, (3,9))\n           \n        \n        \n        self.x=torch.from_numpy(images).type(torch.FloatTensor)\n        self.len=self.x.shape[0]\n        del(images)\n        np.random.seed(0)\n    def __getitem__(self,index):      \n        return self.x[index],self.y[index]\n    def __len__(self):\n        return self.len\n</pre> from torch.utils.data import Dataset, DataLoader class Data(Dataset):     def __init__(self,N_images=100,offset=0,p=0.9, train=False):         \"\"\"         p:portability that pixel is wight           N_images:number of images          offset:set a random vertical and horizontal offset images by a sample should be less than 3          \"\"\"         if train==True:             np.random.seed(1)                    #make images multiple of 3          N_images=2*(N_images//2)         images=np.zeros((N_images,1,11,11))         start1=3         start2=1         self.y=torch.zeros(N_images).type(torch.long)          for n in range(N_images):             if offset&gt;0:                          low=int(np.random.randint(low=start1, high=start1+offset, size=1))                 high=int(np.random.randint(low=start2, high=start2+offset, size=1))             else:                 low=4                 high=1                      if n&lt;=N_images//2:                 self.y[n]=0                 images[n,0,high:high+9,low:low+3]= np.random.binomial(1, p, (9,3))             elif  n&gt;N_images//2:                 self.y[n]=1                 images[n,0,low:low+3,high:high+9] = np.random.binomial(1, p, (3,9))                                       self.x=torch.from_numpy(images).type(torch.FloatTensor)         self.len=self.x.shape[0]         del(images)         np.random.seed(0)     def __getitem__(self,index):               return self.x[index],self.y[index]     def __len__(self):         return self.len <p><code>plot_activation</code>: plot out the activations of the Convolutional layers</p> In\u00a0[6]: Copied! <pre>def plot_activations(A,number_rows= 1,name=\"\"):\n    A=A[0,:,:,:].detach().numpy()\n    n_activations=A.shape[0]\n    \n    \n    print(n_activations)\n    A_min=A.min().item()\n    A_max=A.max().item()\n\n    if n_activations==1:\n\n        # Plot the image.\n        plt.imshow(A[0,:], vmin=A_min, vmax=A_max, cmap='seismic')\n\n    else:\n        fig, axes = plt.subplots(number_rows, n_activations//number_rows)\n        fig.subplots_adjust(hspace = 0.4)\n        for i,ax in enumerate(axes.flat):\n            if i&lt; n_activations:\n                # Set the label for the sub-plot.\n                ax.set_xlabel( \"activation:{0}\".format(i+1))\n\n                # Plot the image.\n                ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')\n                ax.set_xticks([])\n                ax.set_yticks([])\n    plt.show()\n</pre> def plot_activations(A,number_rows= 1,name=\"\"):     A=A[0,:,:,:].detach().numpy()     n_activations=A.shape[0]               print(n_activations)     A_min=A.min().item()     A_max=A.max().item()      if n_activations==1:          # Plot the image.         plt.imshow(A[0,:], vmin=A_min, vmax=A_max, cmap='seismic')      else:         fig, axes = plt.subplots(number_rows, n_activations//number_rows)         fig.subplots_adjust(hspace = 0.4)         for i,ax in enumerate(axes.flat):             if i&lt; n_activations:                 # Set the label for the sub-plot.                 ax.set_xlabel( \"activation:{0}\".format(i+1))                  # Plot the image.                 ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')                 ax.set_xticks([])                 ax.set_yticks([])     plt.show() <p>Utility function for computing output of convolutions takes a tuple of (h,w) and returns a tuple of (h,w)</p> In\u00a0[7]: Copied! <pre>def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n    #by Duane Nielsen\n    from math import floor\n    if type(kernel_size) is not tuple:\n        kernel_size = (kernel_size, kernel_size)\n    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n    return h, w\n</pre>  def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):     #by Duane Nielsen     from math import floor     if type(kernel_size) is not tuple:         kernel_size = (kernel_size, kernel_size)     h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)     w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)     return h, w <p></p> Prepare Data  <p>Load the training dataset with 10000 samples</p> In\u00a0[8]: Copied! <pre>N_images=10000\ntrain_dataset=Data(N_images=N_images)\n</pre> N_images=10000 train_dataset=Data(N_images=N_images) <p>Load the testing dataset</p> In\u00a0[9]: Copied! <pre>validation_dataset=Data(N_images=1000,train=False)\nvalidation_dataset\n</pre> validation_dataset=Data(N_images=1000,train=False) validation_dataset Out[9]: <pre>&lt;__main__.Data at 0x7023c545ee10&gt;</pre> <p>we can see the data type is long</p> <p>Each element in the rectangular  tensor corresponds to a number representing a pixel intensity  as demonstrated by  the following image.</p> <p>We can print out the third label</p> In\u00a0[10]: Copied! <pre>show_data(train_dataset,0)\n</pre> show_data(train_dataset,0) In\u00a0[11]: Copied! <pre>show_data(train_dataset,N_images//2+2)\n</pre> show_data(train_dataset,N_images//2+2) <p>we can plot the 3rd  sample</p> <p></p> <p>The input image is 11 x11, the following will change the size of the activations:</p> <ul> convolutional layer </ul> <ul> max pooling layer </ul> <ul> convolutional layer  </ul> <ul> max pooling layer  </ul> <p>with the following parameters <code>kernel_size</code>, <code>stride</code> and <code> pad</code>. We use the following  lines of code to change the image before we get tot he fully connected layer</p> In\u00a0[12]: Copied! <pre>out=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out)\nout1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out1)\nout2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out2)\n\nout3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out3)\n</pre> out=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1) print(out) out1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1) print(out1) out2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1) print(out2)  out3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1) print(out3) <pre>(10, 10)\n(9, 9)\n(8, 8)\n(7, 7)\n</pre> <p>Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.</p> In\u00a0[13]: Copied! <pre>class CNN(nn.Module):\n    def __init__(self,out_1=2,out_2=1):\n        \n        super(CNN,self).__init__()\n        #first Convolutional layers \n        self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)\n        self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1)\n\n        #second Convolutional layers\n        self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)\n        self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1)\n        #max pooling \n\n        #fully connected layer \n        self.fc1=nn.Linear(out_2*7*7,2)\n        \n    def forward(self,x):\n        #first Convolutional layers\n        x=self.cnn1(x)\n        #activation function \n        x=torch.relu(x)\n        #max pooling \n        x=self.maxpool1(x)\n        #first Convolutional layers\n        x=self.cnn2(x)\n        #activation function\n        x=torch.relu(x)\n        #max pooling\n        x=self.maxpool2(x)\n        #flatten output \n        x=x.view(x.size(0),-1)\n        #fully connected layer\n        x=self.fc1(x)\n        return x\n    \n    def activations(self,x):\n        #outputs activation this is not necessary just for fun \n        z1=self.cnn1(x)\n        a1=torch.relu(z1)\n        out=self.maxpool1(a1)\n        \n        z2=self.cnn2(out)\n        a2=torch.relu(z2)\n        out=self.maxpool2(a2)\n        out=out.view(out.size(0),-1)\n        return z1,a1,z2,a2,out        \n</pre> class CNN(nn.Module):     def __init__(self,out_1=2,out_2=1):                  super(CNN,self).__init__()         #first Convolutional layers          self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)         self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1)          #second Convolutional layers         self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)         self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1)         #max pooling           #fully connected layer          self.fc1=nn.Linear(out_2*7*7,2)              def forward(self,x):         #first Convolutional layers         x=self.cnn1(x)         #activation function          x=torch.relu(x)         #max pooling          x=self.maxpool1(x)         #first Convolutional layers         x=self.cnn2(x)         #activation function         x=torch.relu(x)         #max pooling         x=self.maxpool2(x)         #flatten output          x=x.view(x.size(0),-1)         #fully connected layer         x=self.fc1(x)         return x          def activations(self,x):         #outputs activation this is not necessary just for fun          z1=self.cnn1(x)         a1=torch.relu(z1)         out=self.maxpool1(a1)                  z2=self.cnn2(out)         a2=torch.relu(z2)         out=self.maxpool2(a2)         out=out.view(out.size(0),-1)         return z1,a1,z2,a2,out         <p></p>  Define the Convolutional Neral Network Classifier , Criterion function, Optimizer and Train the  Model   <p>There are 2 output channels for the first layer, and 1 outputs channel for the second layer</p> In\u00a0[14]: Copied! <pre>model=CNN(2,1)\n</pre> model=CNN(2,1) <p>we can see the model parameters with the object</p> In\u00a0[15]: Copied! <pre>model\n</pre> model Out[15]: <pre>CNN(\n  (cnn1): Conv2d(1, 2, kernel_size=(2, 2), stride=(1, 1))\n  (maxpool1): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n  (cnn2): Conv2d(2, 1, kernel_size=(2, 2), stride=(1, 1))\n  (maxpool2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=49, out_features=2, bias=True)\n)</pre> <p>Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly.</p> In\u00a0[16]: Copied! <pre>plot_channels(model.state_dict()['cnn1.weight'])\n</pre>  plot_channels(model.state_dict()['cnn1.weight'])  <p>Loss function</p> In\u00a0[17]: Copied! <pre>plot_channels(model.state_dict()['cnn2.weight'])\n</pre> plot_channels(model.state_dict()['cnn2.weight']) <p>Define the loss function</p> In\u00a0[18]: Copied! <pre>criterion=nn.CrossEntropyLoss()\n</pre> criterion=nn.CrossEntropyLoss() <p>optimizer class</p> In\u00a0[19]: Copied! <pre>learning_rate=0.001\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n</pre> learning_rate=0.001  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) <p>Define the optimizer class</p> In\u00a0[20]: Copied! <pre>train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=10)\nvalidation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=20)\n</pre>  train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=10) validation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=20) <p>Train the model and determine validation accuracy technically test accuracy (This may take a long time)</p> In\u00a0[21]: Copied! <pre>n_epochs=10\ncost_list=[]\naccuracy_list=[]\nN_test=len(validation_dataset)\ncost=0\n#n_epochs\nfor epoch in range(n_epochs):\n    cost=0    \n    for x, y in train_loader:\n      \n\n        #clear gradient \n        optimizer.zero_grad()\n        #make a prediction \n        z=model(x)\n        # calculate loss \n        loss=criterion(z,y)\n        # calculate gradients of parameters \n        loss.backward()\n        # update parameters \n        optimizer.step()\n        cost+=loss.item()\n    cost_list.append(cost)\n        \n        \n    correct=0\n    #perform a prediction on the validation  data  \n    for x_test, y_test in validation_loader:\n\n        z=model(x_test)\n        _,yhat=torch.max(z.data,1)\n\n        correct+=(yhat==y_test).sum().item()\n        \n\n    accuracy=correct/N_test\n\n    accuracy_list.append(accuracy)\n    \n</pre> n_epochs=10 cost_list=[] accuracy_list=[] N_test=len(validation_dataset) cost=0 #n_epochs for epoch in range(n_epochs):     cost=0         for x, y in train_loader:                 #clear gradient          optimizer.zero_grad()         #make a prediction          z=model(x)         # calculate loss          loss=criterion(z,y)         # calculate gradients of parameters          loss.backward()         # update parameters          optimizer.step()         cost+=loss.item()     cost_list.append(cost)                       correct=0     #perform a prediction on the validation  data       for x_test, y_test in validation_loader:          z=model(x_test)         _,yhat=torch.max(z.data,1)          correct+=(yhat==y_test).sum().item()               accuracy=correct/N_test      accuracy_list.append(accuracy)        <p>Plot the loss and accuracy on the validation data:</p> In\u00a0[22]: Copied! <pre>fig, ax1 = plt.subplots()\ncolor = 'tab:red'\nax1.plot(cost_list,color=color)\nax1.set_xlabel('epoch',color=color)\nax1.set_ylabel('total loss',color=color)\nax1.tick_params(axis='y', color=color)\n    \nax2 = ax1.twinx()  \ncolor = 'tab:blue'\nax2.set_ylabel('accuracy', color=color)  \nax2.plot( accuracy_list, color=color)\nax2.tick_params(axis='y', labelcolor=color)\nfig.tight_layout()\n</pre> fig, ax1 = plt.subplots() color = 'tab:red' ax1.plot(cost_list,color=color) ax1.set_xlabel('epoch',color=color) ax1.set_ylabel('total loss',color=color) ax1.tick_params(axis='y', color=color)      ax2 = ax1.twinx()   color = 'tab:blue' ax2.set_ylabel('accuracy', color=color)   ax2.plot( accuracy_list, color=color) ax2.tick_params(axis='y', labelcolor=color) fig.tight_layout() <p>View the results of the parameters for the Convolutional layers</p> In\u00a0[23]: Copied! <pre>model.state_dict()['cnn1.weight']\n</pre> model.state_dict()['cnn1.weight'] Out[23]: <pre>tensor([[[[ 0.3507,  0.4734],\n          [-0.1160, -0.1536]]],\n\n\n        [[[-0.4187, -0.2707],\n          [ 0.9412,  0.8749]]]])</pre> In\u00a0[24]: Copied! <pre>plot_channels(model.state_dict()['cnn1.weight'])\n</pre> plot_channels(model.state_dict()['cnn1.weight']) In\u00a0[25]: Copied! <pre>model.state_dict()['cnn1.weight']\n</pre> model.state_dict()['cnn1.weight'] Out[25]: <pre>tensor([[[[ 0.3507,  0.4734],\n          [-0.1160, -0.1536]]],\n\n\n        [[[-0.4187, -0.2707],\n          [ 0.9412,  0.8749]]]])</pre> In\u00a0[26]: Copied! <pre>plot_channels(model.state_dict()['cnn2.weight'])\n</pre> plot_channels(model.state_dict()['cnn2.weight']) <p>Consider the following sample</p> In\u00a0[27]: Copied! <pre>show_data(train_dataset,N_images//2+2)\n</pre> show_data(train_dataset,N_images//2+2) <p>Determine the activations</p> In\u00a0[28]: Copied! <pre>out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))\nout=model.activations(train_dataset[0][0].view(1,1,11,11))\n</pre> out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11)) out=model.activations(train_dataset[0][0].view(1,1,11,11)) <p>Plot them out</p> In\u00a0[29]: Copied! <pre>plot_activations(out[0],number_rows=1,name=\" feature map\")\nplt.show()\n</pre> plot_activations(out[0],number_rows=1,name=\" feature map\") plt.show()  <pre>2\n</pre> In\u00a0[30]: Copied! <pre>plot_activations(out[2],number_rows=1,name=\"2nd feature map\")\nplt.show()\n</pre> plot_activations(out[2],number_rows=1,name=\"2nd feature map\") plt.show() <pre>1\n</pre> In\u00a0[31]: Copied! <pre>plot_activations(out[3],number_rows=1,name=\"first feature map\")\nplt.show()\n</pre> plot_activations(out[3],number_rows=1,name=\"first feature map\") plt.show() <pre>1\n</pre> <p>we save the output of the activation after flattening</p> In\u00a0[32]: Copied! <pre>out1=out[4][0].detach().numpy()\n</pre> out1=out[4][0].detach().numpy() <p>we can do the same for a sample  where y=0</p> In\u00a0[33]: Copied! <pre>out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy()\nout0\n</pre> out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy() out0 Out[33]: <pre>array([0.7374982 , 1.7757462 , 2.398145  , 2.4768693 , 2.4768693 ,\n       2.1022153 , 1.0242625 , 0.6254372 , 1.4152323 , 1.9039373 ,\n       2.0423164 , 2.0423164 , 1.8148925 , 1.0581893 , 0.6254372 ,\n       1.4152323 , 1.9821635 , 2.1456885 , 2.1456885 , 1.8400384 ,\n       1.0581893 , 0.67411214, 1.6115171 , 2.1684833 , 2.1684833 ,\n       2.1456885 , 1.8400384 , 0.96484905, 0.7374982 , 1.6366628 ,\n       2.1684833 , 2.1684833 , 2.1105773 , 1.618608  , 0.95454437,\n       0.7374982 , 1.6366628 , 2.0902567 , 2.0902567 , 2.0072055 ,\n       1.8148925 , 1.0581893 , 0.6254372 , 1.4422549 , 2.0730698 ,\n       2.178489  , 2.178489  , 1.99857   , 1.0581893 ], dtype=float32)</pre> In\u00a0[34]: Copied! <pre>plt.subplot(2, 1, 1)\nplt.plot( out1, 'b')\nplt.title('Flatted Activation Values  ')\nplt.ylabel('Activation')\nplt.xlabel('index')\nplt.subplot(2, 1, 2)\nplt.plot(out0, 'r')\nplt.xlabel('index')\nplt.ylabel('Activation')\n</pre> plt.subplot(2, 1, 1) plt.plot( out1, 'b') plt.title('Flatted Activation Values  ') plt.ylabel('Activation') plt.xlabel('index') plt.subplot(2, 1, 2) plt.plot(out0, 'r') plt.xlabel('index') plt.ylabel('Activation') Out[34]: <pre>Text(0, 0.5, 'Activation')</pre> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week9-CNNs/9.4.1ConvolutionalNeralNetworkSimple%20example/#author-juma-shafara-date-2024-08-12-title-simple-convolutional-neural-network-keywords-training-two-parameter-mini-batch-gradient-decent-training-two-parameter-mini-batch-gradient-decent-description-in-this-lab-we-will-use-a-convolutional-neral-networks-to-classify-horizontal-an-vertical-lines","title":"author: Juma Shafara date: \"2024-08-12\" title: Simple Convolutional Neural Network keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, we will use a Convolutional Neral Networks to classify horizontal an vertical Lines\u00b6","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.1ConvolutionalNeralNetworkSimple%20example/#table-of-contents","title":"Table of Contents\u00b6","text":"<p>In this lab, we will use a Convolutional Neral Networks to classify horizontal an vertical Lines</p> <li>Helper functions </li> <li> Prepare Data </li> <li>Convolutional Neral Network </li> <li>Define Softmax , Criterion function, Optimizer and Train the  Model</li> <li>Analyse Results</li> <p></p> Estimated Time Needed: 25 min"},{"location":"Deep%20Learning/Week9-CNNs/9.4.1ConvolutionalNeralNetworkSimple%20example/#data-visualization","title":"Data Visualization\u00b6","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.1ConvolutionalNeralNetworkSimple%20example/#build-a-convolutional-neral-network-class","title":"Build a Convolutional Neral Network Class\u00b6","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.1ConvolutionalNeralNetworkSimple%20example/","title":"\u00b6","text":"Analyse Results"},{"location":"Deep%20Learning/Week9-CNNs/9.4.2CNN_Small_Image/","title":"CNN Small Image","text":"Objective for this Notebook  1. Learn how to use a Convolutional Neural Network to classify handwritten digits from the MNIST database  2. Learn hot to reshape the images to make them faster to process  Table of Contents <p>In this lab, we will use a Convolutional Neural Network to classify handwritten digits from the MNIST database. We will reshape the images to make them faster to process </p> <ul> <li>Get Some Data</li> <li>Convolutional Neural Network</li> <li>Define Softmax, Criterion function, Optimizer and Train the Model</li> <li>Analyze Results</li> </ul> <p>Estimated Time Needed: 25 min 14 min to train model </p>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> Preparation In\u00a0[1]: Copied! <pre># Import the libraries we need to use in this lab\n\n# Using the following line code to install the torchvision library\n# !mamba install -y torchvision\n\n!pip install torchvision==0.9.1 torch==1.8.1 \nimport torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport matplotlib.pylab as plt\nimport numpy as np\n</pre> # Import the libraries we need to use in this lab  # Using the following line code to install the torchvision library # !mamba install -y torchvision  !pip install torchvision==0.9.1 torch==1.8.1  import torch  import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets import matplotlib.pylab as plt import numpy as np <pre>ERROR: Ignored the following yanked versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3\nERROR: Could not find a version that satisfies the requirement torchvision==0.9.1 (from versions: 0.17.0, 0.17.1, 0.17.2, 0.18.0, 0.18.1, 0.19.0)\nERROR: No matching distribution found for torchvision==0.9.1\n</pre> <p>Define the function <code>plot_channels</code> to plot out the kernel parameters of  each channel</p> In\u00a0[2]: Copied! <pre># Define the function for plotting the channels\n\ndef plot_channels(W):\n    n_out = W.shape[0]\n    n_in = W.shape[1]\n    w_min = W.min().item()\n    w_max = W.max().item()\n    fig, axes = plt.subplots(n_out, n_in)\n    fig.subplots_adjust(hspace=0.1)\n    out_index = 0\n    in_index = 0\n    \n    #plot outputs as rows inputs as columns \n    for ax in axes.flat:\n        if in_index &gt; n_in-1:\n            out_index = out_index + 1\n            in_index = 0\n        ax.imshow(W[out_index, in_index, :, :], vmin=w_min, vmax=w_max, cmap='seismic')\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n        in_index = in_index + 1\n\n    plt.show()\n</pre> # Define the function for plotting the channels  def plot_channels(W):     n_out = W.shape[0]     n_in = W.shape[1]     w_min = W.min().item()     w_max = W.max().item()     fig, axes = plt.subplots(n_out, n_in)     fig.subplots_adjust(hspace=0.1)     out_index = 0     in_index = 0          #plot outputs as rows inputs as columns      for ax in axes.flat:         if in_index &gt; n_in-1:             out_index = out_index + 1             in_index = 0         ax.imshow(W[out_index, in_index, :, :], vmin=w_min, vmax=w_max, cmap='seismic')         ax.set_yticklabels([])         ax.set_xticklabels([])         in_index = in_index + 1      plt.show() <p>Define the function <code>plot_parameters</code> to plot out the kernel parameters of each channel with Multiple outputs .</p> In\u00a0[3]: Copied! <pre># Define the function for plotting the parameters\n\ndef plot_parameters(W, number_rows=1, name=\"\", i=0):\n    W = W.data[:, i, :, :]\n    n_filters = W.shape[0]\n    w_min = W.min().item()\n    w_max = W.max().item()\n    fig, axes = plt.subplots(number_rows, n_filters // number_rows)\n    fig.subplots_adjust(hspace=0.4)\n\n    for i, ax in enumerate(axes.flat):\n        if i &lt; n_filters:\n            # Set the label for the sub-plot.\n            ax.set_xlabel(\"kernel:{0}\".format(i + 1))\n\n            # Plot the image.\n            ax.imshow(W[i, :], vmin=w_min, vmax=w_max, cmap='seismic')\n            ax.set_xticks([])\n            ax.set_yticks([])\n    plt.suptitle(name, fontsize=10)    \n    plt.show()\n</pre> # Define the function for plotting the parameters  def plot_parameters(W, number_rows=1, name=\"\", i=0):     W = W.data[:, i, :, :]     n_filters = W.shape[0]     w_min = W.min().item()     w_max = W.max().item()     fig, axes = plt.subplots(number_rows, n_filters // number_rows)     fig.subplots_adjust(hspace=0.4)      for i, ax in enumerate(axes.flat):         if i &lt; n_filters:             # Set the label for the sub-plot.             ax.set_xlabel(\"kernel:{0}\".format(i + 1))              # Plot the image.             ax.imshow(W[i, :], vmin=w_min, vmax=w_max, cmap='seismic')             ax.set_xticks([])             ax.set_yticks([])     plt.suptitle(name, fontsize=10)         plt.show() <p>Define the function <code>plot_activation</code> to plot out the activations of the Convolutional layers</p> In\u00a0[4]: Copied! <pre># Define the function for plotting the activations\n\ndef plot_activations(A, number_rows=1, name=\"\", i=0):\n    A = A[0, :, :, :].detach().numpy()\n    n_activations = A.shape[0]\n    A_min = A.min().item()\n    A_max = A.max().item()\n    fig, axes = plt.subplots(number_rows, n_activations // number_rows)\n    fig.subplots_adjust(hspace = 0.4)\n\n    for i, ax in enumerate(axes.flat):\n        if i &lt; n_activations:\n            # Set the label for the sub-plot.\n            ax.set_xlabel(\"activation:{0}\".format(i + 1))\n\n            # Plot the image.\n            ax.imshow(A[i, :], vmin=A_min, vmax=A_max, cmap='seismic')\n            ax.set_xticks([])\n            ax.set_yticks([])\n    plt.show()\n</pre> # Define the function for plotting the activations  def plot_activations(A, number_rows=1, name=\"\", i=0):     A = A[0, :, :, :].detach().numpy()     n_activations = A.shape[0]     A_min = A.min().item()     A_max = A.max().item()     fig, axes = plt.subplots(number_rows, n_activations // number_rows)     fig.subplots_adjust(hspace = 0.4)      for i, ax in enumerate(axes.flat):         if i &lt; n_activations:             # Set the label for the sub-plot.             ax.set_xlabel(\"activation:{0}\".format(i + 1))              # Plot the image.             ax.imshow(A[i, :], vmin=A_min, vmax=A_max, cmap='seismic')             ax.set_xticks([])             ax.set_yticks([])     plt.show() <p>Define the function <code>show_data</code> to plot out data samples as images.</p> In\u00a0[5]: Copied! <pre>def show_data(data_sample):\n    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n    plt.title('y = '+ str(data_sample[1]))\n</pre> def show_data(data_sample):     plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')     plt.title('y = '+ str(data_sample[1])) <p>we create a transform to resize the image and convert it to a tensor .</p> In\u00a0[7]: Copied! <pre>IMAGE_SIZE = 16\n\ncomposed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])\n</pre> IMAGE_SIZE = 16  composed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()]) <p>Load the training dataset by setting the parameters <code>train </code> to <code>True</code>. We use the transform defined above.</p> In\u00a0[8]: Copied! <pre>train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=composed)\n</pre>  train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=composed) <pre>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n</pre> <pre>100.0%\n</pre> <pre>Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n</pre> <pre>100.0%\n</pre> <pre>Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n</pre> <pre>100.0%\n</pre> <pre>Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n</pre> <pre>100.0%</pre> <pre>Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n</pre> <pre>\n</pre> <p>Load the testing dataset by setting the parameters train  <code>False</code>.</p> In\u00a0[9]: Copied! <pre># Make the validating \n\nvalidation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=composed)\n</pre> # Make the validating   validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=composed) <p>We can see the data type is long.</p> In\u00a0[10]: Copied! <pre># Show the data type for each element in dataset\n\ntype(train_dataset[0][1])\n</pre> # Show the data type for each element in dataset  type(train_dataset[0][1]) Out[10]: <pre>int</pre> <p>Each element in the rectangular tensor corresponds to a number representing a pixel intensity as demonstrated by the following image.</p> <p>Print out the fourth label</p> In\u00a0[11]: Copied! <pre># The label for the fourth data element\n\ntrain_dataset[3][1]\n</pre> # The label for the fourth data element  train_dataset[3][1] Out[11]: <pre>1</pre> <p>Plot the fourth sample</p> In\u00a0[12]: Copied! <pre># The image for the fourth data element\nshow_data(train_dataset[3])\n</pre> # The image for the fourth data element show_data(train_dataset[3])  <p>The fourth sample is a \"1\".</p> <p>Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.</p> In\u00a0[13]: Copied! <pre>class CNN(nn.Module):\n    \n    # Contructor\n    def __init__(self, out_1=16, out_2=32):\n        super(CNN, self).__init__()\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n\n        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n        self.fc1 = nn.Linear(out_2 * 4 * 4, 10)\n    \n    # Prediction\n    def forward(self, x):\n        x = self.cnn1(x)\n        x = torch.relu(x)\n        x = self.maxpool1(x)\n        x = self.cnn2(x)\n        x = torch.relu(x)\n        x = self.maxpool2(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        return x\n    \n    # Outputs in each steps\n    def activations(self, x):\n        #outputs activation this is not necessary\n        z1 = self.cnn1(x)\n        a1 = torch.relu(z1)\n        out = self.maxpool1(a1)\n        \n        z2 = self.cnn2(out)\n        a2 = torch.relu(z2)\n        out1 = self.maxpool2(a2)\n        out = out.view(out.size(0),-1)\n        return z1, a1, z2, a2, out1,out\n</pre> class CNN(nn.Module):          # Contructor     def __init__(self, out_1=16, out_2=32):         super(CNN, self).__init__()         self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)         self.maxpool1=nn.MaxPool2d(kernel_size=2)          self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)         self.maxpool2=nn.MaxPool2d(kernel_size=2)         self.fc1 = nn.Linear(out_2 * 4 * 4, 10)          # Prediction     def forward(self, x):         x = self.cnn1(x)         x = torch.relu(x)         x = self.maxpool1(x)         x = self.cnn2(x)         x = torch.relu(x)         x = self.maxpool2(x)         x = x.view(x.size(0), -1)         x = self.fc1(x)         return x          # Outputs in each steps     def activations(self, x):         #outputs activation this is not necessary         z1 = self.cnn1(x)         a1 = torch.relu(z1)         out = self.maxpool1(a1)                  z2 = self.cnn2(out)         a2 = torch.relu(z2)         out1 = self.maxpool2(a2)         out = out.view(out.size(0),-1)         return z1, a1, z2, a2, out1,out <p>There are 16 output channels for the first layer, and 32 output channels for the second layer</p> In\u00a0[14]: Copied! <pre># Create the model object using CNN class\n\nmodel = CNN(out_1=16, out_2=32)\n</pre> # Create the model object using CNN class  model = CNN(out_1=16, out_2=32) <p>Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly.</p> In\u00a0[15]: Copied! <pre># Plot the parameters\n\nplot_parameters(model.state_dict()['cnn1.weight'], number_rows=4, name=\"1st layer kernels before training \")\nplot_parameters(model.state_dict()['cnn2.weight'], number_rows=4, name='2nd layer kernels before training' )\n</pre> # Plot the parameters  plot_parameters(model.state_dict()['cnn1.weight'], number_rows=4, name=\"1st layer kernels before training \") plot_parameters(model.state_dict()['cnn2.weight'], number_rows=4, name='2nd layer kernels before training' ) <p>Define the loss function, the optimizer and the dataset loader</p> In\u00a0[16]: Copied! <pre>criterion = nn.CrossEntropyLoss()\nlearning_rate = 0.1\noptimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)\n</pre> criterion = nn.CrossEntropyLoss() learning_rate = 0.1 optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100) validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000) <p>Train the model and determine validation accuracy technically test accuracy (This may take a long time)</p> In\u00a0[17]: Copied! <pre># Train the model\n\nn_epochs=3\ncost_list=[]\naccuracy_list=[]\nN_test=len(validation_dataset)\nCOST=0\n\ndef train_model(n_epochs):\n    for epoch in range(n_epochs):\n        COST=0\n        for x, y in train_loader:\n            optimizer.zero_grad()\n            z = model(x)\n            loss = criterion(z, y)\n            loss.backward()\n            optimizer.step()\n            COST+=loss.data\n        \n        cost_list.append(COST)\n        correct=0\n        #perform a prediction on the validation  data  \n        for x_test, y_test in validation_loader:\n            z = model(x_test)\n            _, yhat = torch.max(z.data, 1)\n            correct += (yhat == y_test).sum().item()\n        accuracy = correct / N_test\n        accuracy_list.append(accuracy)\n     \ntrain_model(n_epochs)\n</pre> # Train the model  n_epochs=3 cost_list=[] accuracy_list=[] N_test=len(validation_dataset) COST=0  def train_model(n_epochs):     for epoch in range(n_epochs):         COST=0         for x, y in train_loader:             optimizer.zero_grad()             z = model(x)             loss = criterion(z, y)             loss.backward()             optimizer.step()             COST+=loss.data                  cost_list.append(COST)         correct=0         #perform a prediction on the validation  data           for x_test, y_test in validation_loader:             z = model(x_test)             _, yhat = torch.max(z.data, 1)             correct += (yhat == y_test).sum().item()         accuracy = correct / N_test         accuracy_list.append(accuracy)       train_model(n_epochs) <p>Plot the loss and accuracy on the validation data:</p> In\u00a0[18]: Copied! <pre># Plot the loss and accuracy\n\nfig, ax1 = plt.subplots()\ncolor = 'tab:red'\nax1.plot(cost_list, color=color)\nax1.set_xlabel('epoch', color=color)\nax1.set_ylabel('Cost', color=color)\nax1.tick_params(axis='y', color=color)\n    \nax2 = ax1.twinx()  \ncolor = 'tab:blue'\nax2.set_ylabel('accuracy', color=color) \nax2.set_xlabel('epoch', color=color)\nax2.plot( accuracy_list, color=color)\nax2.tick_params(axis='y', color=color)\nfig.tight_layout()\n</pre> # Plot the loss and accuracy  fig, ax1 = plt.subplots() color = 'tab:red' ax1.plot(cost_list, color=color) ax1.set_xlabel('epoch', color=color) ax1.set_ylabel('Cost', color=color) ax1.tick_params(axis='y', color=color)      ax2 = ax1.twinx()   color = 'tab:blue' ax2.set_ylabel('accuracy', color=color)  ax2.set_xlabel('epoch', color=color) ax2.plot( accuracy_list, color=color) ax2.tick_params(axis='y', color=color) fig.tight_layout() <p>View the results of the parameters for the Convolutional layers</p> In\u00a0[19]: Copied! <pre># Plot the channels\n\nplot_channels(model.state_dict()['cnn1.weight'])\nplot_channels(model.state_dict()['cnn2.weight'])\n</pre> # Plot the channels  plot_channels(model.state_dict()['cnn1.weight']) plot_channels(model.state_dict()['cnn2.weight']) <p>Consider the following sample</p> In\u00a0[20]: Copied! <pre># Show the second image\n\nshow_data(train_dataset[1])\n</pre> # Show the second image  show_data(train_dataset[1]) <p>Determine the activations</p> In\u00a0[21]: Copied! <pre># Use the CNN activations class to see the steps\n\nout = model.activations(train_dataset[1][0].view(1, 1, IMAGE_SIZE, IMAGE_SIZE))\n</pre> # Use the CNN activations class to see the steps  out = model.activations(train_dataset[1][0].view(1, 1, IMAGE_SIZE, IMAGE_SIZE)) <p>Plot out the first set of activations</p> In\u00a0[22]: Copied! <pre># Plot the outputs after the first CNN\n\nplot_activations(out[0], number_rows=4, name=\"Output after the 1st CNN\")\n</pre> # Plot the outputs after the first CNN  plot_activations(out[0], number_rows=4, name=\"Output after the 1st CNN\") <p>The image below is the result after applying the relu activation function</p> In\u00a0[23]: Copied! <pre># Plot the outputs after the first Relu\n\nplot_activations(out[1], number_rows=4, name=\"Output after the 1st Relu\")\n</pre> # Plot the outputs after the first Relu  plot_activations(out[1], number_rows=4, name=\"Output after the 1st Relu\") <p>The image below is the result of the activation map after the second output layer.</p> In\u00a0[24]: Copied! <pre># Plot the outputs after the second CNN\n\nplot_activations(out[2], number_rows=32 // 4, name=\"Output after the 2nd CNN\")\n</pre> # Plot the outputs after the second CNN  plot_activations(out[2], number_rows=32 // 4, name=\"Output after the 2nd CNN\") <p>The image below is the result of the activation map after applying the second relu</p> In\u00a0[25]: Copied! <pre># Plot the outputs after the second Relu\n\nplot_activations(out[3], number_rows=4, name=\"Output after the 2nd Relu\")\n</pre> # Plot the outputs after the second Relu  plot_activations(out[3], number_rows=4, name=\"Output after the 2nd Relu\") <p>We can  see the result for the third sample</p> In\u00a0[26]: Copied! <pre># Show the third image\n\nshow_data(train_dataset[2])\n</pre> # Show the third image  show_data(train_dataset[2]) In\u00a0[27]: Copied! <pre># Use the CNN activations class to see the steps\n\nout = model.activations(train_dataset[2][0].view(1, 1, IMAGE_SIZE, IMAGE_SIZE))\n</pre> # Use the CNN activations class to see the steps  out = model.activations(train_dataset[2][0].view(1, 1, IMAGE_SIZE, IMAGE_SIZE)) In\u00a0[28]: Copied! <pre># Plot the outputs after the first CNN\n\nplot_activations(out[0], number_rows=4, name=\"Output after the 1st CNN\")\n</pre> # Plot the outputs after the first CNN  plot_activations(out[0], number_rows=4, name=\"Output after the 1st CNN\") In\u00a0[29]: Copied! <pre># Plot the outputs after the first Relu\n\nplot_activations(out[1], number_rows=4, name=\"Output after the 1st Relu\")\n</pre> # Plot the outputs after the first Relu  plot_activations(out[1], number_rows=4, name=\"Output after the 1st Relu\") In\u00a0[30]: Copied! <pre># Plot the outputs after the second CNN\n\nplot_activations(out[2], number_rows=32 // 4, name=\"Output after the 2nd CNN\")\n</pre> # Plot the outputs after the second CNN  plot_activations(out[2], number_rows=32 // 4, name=\"Output after the 2nd CNN\") In\u00a0[31]: Copied! <pre># Plot the outputs after the second Relu\n\nplot_activations(out[3], number_rows=4, name=\"Output after the 2nd Relu\")\n</pre> # Plot the outputs after the second Relu  plot_activations(out[3], number_rows=4, name=\"Output after the 2nd Relu\") <p>Plot the first five mis-classified samples:</p> In\u00a0[32]: Copied! <pre># Plot the mis-classified samples\n\ncount = 0\nfor x, y in torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=1):\n    z = model(x)\n    _, yhat = torch.max(z, 1)\n    if yhat != y:\n        show_data((x, y))\n        plt.show()\n        print(\"yhat: \",yhat)\n        count += 1\n    if count &gt;= 5:\n        break  \n</pre> # Plot the mis-classified samples  count = 0 for x, y in torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=1):     z = model(x)     _, yhat = torch.max(z, 1)     if yhat != y:         show_data((x, y))         plt.show()         print(\"yhat: \",yhat)         count += 1     if count &gt;= 5:         break   <pre>yhat:  tensor([3])\n</pre> <pre>yhat:  tensor([5])\n</pre> <pre>yhat:  tensor([2])\n</pre> <pre>yhat:  tensor([0])\n</pre> <pre>yhat:  tensor([4])\n</pre> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week9-CNNs/9.4.2CNN_Small_Image/#author-juma-shafara-date-2024-08-12-title-convolutional-neural-network-with-small-images-keywords-training-two-parameter-mini-batch-gradient-decent-training-two-parameter-mini-batch-gradient-decent-description-in-this-lab-we-will-use-a-convolutional-neral-networks-to-classify-horizontal-an-vertical-lines","title":"author: Juma Shafara date: \"2024-08-12\" title: Convolutional Neural Network with Small Images keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, we will use a Convolutional Neral Networks to classify horizontal an vertical Lines\u00b6","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.2CNN_Small_Image/#Makeup_Data","title":"Get the Data","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.2CNN_Small_Image/#CNN","title":"Build a Convolutional Neural Network Class","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.2CNN_Small_Image/#Train","title":"Define the Convolutional Neural Network Classifier, Criterion function, Optimizer and Train the Model","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.2CNN_Small_Image/#Result","title":"Analyze Results","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.3CNN_Small_Image_batch/","title":"CNN Batch Processing","text":"Objective for this Notebook  1. Learn how to compare a Convolutional Neural Network using Batch Normalization with a regular Convolutional Neural Network  to classify handwritten digits from the MNIST database.. Table of Contents This lab takes a long time to run so the results are given. You can run the notebook your self but it may take a long time. <p>In this lab, we will compare a Convolutional Neural Network using Batch Normalization with a regular Convolutional Neural Network  to classify handwritten digits from the MNIST database. We will reshape the images to make them faster to process. </p> <ul> <li>Read me Batch Norm for Convolution Operation  </li> <li>Get Some Data</li> <li>Two Types of Convolutional Neural Network</li> <li>Define Criterion function, Optimizer and Train the Model</li> <li>Analyze Results</li> </ul> <p>Estimated Time Needed: 25 min</p>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>Like a fully connected network, we create a <code>BatchNorm2d</code> object, but we apply it to the 2D convolution object. First, we create objects <code>Conv2d</code> object; we require the number of output channels, specified by the variable <code>OUT</code>.</p> <p><code>self.cnn1 = nn.Conv2d(in_channels=1, out_channels=OUT, kernel_size=5, padding=2) </code></p> <p>We then create a Batch Norm  object for 2D convolution as follows:</p> <p><code>self.conv1_bn = nn.BatchNorm2d(OUT)</code></p> <p>The parameter out is the number of channels in the output. We can then apply batch norm  after  the convolution operation :</p> <p><code>x = self.cnn1(x)</code></p> <p></p> <code> x=self.conv1_bn(x)</code> Preparation In\u00a0[\u00a0]: Copied! <pre># Import the libraries we need to use in this lab\n\n# Using the following line code to install the torchvision library\n# !mamba install -y torchvision\n\n!pip install torchvision==0.9.1 torch==1.8.1 \nimport torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport matplotlib.pylab as plt\nimport numpy as np\ndef show_data(data_sample):\n    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n    plt.title('y = '+ str(data_sample[1]))\n</pre>  # Import the libraries we need to use in this lab  # Using the following line code to install the torchvision library # !mamba install -y torchvision  !pip install torchvision==0.9.1 torch==1.8.1  import torch  import torch.nn as nn import torchvision.transforms as transforms import torchvision.datasets as dsets import matplotlib.pylab as plt import numpy as np def show_data(data_sample):     plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')     plt.title('y = '+ str(data_sample[1])) <p>we create a transform to resize the image and convert it to a tensor :</p> In\u00a0[\u00a0]: Copied! <pre>IMAGE_SIZE = 16\n\ncomposed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])\n</pre>  IMAGE_SIZE = 16  composed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()]) <p>Load the training dataset by setting the parameters <code>train </code> to <code>True</code>. We use the transform defined above.</p> In\u00a0[\u00a0]: Copied! <pre>train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=composed)\n</pre>  train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=composed) <p>Load the testing dataset by setting the parameters train  <code>False</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Make the validating \n\nvalidation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=composed)\n</pre> # Make the validating   validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=composed) <p>We can see the data type is long.</p> In\u00a0[\u00a0]: Copied! <pre># Show the data type for each element in dataset\n\ntype(train_dataset[0][1])\n</pre> # Show the data type for each element in dataset  type(train_dataset[0][1]) <p>Each element in the rectangular tensor corresponds to a number representing a pixel intensity as demonstrated by the following image.</p> <p>Print out the fourth label</p> In\u00a0[\u00a0]: Copied! <pre># The label for the fourth data element\n\ntrain_dataset[3][1]\n</pre> # The label for the fourth data element  train_dataset[3][1] <p>Plot the fourth sample</p> In\u00a0[\u00a0]: Copied! <pre># The image for the fourth data element\nshow_data(train_dataset[3])\n</pre> # The image for the fourth data element show_data(train_dataset[3])  <p>The fourth sample is a \"1\".</p> <p>Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.</p> In\u00a0[\u00a0]: Copied! <pre>class CNN(nn.Module):\n    \n    # Contructor\n    def __init__(self, out_1=16, out_2=32):\n        super(CNN, self).__init__()\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n\n        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n        self.fc1 = nn.Linear(out_2 * 4 * 4, 10)\n    \n    # Prediction\n    def forward(self, x):\n        x = self.cnn1(x)\n        x = torch.relu(x)\n        x = self.maxpool1(x)\n        x = self.cnn2(x)\n        x = torch.relu(x)\n        x = self.maxpool2(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        return x\n</pre> class CNN(nn.Module):          # Contructor     def __init__(self, out_1=16, out_2=32):         super(CNN, self).__init__()         self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)         self.maxpool1=nn.MaxPool2d(kernel_size=2)          self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)         self.maxpool2=nn.MaxPool2d(kernel_size=2)         self.fc1 = nn.Linear(out_2 * 4 * 4, 10)          # Prediction     def forward(self, x):         x = self.cnn1(x)         x = torch.relu(x)         x = self.maxpool1(x)         x = self.cnn2(x)         x = torch.relu(x)         x = self.maxpool2(x)         x = x.view(x.size(0), -1)         x = self.fc1(x)         return x  <p>Build a Convolutional Network class with two Convolutional layers and one fully connected layer. But we add Batch Norm for the convolutional layers.</p> In\u00a0[\u00a0]: Copied! <pre>class CNN_batch(nn.Module):\n    \n    # Contructor\n    def __init__(self, out_1=16, out_2=32,number_of_classes=10):\n        super(CNN_batch, self).__init__()\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n        self.conv1_bn = nn.BatchNorm2d(out_1)\n\n        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n        \n        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n        self.conv2_bn = nn.BatchNorm2d(out_2)\n\n        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n        self.fc1 = nn.Linear(out_2 * 4 * 4, number_of_classes)\n        self.bn_fc1 = nn.BatchNorm1d(10)\n    \n    # Prediction\n    def forward(self, x):\n        x = self.cnn1(x)\n        x=self.conv1_bn(x)\n        x = torch.relu(x)\n        x = self.maxpool1(x)\n        x = self.cnn2(x)\n        x=self.conv2_bn(x)\n        x = torch.relu(x)\n        x = self.maxpool2(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        x=self.bn_fc1(x)\n        return x\n</pre> class CNN_batch(nn.Module):          # Contructor     def __init__(self, out_1=16, out_2=32,number_of_classes=10):         super(CNN_batch, self).__init__()         self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)         self.conv1_bn = nn.BatchNorm2d(out_1)          self.maxpool1=nn.MaxPool2d(kernel_size=2)                  self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)         self.conv2_bn = nn.BatchNorm2d(out_2)          self.maxpool2=nn.MaxPool2d(kernel_size=2)         self.fc1 = nn.Linear(out_2 * 4 * 4, number_of_classes)         self.bn_fc1 = nn.BatchNorm1d(10)          # Prediction     def forward(self, x):         x = self.cnn1(x)         x=self.conv1_bn(x)         x = torch.relu(x)         x = self.maxpool1(x)         x = self.cnn2(x)         x=self.conv2_bn(x)         x = torch.relu(x)         x = self.maxpool2(x)         x = x.view(x.size(0), -1)         x = self.fc1(x)         x=self.bn_fc1(x)         return x <p>Function to train the model</p> In\u00a0[\u00a0]: Copied! <pre>def train_model(model,train_loader,validation_loader,optimizer,n_epochs=4):\n    \n    #global variable \n    N_test=len(validation_dataset)\n    accuracy_list=[]\n    loss_list=[]\n    for epoch in range(n_epochs):\n        for x, y in train_loader:\n            model.train()\n            optimizer.zero_grad()\n            z = model(x)\n            loss = criterion(z, y)\n            loss.backward()\n            optimizer.step()\n            loss_list.append(loss.data)\n\n        correct=0\n        #perform a prediction on the validation  data  \n        for x_test, y_test in validation_loader:\n            model.eval()\n            z = model(x_test)\n            _, yhat = torch.max(z.data, 1)\n            correct += (yhat == y_test).sum().item()\n        accuracy = correct / N_test\n        accuracy_list.append(accuracy)\n     \n    return accuracy_list, loss_list\n</pre> def train_model(model,train_loader,validation_loader,optimizer,n_epochs=4):          #global variable      N_test=len(validation_dataset)     accuracy_list=[]     loss_list=[]     for epoch in range(n_epochs):         for x, y in train_loader:             model.train()             optimizer.zero_grad()             z = model(x)             loss = criterion(z, y)             loss.backward()             optimizer.step()             loss_list.append(loss.data)          correct=0         #perform a prediction on the validation  data           for x_test, y_test in validation_loader:             model.eval()             z = model(x_test)             _, yhat = torch.max(z.data, 1)             correct += (yhat == y_test).sum().item()         accuracy = correct / N_test         accuracy_list.append(accuracy)           return accuracy_list, loss_list <p>There are 16 output channels for the first layer, and 32 output channels for the second layer</p> In\u00a0[\u00a0]: Copied! <pre># Create the model object using CNN class\nmodel = CNN(out_1=16, out_2=32)\n</pre> # Create the model object using CNN class model = CNN(out_1=16, out_2=32) <p>Define the loss function, the optimizer and the dataset loader</p> In\u00a0[\u00a0]: Copied! <pre>criterion = nn.CrossEntropyLoss()\nlearning_rate = 0.1\noptimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)\n</pre> criterion = nn.CrossEntropyLoss() learning_rate = 0.1 optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100) validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000) <p>Train the model and determine validation accuracy technically test accuracy (This may take a long time)</p> In\u00a0[\u00a0]: Copied! <pre># Train the model\naccuracy_list_normal, loss_list_normal=train_model(model=model,n_epochs=10,train_loader=train_loader,validation_loader=validation_loader,optimizer=optimizer)\n</pre> # Train the model accuracy_list_normal, loss_list_normal=train_model(model=model,n_epochs=10,train_loader=train_loader,validation_loader=validation_loader,optimizer=optimizer) <p>Repeat the Process for the model with  batch norm</p> In\u00a0[\u00a0]: Copied! <pre>model_batch=CNN_batch(out_1=16, out_2=32)\ncriterion = nn.CrossEntropyLoss()\nlearning_rate = 0.1\noptimizer = torch.optim.SGD(model_batch.parameters(), lr = learning_rate)\naccuracy_list_batch, loss_list_batch=train_model(model=model_batch,n_epochs=10,train_loader=train_loader,validation_loader=validation_loader,optimizer=optimizer)\n</pre> model_batch=CNN_batch(out_1=16, out_2=32) criterion = nn.CrossEntropyLoss() learning_rate = 0.1 optimizer = torch.optim.SGD(model_batch.parameters(), lr = learning_rate) accuracy_list_batch, loss_list_batch=train_model(model=model_batch,n_epochs=10,train_loader=train_loader,validation_loader=validation_loader,optimizer=optimizer) <p>Plot the loss with both networks.</p> In\u00a0[\u00a0]: Copied! <pre># Plot the loss and accuracy\n\nplt.plot(loss_list_normal, 'b',label='loss normal cnn ')\nplt.plot(loss_list_batch,'r',label='loss batch cnn')\nplt.xlabel('iteration')\nplt.title(\"loss\")\nplt.legend()\n</pre> # Plot the loss and accuracy  plt.plot(loss_list_normal, 'b',label='loss normal cnn ') plt.plot(loss_list_batch,'r',label='loss batch cnn') plt.xlabel('iteration') plt.title(\"loss\") plt.legend() In\u00a0[\u00a0]: Copied! <pre>plt.plot(accuracy_list_normal, 'b',label=' normal CNN')\nplt.plot(accuracy_list_batch,'r',label=' CNN with Batch Norm')\nplt.xlabel('Epoch')\nplt.title(\"Accuracy \")\nplt.legend()\nplt.show()\n</pre> plt.plot(accuracy_list_normal, 'b',label=' normal CNN') plt.plot(accuracy_list_batch,'r',label=' CNN with Batch Norm') plt.xlabel('Epoch') plt.title(\"Accuracy \") plt.legend() plt.show() <p>We see the CNN with batch norm performers better, with faster convergence.</p> What's on your mind? Put it in the comments!"},{"location":"Deep%20Learning/Week9-CNNs/9.4.3CNN_Small_Image_batch/#author-juma-shafara-date-2024-08-12-title-convolutional-neural-network-with-batch-normalization-keywords-training-two-parameter-mini-batch-gradient-decent-training-two-parameter-mini-batch-gradient-decent-description-in-this-lab-we-will-compare-a-convolutional-neural-network-using-batch-normalization-with-a-regular-convolutional-neural-network","title":"author: Juma Shafara date: \"2024-08-12\" title: Convolutional Neural Network with Batch-Normalization keywords: [Training Two Parameter, Mini-Batch Gradient Decent, Training Two Parameter Mini-Batch Gradient Decent] description: In this lab, we will compare a Convolutional Neural Network using Batch Normalization with a regular Convolutional Neural Network\u00b6","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.3CNN_Small_Image_batch/#read_me","title":"Read me Batch Norm for Convolution Operation","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.3CNN_Small_Image_batch/#Makeup_Data","title":"Get the Data","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.3CNN_Small_Image_batch/#CNN","title":"Build a Two Convolutional Neural Network Class","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.3CNN_Small_Image_batch/#Train","title":"Define the Convolutional Neural Network Classifier, Criterion function, Optimizer and Train the Model","text":""},{"location":"Deep%20Learning/Week9-CNNs/9.4.3CNN_Small_Image_batch/#Result","title":"Analyze Results","text":""},{"location":"Extras/classification_metrics/","title":"Classification Metrics Practice","text":"<p>In this notebook, we'll walk through the process of building and evaluating a decision tree classifier using Scikit-Learn. We'll use the Iris dataset for demonstration and then provide an exercise to apply the same steps to the Wine dataset.</p> Don't Miss Any Updates! <p> To be among the first to hear about future updates of the course materials, simply enter your email below, follow us on   (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Extras/classification_metrics/#importing-necessary-libraries","title":"Importing Necessary Libraries","text":"<p>First, we import the necessary libraries for data manipulation and loading the dataset.</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n</code></pre> <ul> <li><code>numpy</code> and <code>pandas</code> are imported for data manipulation.</li> <li><code>load_iris</code> from <code>sklearn.datasets</code> is imported to load the Iris dataset.</li> </ul>"},{"location":"Extras/classification_metrics/#loading-the-iris-dataset","title":"Loading the Iris Dataset","text":"<pre><code>iris = load_iris()\n</code></pre> <p>The Iris dataset is loaded and stored in the variable iris.</p>"},{"location":"Extras/classification_metrics/#displaying-dataset-description","title":"Displaying Dataset Description","text":"<p>For a better understanding of the dataset, we can uncomment the following line to print the description of the Iris dataset.</p> <pre><code>## uncomment and run to read the data description\n# print(iris['DESCR'])\n</code></pre>"},{"location":"Extras/classification_metrics/#extracting-features-and-target-variables","title":"Extracting Features and Target Variables","text":"<pre><code>X = iris['data']\ny = iris['target']\n</code></pre> <ul> <li>X contains the feature data (sepal length, sepal width, petal length, petal width).</li> <li>y contains the target data (class labels: 0, 1, 2).</li> </ul>"},{"location":"Extras/classification_metrics/#importing-train-test-split-function","title":"Importing Train-Test Split Function","text":"<pre><code>from sklearn.model_selection import train_test_split\n</code></pre> <p><code>train_test_split</code> is imported to split the data into training and testing sets.</p>"},{"location":"Extras/classification_metrics/#splitting-the-data","title":"Splitting the Data","text":"<pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n</code></pre> <p>The dataset is split into training (70%) and testing (30%) sets.</p>"},{"location":"Extras/classification_metrics/#importing-decision-tree-classifier","title":"Importing Decision Tree Classifier","text":"<p>Next, we import the Decision Tree classifier from Scikit-Learn.</p> <pre><code>from sklearn.tree import DecisionTreeClassifier\n</code></pre>"},{"location":"Extras/classification_metrics/#initializing-the-classifier","title":"Initializing the Classifier","text":"<p>We create an instance of the Decision Tree classifier</p> <pre><code>classifier = DecisionTreeClassifier()\n</code></pre>"},{"location":"Extras/classification_metrics/#training-the-classifier","title":"Training the Classifier","text":"<p>We train the classifier using the training data.</p> <pre><code>classifier.fit(X_train, y_train)\n</code></pre> <pre>DecisionTreeClassifier()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFitted<pre>DecisionTreeClassifier()</pre>"},{"location":"Extras/classification_metrics/#making-predictions","title":"Making Predictions","text":"<p>We then make predictions on the test data using the the <code>predict()</code> method on the model</p> <pre><code>preds = classifier.predict(X_test)\n</code></pre>"},{"location":"Extras/classification_metrics/#importing-metrics-for-evaluation","title":"Importing Metrics for Evaluation","text":"<p>To evaluate our model, we import various metrics from Scikit-Learn.</p> <pre><code>from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n</code></pre>"},{"location":"Extras/classification_metrics/#calculating-accuracy","title":"Calculating Accuracy","text":"<p>Accuracy refers to the proportion of correctly predicted instances out of the total instances.</p> <pre><code>accuracy_score(y_test, preds)\n</code></pre> <pre><code>0.9777777777777777\n</code></pre>"},{"location":"Extras/classification_metrics/#calculating-precision","title":"Calculating Precision","text":"<p>Precision is the ratio of correctly predicted positive observations to the total predicted positives.</p> <pre><code>precision_score(y_test, preds, average='weighted')\n</code></pre> <pre><code>0.9794871794871796\n</code></pre>"},{"location":"Extras/classification_metrics/#calculating-recall","title":"Calculating Recall","text":"<p>Recall is the ratio of correctly predicted positive observations to all the actual positives.</p> <pre><code>recall_score(y_test, preds, average='weighted')\n</code></pre> <pre><code>0.9777777777777777\n</code></pre>"},{"location":"Extras/classification_metrics/#calculating-f1-score","title":"Calculating F1 Score","text":"<p>The f1 score refers to the Harmonic mean of Precision and Recall.</p> <pre><code>f1_score(y_test, preds, average='weighted')\n</code></pre> <pre><code>0.977863799283154\n</code></pre>"},{"location":"Extras/classification_metrics/#displaying-the-classification-report","title":"Displaying the Classification Report","text":"<p>We can print the classification report, which provides precision, recall, F1-score, and support for each class.</p> <pre><code>from sklearn.metrics import classification_report\n</code></pre> <pre><code>classification_report = classification_report(y_test, preds)\nprint(classification_report)\n</code></pre> <pre><code>              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        17\n           1       1.00      0.94      0.97        16\n           2       0.92      1.00      0.96        12\n\n    accuracy                           0.98        45\n   macro avg       0.97      0.98      0.98        45\nweighted avg       0.98      0.98      0.98        45\n</code></pre> <p>The results show how well the model performs in classifying the iris species, with metrics providing insights into different aspects of the model's performance.</p> <pre><code>from sklearn.metrics import confusion_matrix\n</code></pre> <pre><code>conf_matrix = confusion_matrix(y_test, preds)\nconf_matrix = pd.DataFrame(conf_matrix, index=[0, 1, 2], columns=[0, 1, 2])\n# print(\"Confusion Matrix:\\n\", conf_matrix)\nconf_matrix\n</code></pre> 0 1 2 0 17 0 0 1 0 15 1 2 0 0 12"},{"location":"Extras/classification_metrics/#exercise","title":"Exercise:What's on your mind? Put it in the comments!","text":"<p>Perform the steps above using the wine dataset from sklearn</p>"},{"location":"Extras/customer_analysis/","title":"Customer Analysis","text":"<p>In this project, I want to look at customer data pulled from github and create some visuals in my jupyter notebook to observe any trends related to customers.</p> Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on   (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Extras/customer_analysis/#downloading-the-dataset","title":"Downloading the Dataset:","text":"<p>First we download our data sets from github.</p> <pre><code>!pip install dataidea --upgrade --quiet\n</code></pre> <pre><code>import opendatasets as od\n\n# download the dataset\ndataset_url = 'https://raw.githubusercontent.com/Kaushik-Varma/Marketing_Data_Analysis/master/Marketing_Analysis.csv'\nod.download(dataset_url)\n</code></pre> <pre><code>Using downloaded and verified file: ./Marketing_Analysis.csv\n</code></pre>"},{"location":"Extras/customer_analysis/#data-preparation-and-cleaning","title":"Data Preparation and Cleaning","text":"<p>Get our dataset into a data frame, examine the tables to check for incorrect, inconsistent, or invalid entries. Handle other cleaning steps as necessary.</p> <pre><code>#import the useful libraries.\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Read the data set of \"Marketing Analysis\" in data.\ndata= pd.read_csv(\"Marketing_Analysis.csv\", low_memory=False)\n\n# Printing the data\ndata.head()\n</code></pre> banking marketing Unnamed: 1 Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14 Unnamed: 15 Unnamed: 16 Unnamed: 17 Unnamed: 18 0 customer id and age. NaN Customer salary and balance. NaN Customer marital status and job with education... NaN particular customer before targeted or not NaN Loan types: loans or housing loans NaN Contact type NaN month of contact duration of call NaN NaN NaN outcome of previous contact response of customer after call happned 1 customerid age salary balance marital jobedu targeted default housing loan contact day month duration campaign pdays previous poutcome response 2 1 58 100000 2143 married management,tertiary yes no yes no unknown 5 may, 2017 261 sec 1 -1 0 unknown no 3 2 44 60000 29 single technician,secondary yes no yes no unknown 5 may, 2017 151 sec 1 -1 0 unknown no 4 3 33 120000 2 married entrepreneur,secondary yes no yes yes unknown 5 may, 2017 76 sec 1 -1 0 unknown no"},{"location":"Extras/customer_analysis/#cleaning-the-data","title":"Cleaning the Data","text":"<p>Here we need to fix some of the columns/rows to make the data easier to use.</p> <pre><code>#import the useful libraries.\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Read the file in data without first two rows as it is of no use.\ndata = pd.read_csv(\"Marketing_Analysis.csv\",skiprows = 2)\n\n#print the head of the data frame.\ndata.head()\n</code></pre> customerid age salary balance marital jobedu targeted default housing loan contact day month duration campaign pdays previous poutcome response 0 1 58.0 100000 2143 married management,tertiary yes no yes no unknown 5 may, 2017 261 sec 1 -1 0 unknown no 1 2 44.0 60000 29 single technician,secondary yes no yes no unknown 5 may, 2017 151 sec 1 -1 0 unknown no 2 3 33.0 120000 2 married entrepreneur,secondary yes no yes yes unknown 5 may, 2017 76 sec 1 -1 0 unknown no 3 4 47.0 20000 1506 married blue-collar,unknown no no yes no unknown 5 may, 2017 92 sec 1 -1 0 unknown no 4 5 33.0 0 1 single unknown,unknown no no no no unknown 5 may, 2017 198 sec 1 -1 0 unknown no <pre><code># Drop the customer id as it is of no use.\ndata.drop('customerid', axis = 1, inplace = True)\n\n#Extract job  &amp; Education in newly from \"jobedu\" column.\ndata['job']= data[\"jobedu\"].apply(lambda x: x.split(\",\")[0])\ndata['education']= data[\"jobedu\"].apply(lambda x: x.split(\",\")[1])\n\n# Drop the \"jobedu\" column from the dataframe.\ndata.drop('jobedu', axis = 1, inplace = True)\n\n# Printing the Dataset\ndata.sample(n=5)\n</code></pre> age salary balance marital targeted default housing loan contact day month duration campaign pdays previous poutcome response job education 7369 28.0 60000 1180 married yes no yes no unknown 29 may, 2017 637 sec 3 -1 0 unknown no technician secondary 31281 44.0 100000 483 single no no no no cellular 6 mar, 2017 3.45 min 2 199 6 success yes management tertiary 736 40.0 20000 -7 married yes no yes no unknown 6 may, 2017 410 sec 2 -1 0 unknown no blue-collar primary 45207 71.0 55000 1729 divorced yes no no no cellular 17 nov, 2017 7.6 min 2 -1 0 unknown yes retired primary 6297 53.0 60000 6 married yes no yes no unknown 27 may, 2017 233 sec 2 -1 0 unknown no self-employed primary <pre><code># Checking the missing values\ndata.isnull().sum()\n</code></pre> <pre><code>age          20\nsalary        0\nbalance       0\nmarital       0\ntargeted      0\ndefault       0\nhousing       0\nloan          0\ncontact       0\nday           0\nmonth        50\nduration      0\ncampaign      0\npdays         0\nprevious      0\npoutcome      0\nresponse     30\njob           0\neducation     0\ndtype: int64\n</code></pre> <pre><code># Dropping the records with age missing in data dataframe.\ndata = data[~data.age.isnull()].copy()\n\n# Checking the missing values in the dataset.\ndata.isnull().sum()\n</code></pre> <pre><code>age           0\nsalary        0\nbalance       0\nmarital       0\ntargeted      0\ndefault       0\nhousing       0\nloan          0\ncontact       0\nday           0\nmonth        50\nduration      0\ncampaign      0\npdays         0\nprevious      0\npoutcome      0\nresponse     30\njob           0\neducation     0\ndtype: int64\n</code></pre> <pre><code># Find the mode of month in data\nmonth_mode = data.month.mode()[0]\n\n# Fill the missing values with mode value of month in data.\ndata.month.fillna(month_mode, inplace = True)\n\n# Let's see the null values in the month column.\ndata.month.isnull().sum()\n</code></pre> <pre><code>/tmp/ipykernel_39902/3697544734.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  data.month.fillna(month_mode, inplace = True)\n\n\n\n\n\n0\n</code></pre> <pre><code>#drop the records with response missing in data.\ndata = data[~data.response.isnull()].copy()\n# Calculate the missing values in each column of data frame\ndata.isnull().sum()\n</code></pre> <pre><code>age          0\nsalary       0\nbalance      0\nmarital      0\ntargeted     0\ndefault      0\nhousing      0\nloan         0\ncontact      0\nday          0\nmonth        0\nduration     0\ncampaign     0\npdays        0\nprevious     0\npoutcome     0\nresponse     0\njob          0\neducation    0\ndtype: int64\n</code></pre>"},{"location":"Extras/customer_analysis/#exploratory-analysis-and-visualization","title":"Exploratory Analysis and Visualization","text":"<p>Now we apply some data manipulation steps and explore some of the findings through the use of visuals. Hopefully we can then gain some useful insights from our data. </p>"},{"location":"Extras/customer_analysis/#what-kind-of-employment-is-most-common-in-our-data","title":"What kind of employment is most common in our data?","text":"<pre><code># Let's calculate the percentage of each job status category.\ndata.job.value_counts(normalize=True)\n\n#plot the bar graph of percentage job categories\ndata.job.value_counts(normalize=True).plot.barh()\nplt.show()\n</code></pre>"},{"location":"Extras/customer_analysis/#what-is-the-education-level","title":"What is the education level?","text":"<pre><code>#calculate the percentage of each education category.\ndata.education.value_counts(normalize=True)\n\n#plot the pie chart of education categories\ndata.education.value_counts(normalize=True).plot.pie()\nplt.show()\n</code></pre> <pre><code>data.salary.describe()\n</code></pre> <pre><code>count     45161.000000\nmean      57004.849317\nstd       32087.698810\nmin           0.000000\n25%       20000.000000\n50%       60000.000000\n75%       70000.000000\nmax      120000.000000\nName: salary, dtype: float64\n</code></pre>"},{"location":"Extras/customer_analysis/#what-are-the-balances-for-individuals-based-on-their-age","title":"What are the balances for individuals based on their age?","text":"<pre><code>#plot the scatter plot of balance and salary variable in data\nplt.scatter(data.salary,data.balance)\nplt.show()\n\n#plot the scatter plot of balance and age variable in data\ndata.plot.scatter(x=\"age\",y=\"balance\")\nplt.show()\n</code></pre>"},{"location":"Extras/customer_analysis/#what-is-correlating-with-balance","title":"What is correlating with balance?","text":"<pre><code>#plot the pair plot of salary, balance and age in data dataframe.\nsns.pairplot(data = data, vars=['salary','balance','age'])\nplt.show()\n</code></pre> <pre><code># Creating a matrix using age, salary, balance as rows and columns\ndata[['age','salary','balance']].corr()\n\n#plot the correlation matrix of salary, balance and age in data dataframe.\nsns.heatmap(data[['age','salary','balance']].corr(), annot=True, cmap = 'Greens')\nplt.show()\n</code></pre>"},{"location":"Extras/customer_analysis/#what-is-the-salary-range-and-averages-for-both-response-types","title":"What is the salary range and averages for both response types?","text":"<pre><code>#create response_rate of numerical data type where response \"yes\"= 1, \"no\"= 0\ndata['response_rate'] = np.where(data.response=='yes',1,0)\ndata.response_rate.value_counts()\n</code></pre> <pre><code>response_rate\n0    39876\n1     5285\nName: count, dtype: int64\n</code></pre>"},{"location":"Extras/customer_analysis/#what-marital-status-has-the-highest-response-rate","title":"What marital status has the highest response rate?","text":"<pre><code>#plot the bar graph of marital status with average value of response_rate\ndata.groupby('marital')['response_rate'].mean().plot.bar()\nplt.show()\n</code></pre>"},{"location":"Extras/customer_analysis/#what-combination-of-education-and-marital-status-has-the-largest-response-rate","title":"What combination of education and marital status has the largest response rate?","text":"<pre><code>result = pd.pivot_table(data=data, index='education', columns='marital',values='response_rate')\nprint(result)\n\n#create heat map of to show correlations betwenn education vs marital vs response_rate\nsns.heatmap(result, annot=True, cmap = 'RdYlGn', center=0.117)\nplt.show()\n</code></pre> <pre><code>marital    divorced   married    single\neducation                              \nprimary    0.138852  0.075601  0.106808\nsecondary  0.103559  0.094650  0.129271\ntertiary   0.137415  0.129835  0.183737\nunknown    0.142012  0.122519  0.162879\n</code></pre>"},{"location":"Extras/customer_analysis/#what-is-the-average-salary-for-each-age-group-in-the-data","title":"What is the average salary for each age group in the data?","text":"<pre><code>#plot the bar graph of age groups with average salary for that group\nbins = [18, 30, 40, 50, 60, 70, 120]\nlabels = ['18-29', '30-39', '40-49', '50-59', '60-69', '70+']\ndata['agerange'] = pd.cut(data.age, bins, labels = labels,include_lowest = True)\n\n#plot the bar graph of average salary per age group\ndata.groupby('agerange')['salary'].mean().plot.bar()\nplt.title('Avg Salary per Age',fontsize = 12)\nplt.show()\n</code></pre> <pre><code>/tmp/ipykernel_39902/506738209.py:7: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  data.groupby('agerange')['salary'].mean().plot.bar()\n</code></pre> <p>Let us save and upload our work to Jovian before finishing up.</p>"},{"location":"Extras/customer_analysis/#conclusions","title":"ConclusionsWhat's on your mind? Put it in the comments!","text":"<p>What we can say from the visuals above are the following:</p> <ol> <li>Approx. 60% of our customers are in the technician/management/blue collar category of work.</li> <li>Half are high school graduates, and less than a third have higher education.</li> <li>For people under 65, the balance is typically between 0-20000. For over 65, we see 0-10000 is the range. </li> <li>Heatmap supports the age-balance correlation to be stronger than salary-balance.</li> <li>Reponse rate is highest for single highly educated and lowest for married and less educated individuals. </li> </ol>"},{"location":"Extras/handling_imbalanced_data/","title":"Imbalanced Datasets","text":""},{"location":"Extras/handling_imbalanced_data/#handling-imbalanced-dataset","title":"Handling Imbalanced Dataset","text":"<p>Handling imbalanced datasets is a common challenge in machine learning, especially in classification tasks where one class significantly outnumbers the other(s). Let's go through a simple example using the popular Iris dataset, which we'll artificially imbalance for demonstration purposes. </p> <p>The Iris dataset consists of 150 samples, each belonging to one of three classes: Iris Setosa, Iris Versicolour, and Iris Virginica. We'll create an imbalanced version of this dataset where one class is underrepresented.</p> <p>First, let's load the dataset and create the imbalance: </p> <pre><code># !pip install imbalanced-learn\n# !pip install --upgrade dataidea\n</code></pre> <pre><code>from dataidea.packages import pd, plt, np\nfrom sklearn.datasets import load_iris\n</code></pre> <pre><code># Load Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Convert to DataFrame for manipulation\ndf = pd.DataFrame(data=np.c_[X, y], \n                  columns=iris.feature_names + ['target'])\n\ndf.head()\n</code></pre> sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 0 5.1 3.5 1.4 0.2 0.0 1 4.9 3.0 1.4 0.2 0.0 2 4.7 3.2 1.3 0.2 0.0 3 4.6 3.1 1.5 0.2 0.0 4 5.0 3.6 1.4 0.2 0.0 Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates, simply enter your email below, follow us on   (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Extras/handling_imbalanced_data/#introducing-imbalance","title":"Introducing Imbalance","text":"<pre><code># Introduce imbalance by removing samples from one class\nclass_to_remove = 2  # Iris Virginica\nimbalance_ratio = 0.5  # Ratio of samples to be removed\nindices_to_remove = np.random.choice(df[df['target'] == class_to_remove].index,\n                                     size=int(imbalance_ratio * len(df[df['target'] == class_to_remove])),\n                                     replace=False)\ndf_imbalanced = df.drop(indices_to_remove)\n\n# Check the class distribution\nvalue_counts = df_imbalanced['target'].value_counts()\nprint(value_counts)\n</code></pre> <pre><code>target\n0.0    50\n1.0    50\n2.0    25\nName: count, dtype: int64\n</code></pre> <pre><code>plt.bar(value_counts.index, \n        value_counts.values, \n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n</code></pre> <p>Now, <code>df_imbalanced</code> contains the imbalanced dataset. Next, we'll demonstrate a few techniques to handle this imbalance:</p> <ol> <li>Resampling Methods:</li> <li>Oversampling: Randomly duplicate samples from the minority class.</li> <li>Undersampling: Randomly remove samples from the majority class.</li> <li>Synthetic Sampling Methods:</li> <li>SMOTE (Synthetic Minority Over-sampling Technique): Generates synthetic samples for the minority class.</li> <li>Algorithmic Techniques:</li> <li>Algorithm Tuning: Adjusting class weights in the algorithm.</li> <li>Ensemble Methods: Using ensemble techniques like bagging or boosting.</li> </ol>"},{"location":"Extras/handling_imbalanced_data/#resampling","title":"Resampling","text":""},{"location":"Extras/handling_imbalanced_data/#oversampling","title":"Oversampling","text":"<p>Let's implement oversampling using the <code>imbalanced-learn</code> library:</p> <pre><code>from imblearn.over_sampling import RandomOverSampler\n\n# Separate features and target\nX_imbalanced = df_imbalanced.drop('target', axis=1)\ny_imbalanced = df_imbalanced['target']\n\n# Apply Random Over-Sampling\noversample = RandomOverSampler(sampling_strategy='auto', \n                               random_state=42)\nX_resampled, y_resampled = oversample.fit_resample(X_imbalanced, y_imbalanced)\n\n# Check the class distribution after oversampling\noversampled_data_value_counts = pd.Series(y_resampled).value_counts()\nprint(oversampled_data_value_counts)\n</code></pre> <pre><code>target\n0.0    50\n1.0    50\n2.0    50\nName: count, dtype: int64\n</code></pre> <pre><code>plt.bar(oversampled_data_value_counts.index, \n        oversampled_data_value_counts.values, \n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n</code></pre> <p></p>"},{"location":"Extras/handling_imbalanced_data/#undersampling","title":"Undersampling:","text":"<p>Undersampling involves reducing the number of samples in the majority class to balance the dataset. Here's how you can apply random undersampling using the <code>imbalanced-learn</code> library:</p> <pre><code>from imblearn.under_sampling import RandomUnderSampler\n\n# Apply Random Under-Sampling\nundersample = RandomUnderSampler(sampling_strategy='auto', random_state=42)\nX_resampled, y_resampled = undersample.fit_resample(X_imbalanced, y_imbalanced)\n\n# Check the class distribution after undersampling\nundersampled_data_value_counts = pd.Series(y_resampled).value_counts()\nprint(undersampled_data_value_counts)\n</code></pre> <pre><code>target\n0.0    25\n1.0    25\n2.0    25\nName: count, dtype: int64\n</code></pre> <pre><code>plt.bar(undersampled_data_value_counts.index, \n        undersampled_data_value_counts.values, \n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n</code></pre> <p></p>"},{"location":"Extras/handling_imbalanced_data/#smote-synthetic-minority-over-sampling-technique","title":"SMOTE (Synthetic Minority Over-sampling Technique)","text":"<p>SMOTE generates synthetic samples for the minority class by interpolating between existing minority class samples.</p> <p>Here's how you can apply SMOTE using the <code>imbalanced-learn</code> library:</p> <pre><code>from imblearn.over_sampling import SMOTE\n\n# Apply SMOTE\nsmote = SMOTE(sampling_strategy='auto', random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_imbalanced, y_imbalanced)\n\n# Check the class distribution after SMOTE\nsmoted_data_value_counts = pd.Series(y_resampled).value_counts()\nprint(smoted_data_value_counts)\n</code></pre> <pre><code>target\n0.0    50\n1.0    50\n2.0    50\nName: count, dtype: int64\n</code></pre> <pre><code>plt.bar(smoted_data_value_counts.index, \n        smoted_data_value_counts.values, \n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n</code></pre> <p></p> <p>By using SMOTE, you can generate synthetic samples for the minority class, effectively increasing its representation in the dataset. This can help to mitigate the class imbalance issue and improve the performance of your machine learning model.</p>"},{"location":"Extras/handling_imbalanced_data/#algorithmic-techniques","title":"Algorithmic Techniques","text":""},{"location":"Extras/handling_imbalanced_data/#algorithm-tuning","title":"Algorithm Tuning:","text":"<p>Many algorithms allow you to specify class weights to penalize misclassifications of the minority class more heavily. Here's an example using the <code>class_weight</code> parameter in a logistic regression classifier:</p> <pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n</code></pre> <pre><code># Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_imbalanced, y_imbalanced, \n                                                    test_size=0.2, random_state=42)\n\n# Define the logistic regression classifier with class weights\nclass_weights = {0: 1, 1: 1, 2: 20}  # Penalize the minority class more heavily\nlog_reg = LogisticRegression(class_weight=class_weights)\n\n# Train the model\nlog_reg.fit(X_train, y_train)\n\n# Evaluate the model\ny_pred = log_reg.predict(X_test)\n\n# display classification report\npd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose()\n</code></pre> precision recall f1-score support 0.0 1.000000 1.000000 1.000000 13.00 1.0 1.000000 0.750000 0.857143 8.00 2.0 0.666667 1.000000 0.800000 4.00 accuracy 0.920000 0.920000 0.920000 0.92 macro avg 0.888889 0.916667 0.885714 25.00 weighted avg 0.946667 0.920000 0.922286 25.00 <p>In this example, the class weight for the minority class is increased to penalize misclassifications more heavily.</p>"},{"location":"Extras/handling_imbalanced_data/#ensemble-methods","title":"Ensemble Methods","text":"<p>Ensemble methods can also be effective for handling imbalanced datasets. Techniques such as bagging and boosting can improve the performance of classifiers, especially when dealing with imbalanced classes.</p> <p>Here's an example of using ensemble methods like Random Forest, a popular bagging algorithm, with an imbalanced dataset:</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# Define and train Random Forest classifier\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_classifier.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred_rf = rf_classifier.predict(X_test)\n\n# Evaluate the performance\nprint(\"Random Forest Classifier:\")\npd.DataFrame(data=classification_report(y_test, y_pred_rf, output_dict=True)).transpose()\n</code></pre> <pre><code>Random Forest Classifier:\n</code></pre> precision recall f1-score support 0.0 1.000000 1.000000 1.000000 13.00 1.0 1.000000 0.875000 0.933333 8.00 2.0 0.800000 1.000000 0.888889 4.00 accuracy 0.960000 0.960000 0.960000 0.96 macro avg 0.933333 0.958333 0.940741 25.00 weighted avg 0.968000 0.960000 0.960889 25.00 <p>Ensemble methods like Random Forest build multiple decision trees and combine their predictions to make a final prediction. This can often lead to better generalization and performance, even in the presence of imbalanced classes.</p>"},{"location":"Extras/handling_imbalanced_data/#adaboost-classifier","title":"AdaBoost Classifier","text":"<p>Another ensemble method that specifically addresses class imbalance is AdaBoost (Adaptive Boosting). AdaBoost focuses more on those training instances that were previously misclassified, thus giving higher weight to the minority class instances.</p> <pre><code>from sklearn.ensemble import AdaBoostClassifier\n\n# Define and train AdaBoost classifier\nada_classifier = AdaBoostClassifier(n_estimators=100, random_state=42)\nada_classifier.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred_ada = ada_classifier.predict(X_test)\n\n# Evaluate the performance\nprint(\"AdaBoost Classifier:\")\npd.DataFrame(data=classification_report(y_test, y_pred_ada, output_dict=True)).transpose()\n</code></pre> <pre><code>AdaBoost Classifier:\n\n\n/home/jumashafara/venvs/dataidea/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n  warnings.warn(\n</code></pre> precision recall f1-score support 0.0 1.000000 1.000000 1.000000 13.00 1.0 1.000000 0.875000 0.933333 8.00 2.0 0.800000 1.000000 0.888889 4.00 accuracy 0.960000 0.960000 0.960000 0.96 macro avg 0.933333 0.958333 0.940741 25.00 weighted avg 0.968000 0.960000 0.960889 25.00 <p>By utilizing ensemble methods like Random Forest and AdaBoost, you can often achieve better performance on imbalanced datasets compared to individual classifiers, as these methods inherently mitigate the effects of class imbalance through their construction.</p> <p>These are just a few techniques for handling imbalanced datasets. It's crucial to experiment with different methods and evaluate their performance using appropriate evaluation metrics to find the best approach for your specific problem.</p> What's on your mind? Put it in the comments!"},{"location":"Extras/how_KNN_works/","title":"Understanding K-Nearest Neighbors (KNN) Regression","text":""},{"location":"Extras/how_KNN_works/#introduction-to-knn-regression","title":"Introduction to KNN Regression","text":"<p>K-Nearest Neighbors (KNN) regression is a type of instance-based learning algorithm used for regression problems. It makes predictions based on the \\(k\\) most similar instances (neighbors) in the training dataset. The algorithm is non-parametric, meaning it makes predictions without assuming any underlying data distribution.</p>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Extras/how_KNN_works/#key-concepts","title":"Key Concepts","text":"<ol> <li>Distance Metric: The method used to calculate the distance between instances. Common metrics include Euclidean, Manhattan, and Minkowski distances.</li> <li>k: The number of neighbors to consider when making a prediction. Choosing the right \\(k\\) is crucial for the algorithm's performance.</li> <li>Weighted KNN: In some variants, closer neighbors have a higher influence on the prediction than more distant ones, often implemented by assigning weights inversely proportional to the distance.</li> </ol>"},{"location":"Extras/how_KNN_works/#how-knn-regression-works","title":"How KNN Regression Works","text":""},{"location":"Extras/how_KNN_works/#step-by-step-process","title":"Step-by-Step Process","text":"<ol> <li>Load the Data: Start with a dataset consisting of feature vectors and their corresponding target values.</li> <li>Choose the Number of Neighbors (k): Select the number of nearest neighbors to consider for making predictions.</li> <li>Distance Calculation: For a new data point, calculate the distance between this point and all points in the training dataset.</li> <li>Find Nearest Neighbors: Identify the \\(k\\) points in the training data that are closest to the new point.</li> <li>Predict the Target Value: Compute the average (or a weighted average) of the target values of the \\(k\\) nearest neighbors.</li> </ol>"},{"location":"Extras/how_KNN_works/#example","title":"Example","text":"<p>Let's walk through an example with a simple dataset.</p> <p>Dataset:</p> Feature (X) Target (Y) 1.0 2.0 2.0 3.0 3.0 4.5 4.0 6.0 5.0 7.5 <p>New Point: \\(X_{new} = 3.5\\)</p> <ol> <li>Choose \\(k\\): Let's select \\(k = 3\\).</li> <li>Calculate Distances:</li> <li>Distance to (1.0, 2.0): \\(\\sqrt{(3.5-1.0)^2} = 2.5\\)</li> <li>Distance to (2.0, 3.0): \\(\\sqrt{(3.5-2.0)^2} = 1.5\\)</li> <li>Distance to (3.0, 4.5): \\(\\sqrt{(3.5-3.0)^2} = 0.5\\)</li> <li>Distance to (4.0, 6.0): \\(\\sqrt{(3.5-4.0)^2} = 0.5\\)</li> <li>Distance to (5.0, 7.5): \\(\\sqrt{(3.5-5.0)^2} = 1.5\\)</li> <li>Find Nearest Neighbors:</li> <li>Neighbors: (3.0, 4.5), (4.0, 6.0), and (2.0, 3.0) (distances 0.5, 0.5, and 1.5 respectively)</li> <li>Predict the Target Value:</li> <li>Average the target values of the nearest neighbors: \\(\\frac{4.5 + 6.0 + 3.0}{3} = \\frac{13.5}{3} = 4.5\\)</li> </ol> <p>So, the predicted target value for \\(X_{new} = 3.5\\) is 4.5.</p>"},{"location":"Extras/how_KNN_works/#visualizing-knn-regression","title":"Visualizing KNN Regression","text":"<p>Below is a visual representation of the KNN regression process:</p> <p></p> <ul> <li>The blue points represent the training data.</li> <li>The red point is the new input for which we want to predict the target value.</li> <li>The green points are the nearest neighbors considered for the prediction.</li> </ul>"},{"location":"Extras/how_KNN_works/#advantages-and-disadvantages","title":"Advantages and Disadvantages","text":""},{"location":"Extras/how_KNN_works/#advantages","title":"Advantages:","text":"<ul> <li>Simplicity: Easy to understand and implement.</li> <li>No Training Phase: The algorithm stores the training dataset and makes predictions at runtime.</li> </ul>"},{"location":"Extras/how_KNN_works/#disadvantages","title":"Disadvantages:","text":"<ul> <li>Computationally Intensive: Requires computing the distance to all training points for each prediction, which can be slow for large datasets.</li> <li>Choosing \\( k \\): Selecting the optimal \\( k \\) can be challenging and often requires cross-validation.</li> <li>Curse of Dimensionality: Performance can degrade in high-dimensional spaces as distances become less meaningful.</li> </ul>"},{"location":"Extras/how_KNN_works/#conclusion","title":"Conclusion","text":"<p>KNN regression is a straightforward and intuitive algorithm for making predictions based on the similarity of data points. Despite its simplicity, it can be quite powerful, especially for smaller datasets. However, it can become computationally expensive for large datasets and high-dimensional data, and it requires careful selection of the number of neighbors \\( k \\).</p> <p>By understanding and visualizing the KNN regression process, you can better appreciate its applications and limitations, allowing you to apply it effectively in your machine learning projects.</p> What's on your mind? Put it in the comments!"},{"location":"Machine%20Learning/41_overview_of_machine_learning/","title":"Overview of Machine Learning","text":""},{"location":"Machine%20Learning/41_overview_of_machine_learning/#1-introduction-to-machine-learning","title":"1. Introduction to Machine Learning","text":"<p>Machine learning (ML) is a branch of artificial intelligence that involves training algorithms to make predictions or decisions based on data. It's widely used in many fields, such as healthcare, finance, and marketing.</p>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#2-types-of-machine-learning","title":"2. Types of Machine Learning","text":"<ul> <li>Supervised Learning: Learn from labeled data.</li> <li>Unsupervised Learning: Discover patterns in unlabeled data.</li> <li>Reinforcement Learning: Agents learn to make decisions by interacting with the environment.</li> </ul>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#3-key-concepts-in-machine-learning","title":"3. Key Concepts in Machine Learning","text":"<ul> <li>Feature: A measurable property of the data.</li> <li>Label: The target variable (what you're predicting).</li> <li>Training Set: The data used to train the model.</li> <li>Test Set: The data used to evaluate the model's performance.</li> </ul>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#4-steps-in-a-machine-learning-workflow","title":"4. Steps in a Machine Learning Workflow","text":"<ol> <li>Data Collection</li> <li>Data Preprocessing</li> <li>Feature Engineering</li> <li>Model Selection</li> <li>Model Training</li> <li>Model Evaluation</li> <li>Hyperparameter Tuning</li> </ol>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#5-real-world-application-with-iris-dataset","title":"5. Real-world Application with Iris Dataset","text":""},{"location":"Machine%20Learning/41_overview_of_machine_learning/#a-data-loading","title":"(a) Data Loading","text":"<p>We will load the Iris dataset using <code>sklearn.datasets</code>.</p> <pre><code>import pandas as pd\nfrom sklearn.datasets import load_iris\n\n# Load Iris dataset\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = pd.Series(iris.target, name='species')\n\n# Display the first 5 rows of the dataset\nX.head()\n</code></pre> sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#b-data-preprocessing","title":"(b) Data Preprocessing","text":"<p>We check for missing values, and split the data into training and testing sets.</p> <pre><code>from sklearn.model_selection import train_test_split\n\n# Split data into train and test sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Check for missing values\nprint(X.isnull().sum())\n</code></pre> <pre><code>sepal length (cm)    0\nsepal width (cm)     0\npetal length (cm)    0\npetal width (cm)     0\ndtype: int64\n</code></pre>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#c-exploratory-data-analysis-eda","title":"(c) Exploratory Data Analysis (EDA)","text":"<p>We'll visualize the relationship between features and the target label.</p> <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Pairplot to visualize relationships between features\nsns.pairplot(pd.concat([X, y], axis=1), hue=\"species\")\nplt.show()\n</code></pre> <p></p>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#d-model-training","title":"(d) Model Training","text":"<p>We will train a Logistic Regression model, which is a simple yet effective supervised learning algorithm.</p> <pre><code>from sklearn.linear_model import LogisticRegression\n\n# Initialize and train the model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n</code></pre> <pre>LogisticRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0LogisticRegression?Documentation for LogisticRegressioniFitted<pre>LogisticRegression()</pre>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#e-model-evaluation","title":"(e) Model Evaluation","text":"<p>We evaluate the model's performance using accuracy and confusion matrix.</p> <pre><code>from sklearn.metrics import accuracy_score, confusion_matrix\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n</code></pre> <pre><code>Accuracy: 1.0\nConfusion Matrix:\n[[15  0  0]\n [ 0 11  0]\n [ 0  0 12]]\n</code></pre>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#f-hyperparameter-tuning","title":"(f) Hyperparameter Tuning","text":"<p>We use <code>GridSearchCV</code> to find the best hyperparameters for our model.</p> <pre><code>from sklearn.model_selection import GridSearchCV\n\n# Define parameter grid\nparam_grid = {'C': [0.1, 1, 10], 'solver': ['lbfgs', 'liblinear']}\n\n# Grid search\ngrid_search = GridSearchCV(LogisticRegression(max_iter=200), param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\n\n# Best parameters\nprint(\"Best Parameters: \", grid_search.best_params_)\n</code></pre> <pre><code>Best Parameters:  {'C': 1, 'solver': 'lbfgs'}\n</code></pre>"},{"location":"Machine%20Learning/41_overview_of_machine_learning/#g-conclusion","title":"(g) Conclusion","text":"<p>Congratulations on reaching the end of the tutorial, with the simple Logistic Regression model, we achieved a good accuracy on the Iris dataset. This example shows how to:</p> <ul> <li> Load and preprocess data</li> <li> Perform EDA</li> <li> Train and evaluate a model</li> <li> Tune hyperparameters for better performance</li> </ul> What's on your mind? Put it in the comments!"},{"location":"Machine%20Learning/42_training_models_meaning/","title":"Training a Model, Meaning","text":"<p>In this lesson you will learn what it means to train a machine learning for:</p> <ul> <li> Linear Regression</li> <li> Logistic Regression</li> </ul>"},{"location":"Machine%20Learning/42_training_models_meaning/#for-one-independent-variable-feature","title":"For one independent variable (feature)","text":""},{"location":"Machine%20Learning/42_training_models_meaning/#1-understanding-linear-regression","title":"1. Understanding Linear Regression:","text":"<ul> <li>Linear regression is a statistical method used to model the relationship between a dependent variable (often denoted as \\(y\\)) and one or more independent variables (often denoted as \\(x\\)).</li> <li>The relationship is modeled as a straight line equation: \\(y = mx + b\\), where \\(m\\) is the slope of the line and \\(b\\) is the y-intercept.</li> </ul>"},{"location":"Machine%20Learning/42_training_models_meaning/#2-fitting-a-model","title":"2. Fitting a Model:","text":"<ul> <li>When we say we're \"fitting a model\" in the context of linear regression, it means we're determining the best-fitting line (or plane in higher dimensions) that represents the relationship between the independent variable(s) and the dependent variable.</li> <li>This process involves finding the values of \\(m\\) and \\(b\\) that minimize the difference between the actual observed values of the dependent variable and the values predicted by the model.</li> <li>In simpler terms, fitting a model means finding the line that best describes the relationship between our input data and the output we want to predict.</li> </ul>"},{"location":"Machine%20Learning/42_training_models_meaning/#3-python-code-example","title":"3. Python Code Example:","text":"<ul> <li>Here's a simple example of how you might fit a linear regression model using Python, particularly with the <code>scikit-learn</code> library:</li> </ul> <pre><code># Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n</code></pre> <pre><code># Creating sample data\n # Independent variable (feature)\nX = np.array([[1], [2], [3], [4], [5]]) \n\n# Dependent variable (target)\ny = np.array([2, 4, 5, 4, 5])             \n</code></pre> <pre><code># Creating a linear regression model\nlinear_regression_model = LinearRegression()\n\n# Fitting the linear_regression_model to our data\nlinear_regression_model.fit(X, y)\n</code></pre> <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0LinearRegression?Documentation for LinearRegressioniFitted<pre>LinearRegression()</pre> <pre><code># Printing the slope (coefficient) and intercept of the best-fitting line\nprint(\"Slope (m):\", linear_regression_model.coef_[0])\nprint(\"Intercept (b):\", linear_regression_model.intercept_)\n</code></pre> <pre><code>Slope (m): 0.6\nIntercept (b): 2.2\n</code></pre> <pre><code># Predictions\ny_predicted = linear_regression_model.predict(X)\n</code></pre> <pre><code>y_predicted\n</code></pre> <pre><code>array([2.8, 3.4, 4. , 4.6, 5.2])\n</code></pre> <ul> <li>In this code:<ul> <li><code>X</code> represents the independent variable (feature), which is a column vector in this case.</li> <li><code>y</code> represents the dependent variable (target).</li> <li>We create a <code>LinearRegression</code> model object.</li> <li>We fit the model to our data using the <code>.fit()</code> method.</li> <li>Finally, we print out the slope (coefficient) and intercept of the best-fitting line.</li> </ul> </li> </ul> <p>Let's do some visualization</p> <pre><code># Plotting the linear regression line\nplt.scatter(X, y, color='blue', label='Data Points')\nplt.plot(X, y_predicted, color='red', label='Linear Regression Line')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Linear Regression')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>So, in summary, \"fitting a model\" means finding the best parameters (like slope and intercept in the case of linear regression) that describe the relationship between our input data and the output we want to predict.</p>"},{"location":"Machine%20Learning/42_training_models_meaning/#for-multiple-independent-variables","title":"For Multiple Independent Variables","text":"<p>If we have multiple independent variables (features) in \\(X\\), the process is still the same, but the equation becomes more complex. This is known as multiple linear regression.</p> <p>Here's how it works:</p> <p>Here are the corrected texts with LaTeX formulas suitable for Jupyter notebooks:</p>"},{"location":"Machine%20Learning/42_training_models_meaning/#1-understanding-multiple-linear-regression","title":"1. Understanding Multiple Linear Regression:","text":"<ul> <li>Instead of a single independent variable \\(x\\), we have multiple independent variables represented as a matrix \\(X\\).</li> <li>The relationship between the dependent variable \\(y\\) and the independent variables \\(X\\) is modeled as:     \\(\\(y = b_0 + b_1 x_1 + b_2 x_2 + \\cdots + b_n x_n\\)\\)    where \\(b_0\\) is the intercept, \\(b_1, b_2, \\ldots, b_n\\) are the coefficients corresponding to each independent variable \\(x_1, x_2, \\ldots, x_n\\).</li> </ul>"},{"location":"Machine%20Learning/42_training_models_meaning/#2-fitting-a-model-with-many-variables","title":"2. Fitting a Model with Many Variables:","text":"<ul> <li>Fitting the model involves finding the values of the coefficients \\(b_0, b_1, \\ldots, b_n\\) that minimize the difference between the actual observed values of the dependent variable and the values predicted by the model.</li> <li>The process is essentially the same as in simple linear regression, but with more coefficients to estimate.</li> </ul>"},{"location":"Machine%20Learning/42_training_models_meaning/#3-python-code-example_1","title":"3. Python Code Example:","text":"<ul> <li>Here's how you might fit a multiple linear regression model using Python:</li> </ul> <pre><code># Creating sample data with multiple variables\n# Independent variables (features)\nX = np.array([[1, 2], [2, 4], [3, 6], [4, 8], [5, 10]])  \n\n# Dependent variable (target)\ny = np.array([2, 4, 5, 4, 5])                            \n</code></pre> <pre><code># Creating a multiple linear regression model\nmultiple_linear_regression_model = LinearRegression()\n\n# Fitting the multiple_linear_regression_model to our data\nmultiple_linear_regression_model.fit(X, y)\n</code></pre> <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0LinearRegression?Documentation for LinearRegressioniFitted<pre>LinearRegression()</pre> <pre><code># Printing the coefficients (intercept and slopes) of the best-fitting line\nprint(\"Intercept (b0):\", multiple_linear_regression_model.intercept_)\nprint(\"Coefficients (b1, b2):\", multiple_linear_regression_model.coef_)\n</code></pre> <pre><code>Intercept (b0): 2.200000000000001\nCoefficients (b1, b2): [0.12 0.24]\n</code></pre> <ul> <li>In this code:<ul> <li><code>X</code> represents the independent variables (features), where each row is a data point and each column represents a different feature.</li> <li><code>y</code> represents the dependent variable (target).</li> <li>We create a <code>LinearRegression</code> model object.</li> <li>We fit the model to our data using the <code>.fit()</code> method.</li> <li>Finally, we print out the intercept and coefficients of the best-fitting line.</li> </ul> </li> </ul> <p>So, fitting a model with many variables involves finding the best parameters (coefficients) that describe the relationship between our input data (multiple independent variables) and the output we want to predict.</p> <p></p>"},{"location":"Machine%20Learning/42_training_models_meaning/#what-about-logistic-regression","title":"What about Logistic Regression?","text":"<p>Logistic regression is a type of regression analysis used for predicting the probability of a binary outcome based on one or more predictor variables. Here's how the fitting process works with logistic regression:</p>"},{"location":"Machine%20Learning/42_training_models_meaning/#1-understanding-logistic-regression","title":"1. Understanding Logistic Regression:","text":"<ul> <li>Logistic regression models the probability that a given input belongs to a particular category (binary classification problem).</li> <li>Instead of fitting a straight line or plane like in linear regression, logistic regression uses the logistic function (also known as the sigmoid function) to model the relationship between the independent variables and the probability of the binary outcome.</li> <li> <p>The logistic function is defined as:</p> <p>\\(\\(P(y=1 \\,|\\, X) = \\frac{1}{1 + e^{-(b_0 + b_1x_1 + ... + b_nx_n)}}\\)\\)    where \\(P(y=1 \\,|\\, X)\\) is the probability of the positive outcome given the input \\(X\\), \\(b_0\\) is the intercept, \\(b_1, b_2, ..., b_n\\) are the coefficients, and \\(x_1, x_2, ..., x_n\\) are the independent variables.</p> </li> </ul> <pre><code># z = b_0 + b_1x_1 + ... + b_nx_n\nsigmoid = lambda z: 1 / (1 + np.exp(-z))\n</code></pre> <pre><code>def sigmoid(z):\n    prob = 1 / (1 + np.exp(-z))\n    return prob\n</code></pre> <pre><code>print('z = 1000000:', sigmoid(z=1000000))\nprint('z = 0.0000001:', sigmoid(z=0.0000001))\n</code></pre> <pre><code>z = 1000000: 1.0\nz = 0.0000001: 0.500000025\n</code></pre>"},{"location":"Machine%20Learning/42_training_models_meaning/#2-fitting-a-logistic-regression-model","title":"2. Fitting a Logistic Regression Model:","text":"<ul> <li>Fitting the logistic regression model involves finding the values of the coefficients \\(b_0, b_1, ..., b_n\\) that maximize the likelihood of observing the given data under the assumed logistic regression model.</li> <li>This is typically done using optimization techniques such as gradient descent or other optimization algorithms.</li> <li>The goal is to find the set of coefficients that best separates the two classes or minimizes the error between the predicted probabilities and the actual binary outcomes in the training data.</li> </ul>"},{"location":"Machine%20Learning/42_training_models_meaning/#3-python-code-example_2","title":"3. Python Code Example:","text":"<ul> <li>Here's how you might fit a logistic regression model using Python with the <code>scikit-learn</code> library:</li> </ul> <pre><code># import the logistic regression model\nfrom sklearn.linear_model import LogisticRegression\n</code></pre> <pre><code># Creating sample data\nX = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9]])  # Independent variable\ny = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])             # Binary outcome (0 or 1)\n</code></pre> <pre><code># Creating a logistic regression model\nlogistic_regression_model = LogisticRegression()\n\n# Fitting the model to our data\nlogistic_regression_model.fit(X, y)\n</code></pre> <pre>LogisticRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0LogisticRegression?Documentation for LogisticRegressioniFitted<pre>LogisticRegression()</pre> <pre><code># Printing the intercept and coefficient(s) of the best-fitting logistic curve\nprint(\"Intercept (b0):\", logistic_regression_model.intercept_)\nprint(\"Coefficient (b1):\", logistic_regression_model.coef_)\n</code></pre> <pre><code>Intercept (b0): [-5.29559243]\nCoefficient (b1): [[1.17808562]]\n</code></pre> <ul> <li>In this code:<ul> <li><code>X</code> represents the independent variable.</li> <li><code>y</code> represents the binary outcome.</li> <li>We create a <code>LogisticRegression</code> model object.</li> <li>We fit the model to our data using the <code>.fit()</code> method.</li> <li>Finally, we print out the intercept and coefficient(s) of the best-fitting logistic curve.</li> </ul> </li> </ul> <pre><code># Predicted probabilities\nprobabilities = logistic_regression_model.predict_proba(X)\n</code></pre> <pre><code>probability_of_1 = logistic_regression_model.predict_proba(X)[:, 1]\n</code></pre> <pre><code># Plotting the logistic regression curve\nplt.scatter(X, y, color='blue', label='Data Points')\nplt.plot(probability_of_1, color='red', label='Logistic Regression Curve')\nplt.xlabel('X')\nplt.ylabel('Probability')\nplt.title('Logistic Regression')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p>So, fitting a logistic regression model involves finding the best parameters (coefficients) that describe the relationship between our input data and the probability of the binary outcome.</p> <pre><code>from dataidea import np,pd\n</code></pre> <pre><code>dataf = pd.DataFrame(\n    data={\n        'income': [1000, 2000, 3000],\n        'age': [10, 20, 30]\n    }\n)\n\ndataf\n</code></pre> income age 0 1000 10 1 2000 20 2 3000 30"},{"location":"Machine%20Learning/42_training_models_meaning/#congratulations","title":"Congratulations!What's on your mind? Put it in the comments!","text":"<p>If you reached here, you've explored the meaning of training a machine learning model of one and multiple independent variable models for Linear and Logistic Regression</p>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning/","title":"Unsupervised Learning","text":"<p>The following topics are covered in this tutorial:</p> <ul> <li> Overview of unsupervised learning algorithms in Scikit-learn.</li> <li> K Means clustering</li> </ul>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <p>Let's install the required libraries.</p> <pre><code># # uncomment and run this cell to install the packages and libraries\n# !pip install dataidea\n</code></pre>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning/#introduction-to-unsupervised-learning","title":"Introduction to Unsupervised Learning","text":"<p>Unsupervised machine learning refers to the category of machine learning techniques where models are trained on a dataset without labels. Unsupervised learning is generally use to discover patterns in data and reduce high-dimensional data to fewer dimensions. Here's how unsupervised learning fits into the landscape of machine learning algorithms(source):</p> <p></p> <p>Here are the topics in machine learning that we're studying in this course (source): </p> <p></p> <p>Scikit-learn offers the following cheatsheet to decide which model to pick for a given problem. Can you identify the unsupervised learning algorithms?</p> <p></p> <p>Here is a full list of unsupervised learning algorithms available in Scikit-learn: https://scikit-learn.org/stable/unsupervised_learning.html</p> <p></p>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning/#clustering","title":"Clustering","text":"<p>Clustering is the process of grouping objects from a dataset such that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (Wikipedia). Scikit-learn offers several clustering algorithms. You can learn more about them here: https://scikit-learn.org/stable/modules/clustering.html</p> <p>Here is a visual representation of clustering:</p> <p></p> <p>Here are some real-world applications of clustering:</p> <ul> <li> Customer segmentation</li> <li> Product recommendation</li> <li> Feature engineering</li> <li> Anomaly/fraud detection</li> <li> Taxonomy creation</li> </ul> <p>We'll use the Iris flower dataset to study some of the clustering algorithms available in <code>scikit-learn</code>. It contains various measurements for 150 flowers belonging to 3 different species.</p> <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\nsns.set_style('darkgrid')\n%matplotlib inline\n</code></pre> <p>Let's load the popular iris and penguin datasets. These datasets are already built in seaborn</p> <pre><code># load the iris dataset\niris_df = sns.load_dataset('iris')\niris_df.head()\n</code></pre> sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa <pre><code>sns.scatterplot(data=iris_df, x='sepal_length', y='petal_length', hue='species')\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Length')\nplt.xlabel('Sepal Length')\nplt.show()\n</code></pre> <p></p> <p>We'll attempt to cluster observations using numeric columns in the data. </p> <pre><code>numeric_cols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\nX = iris_df[numeric_cols]\nX.head()\n</code></pre> sepal_length sepal_width petal_length petal_width 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning/#k-means-clustering","title":"K Means Clustering","text":"<p>The K-means algorithm attempts to classify objects into a pre-determined number of clusters by finding optimal central points (called centroids) for each cluster. Each object is classifed as belonging the cluster represented by the closest centroid.</p> <p></p> <p>Here's how the K-means algorithm works:</p> <ol> <li>Pick K random objects as the initial cluster centers.</li> <li>Classify each object into the cluster whose center is closest to the point.</li> <li>For each cluster of classified objects, compute the centroid (mean).</li> <li>Now reclassify each object using the centroids as cluster centers.</li> <li>Calculate the total variance of the clusters (this is the measure of goodness).</li> <li>Repeat steps 1 to 6 a few more times and pick the cluster centers with the lowest total variance.</li> </ol> <p>Here's a video showing the above steps:</p> <p>Let's apply K-means clustering to the Iris dataset.</p> <pre><code>from sklearn.cluster import KMeans\n</code></pre> <pre><code>model = KMeans(n_clusters=3, random_state=42)\n# training the model\nmodel.fit(X)\n</code></pre> <pre>KMeans(n_clusters=3, random_state=42)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0KMeans?Documentation for KMeansiFitted<pre>KMeans(n_clusters=3, random_state=42)</pre> <p>We can check the cluster centers for each cluster.</p> <pre><code>model.cluster_centers_\n</code></pre> <pre><code>array([[6.85384615, 3.07692308, 5.71538462, 2.05384615],\n       [5.006     , 3.428     , 1.462     , 0.246     ],\n       [5.88360656, 2.74098361, 4.38852459, 1.43442623]])\n</code></pre> <p>We can now classify points using the model.</p> <pre><code># making predictions on X (clustering)\npreds = model.predict(X)\n</code></pre> <pre><code># assign each row to their cluster\nX['clusters'] = preds\n# looking at some samples\nX.tail(n=5)\n</code></pre> sepal_length sepal_width petal_length petal_width clusters 145 6.7 3.0 5.2 2.3 0 146 6.3 2.5 5.0 1.9 2 147 6.5 3.0 5.2 2.0 0 148 6.2 3.4 5.4 2.3 0 149 5.9 3.0 5.1 1.8 2 <p>Let's use seaborn and pyplot to visualize the clusters</p> <pre><code>sns.scatterplot(data=X, x='sepal_length', y='petal_length', hue=preds)\ncenters_x, centers_y = model.cluster_centers_[:,0], model.cluster_centers_[:,2]\nplt.plot(centers_x, centers_y, 'xb')\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Length')\nplt.xlabel('Sepal Length')\nplt.show()\n</code></pre> <p></p> <p>As you can see, K-means algorithm was able to classify (for the most part) different specifies of flowers into separate clusters. Note that we did not provide the \"species\" column as an input to <code>KMeans</code>.</p> <p>We can check the \"goodness\" of the fit by looking at <code>model.inertia_</code>, which contains the sum of squared distances of samples to their closest cluster center. Lower the inertia, better the fit.</p> <pre><code>model.inertia_\n</code></pre> <pre><code>78.8556658259773\n</code></pre> <p>Let's try creating 6 clusters.</p> <pre><code>model = KMeans(n_clusters=6, random_state=42)\n# fitting the model\nmodel.fit(X)\n# making predictions on X (clustering)\npreds = model.predict(X)\n# assign each row to their cluster\nX['clusters'] = preds\n# looking at some samples\nX.sample(n=5)\n</code></pre> sepal_length sepal_width petal_length petal_width clusters 37 4.9 3.6 1.4 0.1 5 54 6.5 2.8 4.6 1.5 0 2 4.7 3.2 1.3 0.2 1 59 5.2 2.7 3.9 1.4 2 56 6.3 3.3 4.7 1.6 0 <p>Let's visualize the clusters</p> <pre><code>sns.scatterplot(data=X, x='sepal_length', y='petal_length', hue=preds)\ncenters_x, centers_y = model.cluster_centers_[:,0], model.cluster_centers_[:,2]\nplt.plot(centers_x, centers_y, 'xb')\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Lenght')\nplt.xlabel('Sepal Length')\nplt.show()\n</code></pre> <p></p> <pre><code># Let's calculate the new model inertia\nmodel.inertia_\n</code></pre> <pre><code>50.560990643274856\n</code></pre>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning/#so-what-number-of-clusters-is-good-enough","title":"So, what number of clusters is good enough?","text":"<p>In most real-world scenarios, there's no predetermined number of clusters. In such a case, you can create a plot of \"No. of clusters\" vs \"Inertia\" to pick the right number of clusters.</p> <pre><code>options = range(2, 11)\ninertias = []\n\nfor n_clusters in options:\n    model = KMeans(n_clusters, random_state=42).fit(X)\n    inertias.append(model.inertia_)\n\nplt.plot(options, inertias, linestyle='-', marker='o')\nplt.title(\"No. of clusters vs. Inertia\")\nplt.xlabel('No. of clusters (K)')\nplt.ylabel('Inertia')\n</code></pre> <pre><code>Text(0, 0.5, 'Inertia')\n</code></pre> <p></p> <p>The chart is creates an \"elbow\" plot, and you can pick the number of clusters beyond which the reduction in inertia decreases sharply.</p> <p>Mini Batch K Means: The K-means algorithm can be quite slow for really large dataset. Mini-batch K-means is an iterative alternative to K-means that works well for large datasets. Learn more about it here: https://scikit-learn.org/stable/modules/clustering.html#mini-batch-kmeans</p> <p>EXERCISE: Perform clustering on the Mall customers dataset on Kaggle. Study the segments carefully and report your observations.</p>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning/#summary-and-references","title":"Summary and ReferencesWhat's on your mind? Put it in the comments!","text":"<p>Congratulations on finishing this lesson. The following topics were covered in this tutorial:</p> <ul> <li> Overview of unsupervised learning algorithms in Scikit-learn</li> <li> K Means Clustering</li> </ul> <p>Check out these resources to learn more:</p> <ul> <li>https://blog.dataidea.org/posts/cost-function-in-machine-learning</li> <li>https://www.coursera.org/learn/machine-learning</li> <li>https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/</li> <li>https://scikit-learn.org/stable/unsupervised_learning.html</li> <li>https://scikit-learn.org/stable/modules/clustering.html</li> </ul>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning2/","title":"Unsupervised Learning Part 2","text":"<p>In the previous section, we looked clustering and KMeans as model for cluster analysis. In this section, we will look at more clustering algorithms and Dimensionality reduction.</p> <ul> <li>Clustering algorithms: Hierarchical clustering etc.</li> <li>Dimensionality reduction (PCA) and manifold learning (t-SNE)</li> </ul> <p>Let's install the required libraries.</p> <pre><code># # uncomment and run this cell to install the packages and libraries\n# !pip install dataidea\n</code></pre>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning2/#introduction-to-unsupervised-learning","title":"Introduction to Unsupervised Learning","text":"<p>Unsupervised machine learning refers to the category of machine learning techniques where models are trained on a dataset without labels. Unsupervised learning is generally use to discover patterns in data and reduce high-dimensional data to fewer dimensions. Here's how unsupervised learning fits into the landscape of machine learning algorithms(source):</p>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning2/#clustering","title":"Clustering","text":"<p>Clustering is the process of grouping objects from a dataset such that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (Wikipedia). Scikit-learn offers several clustering algorithms. You can learn more about them here: https://scikit-learn.org/stable/modules/clustering.html</p> <p>Here is a visual representation of clustering:</p> <p></p> <p>Here are some real-world applications of clustering:</p> <ul> <li> Customer segmentation</li> <li> Product recommendation</li> <li> Feature engineering</li> <li> Anomaly/fraud detection</li> <li> Taxonomy creation</li> </ul> <p>We'll use the Iris flower dataset to study some of the clustering algorithms available in <code>scikit-learn</code>. It contains various measurements for 150 flowers belonging to 3 different species.</p> <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\nsns.set_style('darkgrid')\n%matplotlib inline\n</code></pre> <p>Let's load the popular iris and penguin datasets. These datasets are already built in seaborn</p> <pre><code># load the iris dataset\niris_df = sns.load_dataset('iris')\niris_df.head()\n</code></pre> sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa <pre><code># load the penguin dataset\nsns.get_dataset_names()\nping_df = sns.load_dataset('penguins')\nping_df.head()\n</code></pre> species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex 0 Adelie Torgersen 39.1 18.7 181.0 3750.0 Male 1 Adelie Torgersen 39.5 17.4 186.0 3800.0 Female 2 Adelie Torgersen 40.3 18.0 195.0 3250.0 Female 3 Adelie Torgersen NaN NaN NaN NaN NaN 4 Adelie Torgersen 36.7 19.3 193.0 3450.0 Female <pre><code>sns.scatterplot(data=ping_df, x='bill_length_mm', y='bill_depth_mm', hue='species')\nplt.title('Penguin Bill Depth against Bill Length per Species')\nplt.ylabel('Bill Depth')\nplt.xlabel('Bill Length')\nplt.show()\n</code></pre> <p></p> <pre><code>sns.scatterplot(data=iris_df, x='sepal_length', y='petal_length', hue='species')\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Lenght')\nplt.xlabel('Sepal Length')\nplt.show()\n</code></pre> <p></p> <p>We'll attempt to cluster observations using numeric columns in the data. </p> <pre><code>numeric_cols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\nX = iris_df[numeric_cols]\nX.head()\n</code></pre> sepal_length sepal_width petal_length petal_width 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning2/#dbscan","title":"DBSCAN","text":"<p>Density-based spatial clustering of applications with noise (DBSCAN) uses the density of points in a region to form clusters. It has two main parameters: \"epsilon\" and \"min samples\" using which it classifies each point as a core point, reachable point or noise point (outlier).</p> <p></p> <p>Here's a video explaining how the DBSCAN algorithm works: https://www.youtube.com/watch?v=C3r7tGRe2eI</p> <pre><code>from sklearn.cluster import DBSCAN\n</code></pre> <pre><code>model = DBSCAN(eps=1.1, min_samples=4)\nmodel.fit(X)\n</code></pre> <pre>DBSCAN(eps=1.1, min_samples=4)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0DBSCAN?Documentation for DBSCANiFitted<pre>DBSCAN(eps=1.1, min_samples=4)</pre> <p>In DBSCAN, there's no prediction step. It directly assigns labels to all the inputs.</p> <pre><code>model.labels_\n</code></pre> <pre><code>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1,\n       1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1,\n       1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2])\n</code></pre> <pre><code>sns.scatterplot(data=X, x='sepal_length', y='petal_length', hue=model.labels_)\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Lenght')\nplt.xlabel('Sepal Length')\nplt.show()\n</code></pre> <p></p>  EXERCISE<p>Try changing the values of `eps` and `min_samples` and observe how the number of clusters the classification changes..</p> <p>Here's how the results of DBSCAN and K Means differ:</p> <p></p>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning2/#hierarchical-clustering","title":"Hierarchical Clustering","text":"<p>Hierarchical clustering, as the name suggests, creates a hierarchy or a tree of clusters.</p> <p></p> <p>While there are several approaches to hierarchical clustering, the most common approach works as follows:</p> <ol> <li>Mark each point in the dataset as a cluster.</li> <li>Pick the two closest cluster centers without a parent and combine them into a new cluster. </li> <li>The new cluster is the parent cluster of the two clusters, and its center is the mean of all the points in the cluster.</li> <li>Repeat steps 2 and 3 till there's just one cluster left.</li> </ol> <p>Watch this video for a visual explanation of hierarchical clustering: https://www.youtube.com/watch?v=7xHsRkOdVwo</p>  EXERCISE<p>Implement hierarchical clustering for the Iris dataset using `scikit-learn`</p> <p>There are several other clustering algorithms in Scikit-learn. You can learn more about them and when to use them here: https://scikit-learn.org/stable/modules/clustering.html</p> <p>Let's save our work before continuing.</p>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning2/#dimensionality-reduction-and-manifold-learning","title":"Dimensionality Reduction and Manifold Learning","text":"<p>In machine learning problems, we often encounter datasets with a very large number of dimensions (features or columns). Dimensionality reduction techniques are used to reduce the number of dimensions or features within the data to a manageable or convenient number. </p> <p>Applications of dimensionality reduction:</p> <ul> <li>Reducing size of data without loss of information</li> <li>Training machine learning models efficiently</li> <li>Visualizing high-dimensional data in 2/3 dimensions</li> </ul>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning2/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":"<p>Principal component is a dimensionality reduction technique that uses linear projections of data to reduce their dimensions, while attempting to maximize the variance of data in the projection. Watch this video to learn how PCA works:</p> <p>Here's an example of PCA to reduce 2D data to 1D:</p> <p>Let's apply Principal Component Analysis to the Iris dataset.</p> <pre><code>iris_df = sns.load_dataset('iris')\niris_df.sample(n=5)\n</code></pre> sepal_length sepal_width petal_length petal_width species 86 6.7 3.1 4.7 1.5 versicolor 27 5.2 3.5 1.5 0.2 setosa 146 6.3 2.5 5.0 1.9 virginica 85 6.0 3.4 4.5 1.6 versicolor 30 4.8 3.1 1.6 0.2 setosa <pre><code>numeric_cols\n</code></pre> <pre><code>['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n</code></pre> <pre><code>from sklearn.decomposition import PCA\n</code></pre> <pre><code>pca = PCA(n_components=2)\npca.fit(iris_df[numeric_cols])\n</code></pre> <pre>PCA(n_components=2)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0PCA?Documentation for PCAiFitted<pre>PCA(n_components=2)</pre> <pre><code>transformed = pca.transform(iris_df[numeric_cols])\n</code></pre> <pre><code>sns.scatterplot(x=transformed[:,0], y=transformed[:,1], hue=iris_df['species'])\n</code></pre> <p></p> <p>As you can see, the PCA algorithm has done a very good job of separating different species of flowers using just 2 measures.</p> <p>EXERCISE: Apply Principal Component Analysis to a large high-dimensional dataset and train a machine learning model using the low-dimensional results. Observe the changes in the loss and training time for different numbers of target dimensions.</p> <p>Learn more about Principal Component Analysis here: https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html</p>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning2/#t-distributed-stochastic-neighbor-embedding-t-sne","title":"t-Distributed Stochastic Neighbor Embedding (t-SNE)","text":"<p>Manifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high. Scikit-learn provides many algorithms for manifold learning: https://scikit-learn.org/stable/modules/manifold.html . A commonly-used manifold learning technique is t-Distributed Stochastic Neighbor Embedding or t-SNE, used to visualize high dimensional data in one, two or three dimensions. </p> <p>Here's a visual representation of t-SNE applied to visualize 2 dimensional data in 1 dimension:</p> <p></p> <p>Here's a video explaning how t-SNE works: https://www.youtube.com/watch?v=NEaUSP4YerM</p> <pre><code>from sklearn.manifold import TSNE\n</code></pre> <pre><code>tsne = TSNE(n_components=2)\ntransformed = tsne.fit_transform(iris_df[numeric_cols])\n</code></pre> <pre><code>sns.scatterplot(x=transformed[:,0], y=transformed[:,1], hue=iris_df['species']);\n</code></pre> <p></p> <p>As you can see, the flowers from the same species are clustered very closely together. The relative distance between the species is also conveyed by the gaps between the clusters.</p> <p>EXERCISE: Use t-SNE to visualize the MNIST handwritten digits dataset.</p>"},{"location":"Machine%20Learning/43_sklearn-unsupervised-learning2/#summary-and-references","title":"Summary and References","text":"<p>The following topics were covered in this tutorial:</p> <ul> <li> Overview of unsupervised learning algorithms in Scikit-learn</li> <li> Clustering algorithms: DBScan, Hierarchical clustering etc.</li> <li> Dimensionality reduction (PCA) and manifold learning (t-SNE)</li> </ul> <p>Check out these resources to learn more:</p> <ul> <li>https://www.coursera.org/learn/machine-learning</li> <li>https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/</li> <li>https://scikit-learn.org/stable/unsupervised_learning.html</li> <li>https://scikit-learn.org/stable/modules/clustering.html</li> </ul>"},{"location":"Machine%20Learning/71_feature_selection/","title":"Feature Selection","text":""},{"location":"Machine%20Learning/71_feature_selection/#what-is-feature-selection","title":"What is Feature Selection","text":"<p>Feature selection is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested.</p> <p>Having irrelevant features in your data can decrease the accuracy of many models, especially linear algorithms like linear and logistic regression.</p> <p>Three benefits of performing feature selection before modeling your data are:</p> <ul> <li>Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.</li> <li>Improves Accuracy: Less misleading data means modeling accuracy improves.</li> <li>Reduces Training Time: Less data means that algorithms train faster.</li> </ul> <p>You can learn more about feature selection with scikit-learn in the article Feature selection.</p> <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom dataidea.datasets import loadDataset\n</code></pre> <pre><code>data = loadDataset('../assets/demo_cleaned.csv', \n                    inbuilt=False, file_type='csv')\ndata.head()\n</code></pre> age gender marital_status address income income_category job_category 0 55 f 1 12 72.0 3.0 3 1 56 m 0 29 153.0 4.0 3 2 24 m 1 4 26.0 2.0 1 3 45 m 0 9 76.0 4.0 2 4 44 m 1 17 144.0 4.0 3 <pre><code>data = pd.get_dummies(data, columns=['gender'], \n                      dtype='int', drop_first=True)\ndata.head(n=5)\n</code></pre> age marital_status address income income_category job_category gender_m 0 55 1 12 72.0 3.0 3 0 1 56 0 29 153.0 4.0 3 1 2 24 1 4 26.0 2.0 1 1 3 45 0 9 76.0 4.0 2 1 4 44 1 17 144.0 4.0 3 1  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Machine%20Learning/71_feature_selection/#univariate-feature-selection-techniques","title":"Univariate Feature Selection Techniques","text":"<p>Statistical tests can be used to select those features that have the strongest relationship with the output variable.</p> <p>The scikit-learn library provides the <code>SelectKBest</code> class that can be used with a suite of different statistical tests to select a specific number of features.</p> <p>Many different statistical tests can be used with this selection method. For example the ANOVA F-value method is appropriate for numerical inputs and categorical data. This can be used via the f_classif() function. We will select the 4 best features using this method in the example below.</p> <pre><code>from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import f_regression\n</code></pre> <p>Let's first separate our data into features ie <code>X</code> and outcome ie <code>y</code> as below.</p> <pre><code>X = data.drop('marital_status', axis=1)\ny = data.marital_status\n</code></pre>"},{"location":"Machine%20Learning/71_feature_selection/#numeric-or-continuous-features-with-categorical-outcome","title":"Numeric or Continuous Features with Categorical Outcome","text":"<p>Beginning with the numeric columns, let's find which of them best contributes to the outcome variable</p> <pre><code>X_numeric = X[['age', 'income', 'address']].copy()\n</code></pre> <pre><code># create a test object from SelectKBest\ntest = SelectKBest(score_func=f_classif, k=2)\n\n# fit the test object to the data\nfit = test.fit(X_numeric, y)\n\n# get the scores and features\nscores = fit.scores_\n\n# get the selected indices\nfeatures = fit.transform(X_numeric)\nselected_indices = test.get_support(indices=True)\n\n# print the scores and features\nprint('Feature Scores: ', scores)\nprint('Selected Features Indices: ', selected_indices)\n</code></pre> <pre><code>Feature Scores:  [1.34973748 1.73808724 0.02878244]\nSelected Features Indices:  [0 1]\n</code></pre> <p>This shows us that the best 2 features to use to differentiate between the groups in our outcome are <code>[0, 1]</code> ie <code>age</code> and <code>income</code></p>"},{"location":"Machine%20Learning/71_feature_selection/#numeric-features-with-numeric-outcome","title":"Numeric Features with Numeric Outcome","text":"<p>Let's selecting the input features <code>X</code>, and the output (outcome), <code>y</code></p> <pre><code># pick numeric input and output\nX = data[['age', 'address']].copy()\ny = data.income\n</code></pre> <p>We will still use the <code>SelectKBest</code> class but with our <code>score_func</code> as <code>f_regression</code> instead. </p> <pre><code>test = SelectKBest(score_func=f_regression, k=1)\n\n# Fit the test to the data\nfit = test.fit(X, y)\n\n# get scores\ntest_scores = fit.scores_\n\n# summarize selected features\nfeatures = fit.transform(X)\n\n# Get the selected feature indices\nselected_indices = fit.get_support(indices=True)\n\nprint('Feature Scores: ', test_scores)\nprint('Selected Features Indices: ', selected_indices)\n</code></pre> <pre><code>Feature Scores:  [25.18294605 23.43115992]\nSelected Features Indices:  [0]\n</code></pre> <p>Here, we can see that <code>age</code> is selected because it returns the higher f_statistic between the two features</p>"},{"location":"Machine%20Learning/71_feature_selection/#both-input-and-outcome-categorical","title":"Both input and outcome Categorical","text":"<p>Let's begin by selecting out only the categorical features to make our <code>X</code> set and set <code>y</code> as categorical</p> <pre><code># selecting categorical features\nX = data[['gender_m', 'income_category', 'job_category']].copy()\n\n# selecting categorical outcome\ny = data.marital_status\n</code></pre> <p>Now we shall again use <code>SelectKBest</code> but with the <code>score_func</code> as <code>chi2</code>.</p> <pre><code>from sklearn.feature_selection import chi2\n</code></pre> <pre><code>test = SelectKBest(score_func=chi2, k=2)\nfit = test.fit(X, y)\nscores = fit.scores_\nfeatures = fit.transform(X)\nselected_indices = fit.get_support(indices=True)\n\nprint('Feature Scores: ', scores)\nprint('Selected Features Indices: ', selected_indices)\n</code></pre> <pre><code>Feature Scores:  [0.20921223 0.61979264 0.00555967]\nSelected Features Indices:  [0 1]\n</code></pre> <p>Note: When using the Chi-Square (chi2) as the the score function for feature selection, you use the Chi-Square statistic.</p> <p>Again, we can see that the features with higher f_statistic scores have been selected</p> <ul> <li><code>f_classif</code> is most applicable where the input features are continuous and the outcome is categorical.</li> <li><code>f_regression</code> is most applicable where the input features are continuous and the outcome is continuous.</li> <li><code>chi2</code> is best for when the both the input and outcome are categorical.</li> </ul>"},{"location":"Machine%20Learning/71_feature_selection/#recursive-feature-elimination","title":"Recursive Feature Elimination","text":"<p>The Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain.</p> <p>It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.</p> <p>You can learn more about the RFE class in the scikit-learn documentation.</p>"},{"location":"Machine%20Learning/71_feature_selection/#logistic-regression","title":"Logistic Regression","text":"<pre><code>from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n</code></pre> <pre><code>X = data.drop('marital_status', axis=1)\ny = data.marital_status\n</code></pre> <pre><code># feature extraction\nmodel = LogisticRegression()\nrfe = RFE(model)\nfit = rfe.fit(X, y)\n\nprint(\"Num Features: %d\" % fit.n_features_)\nprint(\"Selected Features: %s\" % fit.support_)\nprint(\"Feature Ranking: %s\" % fit.ranking_)\n</code></pre> <pre><code>Num Features: 3\nSelected Features: [False False False  True  True  True]\nFeature Ranking: [2 3 4 1 1 1]\n</code></pre> <pre><code>X.head()\n</code></pre> age address income income_category job_category gender_m 0 55 12 72.0 3.0 3 0 1 56 29 153.0 4.0 3 1 2 24 4 26.0 2.0 1 1 3 45 9 76.0 4.0 2 1 4 44 17 144.0 4.0 3 1 <p>From the operation above, we can observe features that bring out the best from the <code>LogisticRegression</code> model ranked from <code>1</code> as most best and bigger numbers as less.</p>"},{"location":"Machine%20Learning/71_feature_selection/#feature-importance","title":"Feature Importance","text":"<p>Bagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features.</p> <p>In the example below we construct a ExtraTreesClassifier classifier for the Pima Indians onset of diabetes dataset. You can learn more about the ExtraTreesClassifier class in the scikit-learn API.</p>"},{"location":"Machine%20Learning/71_feature_selection/#extra-trees-classifier","title":"Extra Trees Classifier","text":"<pre><code>from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n</code></pre> <pre><code># feature extraction\nmodel = ExtraTreesClassifier()\nmodel.fit(X, y)\n\n# see the best features\nprint(model.feature_importances_)\n</code></pre> <pre><code>[0.29058207 0.24978811 0.26342117 0.06763375 0.08501043 0.04356447]\n</code></pre>"},{"location":"Machine%20Learning/71_feature_selection/#random-forest-classifier","title":"Random Forest Classifier","text":"<pre><code># feature extraction\nmodel = RandomForestClassifier()\nmodel.fit(X, y)\n\n# see the best features\nprint(model.feature_importances_)\n</code></pre> <pre><code>[0.28927782 0.2515934  0.28839236 0.06166801 0.06610313 0.04296528]\n</code></pre> <p>more about random forest here</p> What's on your mind? Put it in the comments!"},{"location":"Machine%20Learning/72_why_scaling/","title":"Why re-scaling (Iris)?","text":"<p>Let's use the Iris dataset, a popular dataset in machine learning. The Iris dataset consists of 150 samples of iris flowers, with each sample containing four features: sepal length, sepal width, petal length, and petal width. We'll demonstrate the effect of not scaling the features on K-means clustering.</p> <p>First, let's import the necessary libraries and load the Iris dataset:</p> <pre><code>import matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n</code></pre> <pre><code># Load the Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n</code></pre> <pre><code># picking the first row\nX[0]\n</code></pre> <pre><code>array([5.1, 3.5, 1.4, 0.2])\n</code></pre> <p>Next, let's perform K-means clustering on the original dataset without scaling the features:</p> <pre><code># Apply K-means clustering without scaling\nkmeans_unscaled = KMeans(n_clusters=2, random_state=42)\nkmeans_unscaled.fit(X)\n\n# Get the cluster centers and labels\ncentroids_unscaled = kmeans_unscaled.cluster_centers_\nlabels_unscaled = kmeans_unscaled.labels_\n</code></pre> <pre><code>from sklearn.metrics import silhouette_score\n</code></pre> <pre><code>silhouette_avg = silhouette_score(X, kmeans_unscaled.labels_)\nprint('Silhouette Average (Unscaled): ', silhouette_avg)\n</code></pre> <pre><code>Silhouette Average (Unscaled):  0.6810461692117462\n</code></pre> <p>The interpretation of the silhouette score is relatively straightforward:</p> <ul> <li> <p>Close to +1: A silhouette score near +1 indicates that the sample is far away from the neighboring clusters. This is a good indication of a well-clustered data point.</p> </li> <li> <p>Close to 0: A silhouette score near 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters. It could imply that the sample could belong to either one of the clusters.</p> </li> <li> <p>Close to -1: A silhouette score near -1 indicates that the samples might have been assigned to the wrong clusters. This could happen if the clusters overlap significantly or if the wrong number of clusters was chosen.</p> </li> </ul> <p>Read more about the Silhouette Score from here(sklearn)</p> <p>Now, let's visualize the clusters without scaling:</p> <pre><code># Visualize clusters without scaling\nplt.figure(figsize=(10, 6))\n\nplt.scatter(X[:, 0], X[:, 1], c=labels_unscaled, cmap='viridis', s=50)\nplt.scatter(centroids_unscaled[:, 0], centroids_unscaled[:, 1], marker='x', s=200, c='black')\n\nplt.xlabel('Sepal Length (cm)')\nplt.ylabel('Sepal Width (cm)')\nplt.title('K-means Clustering Without Scaling')\n\nplt.show()\n</code></pre> <p></p> <p>You'll notice that the clusters may not seem well-separated or meaningful. This is because the features of the Iris dataset have different scales, with sepal length ranging from approximately 4 to 8 cm, while sepal width ranges from approximately 2 to 4.5 cm.</p> <p>Now, let's repeat the process after scaling the features using StandardScaler:</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\n# Scale the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Apply K-means clustering on scaled features\nkmeans_scaled = KMeans(n_clusters=2, random_state=42)\nkmeans_scaled.fit(X_scaled)\n\n# Get the cluster centers and labels\ncentroids_scaled = kmeans_scaled.cluster_centers_\nlabels_scaled = kmeans_scaled.labels_\n</code></pre> <pre><code>silhouette_avg = silhouette_score(X, kmeans_scaled.labels_)\nprint('Silhouette Average (Scaled): ', silhouette_avg)\n</code></pre> <pre><code>Silhouette Average (Scaled):  0.6867350732769777\n</code></pre> <p>Visualize the clusters after scaling:</p> <pre><code># Visualize clusters with scaling\nplt.figure(figsize=(10, 6))\n\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_scaled, cmap='viridis', s=50)\nplt.scatter(centroids_scaled[:, 0], centroids_scaled[:, 1], marker='x', s=200, c='black')\n\nplt.xlabel('Sepal Length (scaled)')\nplt.ylabel('Sepal Width (scaled)')\nplt.title('K-means Clustering With Scaling')\n\nplt.show()\n</code></pre> <p></p> <p>You should see clearer and more meaningful clusters after scaling the features, demonstrating the importance of feature scaling for K-means clustering, especially when dealing with datasets with features of different scales.</p> What's on your mind? Put it in the comments!"},{"location":"Machine%20Learning/72_why_scaling_fpl/","title":"Why re-scale data (fpl)?","text":"<p>In this notebook, we'll use Kmeans clustering to demonstrate the importance of scaling data</p> <p>K-means clustering is an unsupervised machine learning algorithm used for partitioning a dataset into K distinct, non-overlapping clusters. The goal of K-means is to minimize the sum of squared distances between data points and their respective cluster centroids.</p> <p>Here's how the K-means algorithm works:</p> <ol> <li>Pick K random objects as the initial cluster centers.</li> <li>Classify each object into the cluster whose center is closest to the point.</li> <li>For each cluster of classified objects, compute the centroid (mean).</li> <li>Now reclassify each object using the centroids as cluster centers.</li> <li>Calculate the total variance of the clusters (this is the measure of goodness).</li> <li>Repeat steps 1 to 6 a few more times and pick the cluster centers with the lowest total variance.</li> </ol> <p>Here's a video showing the above steps: https://www.youtube.com/watch?v=4b5d3muPQmA</p> <p>First, let's import the necessary libraries and load the Iris dataset:</p> <pre><code>from dataidea.packages import pd, plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom dataidea.datasets import loadDataset\n</code></pre> <pre><code>fpl_data = loadDataset('fpl')\n</code></pre> <p>We can be able to load it like this because it inbuilt in the dataidea package</p> <p>Now let's pick out a few numeric columns that we might consider moving forward</p> <pre><code>fpl_sample = fpl_data[['Goals_Scored', 'Assists','Total_Points', \n                       'Minutes', 'Saves', 'Goals_Conceded', \n                       'Creativity', 'Influence'\n                      ]]\n</code></pre> <pre><code>fpl_sample.head()\n</code></pre> Goals_Scored Assists Total_Points Minutes Saves Goals_Conceded Creativity Influence 0 18 14 244 3101 0 36 1414.9 1292.6 1 23 14 242 3083 0 39 659.1 1318.2 2 22 6 231 3077 0 41 825.7 1056.0 3 17 11 228 3119 0 36 1049.9 1052.2 4 17 11 194 3052 0 50 371.0 867.2"},{"location":"Machine%20Learning/72_why_scaling_fpl/#clustering-without-scaling","title":"Clustering without Scaling","text":"<p>Next, let's perform K-means clustering on the original dataset without scaling the features:</p> <pre><code># Apply K-means clustering without scaling\nkmeans_unscaled = KMeans(n_clusters=4, random_state=42)\nkmeans_unscaled.fit(fpl_sample)\n\n# Get the cluster centers and labels\ncentroids_unscaled = kmeans_unscaled.cluster_centers_\nlabels_unscaled = kmeans_unscaled.labels_\n</code></pre> <p>Let's see the performance</p> <pre><code># Visualize clusters without scaling\nplt.figure(figsize=(10, 6))\n\nplt.scatter(fpl_sample.Assists, fpl_sample.Goals_Scored, c=labels_unscaled, cmap='viridis',)\n\nplt.show()\n</code></pre> <p></p> <p>You'll notice that the clusters may not seem well-separated or meaningful. This is because the features of the Iris dataset have different scales, </p>"},{"location":"Machine%20Learning/72_why_scaling_fpl/#clustering-after-scaling","title":"Clustering after Scaling","text":"<p>Now, let's repeat the process after scaling the features using StandardScaler:</p> <pre><code># Scale the features\nscaler = StandardScaler()\nfpl_sample_scaled = scaler.fit_transform(fpl_sample)\n\n# Transform scaled features back to DataFrame\nfpl_sample_scaled_dataframe = pd.DataFrame(fpl_sample_scaled, columns=fpl_sample.columns)\n\n# Apply K-means clustering on scaled features\nkmeans_scaled = KMeans(n_clusters=4, random_state=42)\nkmeans_scaled.fit(fpl_sample_scaled)\n\n# Get the cluster centers and labels\ncentroids_scaled = kmeans_scaled.cluster_centers_\nlabels_scaled = kmeans_scaled.labels_\n</code></pre> <pre><code># Visualize clusters without scaling\nplt.figure(figsize=(10, 6))\n\nplt.scatter(fpl_sample_scaled_dataframe.Assists, \n            fpl_sample_scaled_dataframe.Goals_Scored, \n            c=labels_scaled, cmap='viridis')\n\nplt.show()\n</code></pre> <p></p> <p>You should see clearer and more meaningful clusters after scaling the features, demonstrating the importance of feature scaling for K-means clustering, especially when dealing with datasets with features of different scales.</p>"},{"location":"Machine%20Learning/72_why_scaling_fpl/#take-away","title":"Take away","text":"<p>If the data doesn't follow a standard scale, meaning the features have different scales or variances, it can lead to some issues when applying K-means clustering:</p> <ol> <li> <p>Unequal feature influence: Features with larger scales or variances can dominate the clustering process. Since K-means relies on Euclidean distance, features with larger scales will contribute more to the distance calculation, potentially biasing the clustering results towards those features.</p> </li> <li> <p>Incorrect cluster shapes: K-means assumes that clusters are isotropic (spherical) and have similar variances along all dimensions. If the data has features with different scales, clusters may be stretched along certain dimensions, leading to suboptimal cluster assignments.</p> </li> <li> <p>Convergence speed: Features with larger scales can cause centroids to move more quickly towards areas with denser data, potentially affecting the convergence speed of the algorithm.</p> </li> </ol> <p>By scaling the data before clustering, you ensure that each feature contributes equally to the distance calculations, helping to mitigate the issues associated with different feature scales. This can lead to more accurate and reliable clustering results.</p> <p></p> <p> To be among the first to hear about future updates, simply enter your email below, follow us on   (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Machine%20Learning/73_normalization_and_standardization/","title":"Normalization vs Standardization","text":""},{"location":"Machine%20Learning/73_normalization_and_standardization/#introduction","title":"Introduction:","text":"<p>In data analysis and machine learning, preprocessing steps such as data normalization and standardization are crucial for improving the performance and interpretability of models.</p> <p>This Jupyter Notebook provides an overview of the importance of data normalization and standardization in preparing data for analysis and modeling.</p> <pre><code>import numpy as np\nimport dataidea as di\n</code></pre>"},{"location":"Machine%20Learning/73_normalization_and_standardization/#normalization","title":"Normalization","text":"<ol> <li>Normalization: Normalization typically refers to scaling numerical features to a common scale, often between 0 and 1. This is usually done by subtracting the minimum value and then dividing by the range (maximum - minimum). Normalization is useful when the distribution of the data does not follow a Gaussian distribution (Normal Distribution).</li> </ol> <pre><code># Data Normalization without libraries:\ndef minMaxScaling(data):\n    min_val = min(data)\n    max_val = max(data)\n\n    scaled_data = []\n    for value in data:\n        scaled = (value - min_val) / (max_val - min_val)\n        scaled_data.append(scaled)\n    return scaled_data\n</code></pre> <pre><code># Example data\ndata = np.array([10, 20, 30, 40, 50])\nnormalized_data = minMaxScaling(data)\nprint(\"Normalized data (Min-Max Scaling):\", normalized_data)\n</code></pre> <pre><code>Normalized data (Min-Max Scaling): [0.0, 0.25, 0.5, 0.75, 1.0]\n</code></pre> <pre><code>from sklearn.preprocessing import MinMaxScaler\n\n# Sample data\ndata = np.array([[1, 2], [3, 4], [5, 6]])\n\n# Create the scaler\nscaler = MinMaxScaler()\n\n# Fit the scaler to the data and transform the data\nnormalized_data = scaler.fit_transform(data)\n\nprint(\"Original data:\")\nprint(data)\nprint(\"\\nNormalized data:\")\nprint(normalized_data)\n</code></pre> <pre><code>Original data:\n[[1 2]\n [3 4]\n [5 6]]\n\nNormalized data:\n[[0.  0. ]\n [0.5 0.5]\n [1.  1. ]]\n</code></pre> <p>Let's now try on a real world dataset!</p> <pre><code>boston_data = di.loadDataset('boston')\nboston_data.head()\n</code></pre> CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18.0 2.31 0 0.538 6.575 65.2 4.0900 1 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0 0.469 6.421 78.9 4.9671 2 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0 0.469 7.185 61.1 4.9671 2 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0 0.458 6.998 45.8 6.0622 3 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0 0.458 7.147 54.2 6.0622 3 222.0 18.7 396.90 5.33 36.2 <pre><code>boston_scaler = MinMaxScaler()\nnormalized_data = boston_scaler.fit_transform(boston_data[['CRIM', 'AGE', 'TAX']])\nnp.set_printoptions(suppress=True)\nnormalized_data\n</code></pre> <pre><code>array([[0.        , 0.64160659, 0.20801527],\n       [0.00023592, 0.78269825, 0.10496183],\n       [0.0002357 , 0.59938208, 0.10496183],\n       ...,\n       [0.00061189, 0.90731205, 0.16412214],\n       [0.00116073, 0.88980433, 0.16412214],\n       [0.00046184, 0.80226571, 0.16412214]])\n</code></pre>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Machine%20Learning/73_normalization_and_standardization/#standardization","title":"Standardization","text":"<ol> <li>Standardization: Standardization, often implemented with a method like z-score standardization, transforms the data to have a mean of 0 and a standard deviation of 1. This means that the data will have a Gaussian distribution (if the original data had a Gaussian distribution). </li> </ol> <pre><code>def zScoreNormalization(data):\n    mean = sum(data) / len(data)\n    variance = sum((x - mean) ** 2 for x in data) / len(data)\n    std_dev = variance ** 0.5\n    standardized_data = [(x - mean) / std_dev for x in data]\n    return standardized_data\n</code></pre> <pre><code># Example data\ndata = [10, 20, 30, 40, 50]\nstandardized_data = zScoreNormalization(data)\nprint(\"Standardized data (Z-Score Normalization):\", standardized_data)\n</code></pre> <pre><code>Standardized data (Z-Score Normalization): [-1.414213562373095, -0.7071067811865475, 0.0, 0.7071067811865475, 1.414213562373095]\n</code></pre> <p>In Python, we can also typically use the <code>StandardScaler</code> from the <code>sklearn.preprocessing</code> module to standardize data.</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\n# Sample data\ndata = np.array([[1, 2, 3], [3, 4, 5], [5, 6, 7]])\n\n# Create the scaler\nscaler = StandardScaler()\n\n# Fit the scaler to the data and transform the data\nstandardized_data = scaler.fit_transform(data)\n\nprint(\"Original data:\")\nprint(data)\nprint(\"\\nStandardized data:\")\nprint(standardized_data)\n</code></pre> <pre><code>Original data:\n[[1 2 3]\n [3 4 5]\n [5 6 7]]\n\nStandardized data:\n[[-1.22474487 -1.22474487 -1.22474487]\n [ 0.          0.          0.        ]\n [ 1.22474487  1.22474487  1.22474487]]\n</code></pre> <pre><code>boston_scaler = StandardScaler()\nstandardized_data = boston_scaler.fit_transform(boston_data[['CRIM', 'AGE', 'TAX']])\nnp.set_printoptions(suppress=True)\nstandardized_data\n</code></pre> <pre><code>array([[-0.41978194, -0.12001342, -0.66660821],\n       [-0.41733926,  0.36716642, -0.98732948],\n       [-0.41734159, -0.26581176, -0.98732948],\n       ...,\n       [-0.41344658,  0.79744934, -0.80321172],\n       [-0.40776407,  0.73699637, -0.80321172],\n       [-0.41500016,  0.43473151, -0.80321172]])\n</code></pre>"},{"location":"Machine%20Learning/73_normalization_and_standardization/#importance","title":"Importance:","text":"<ol> <li>Data Normalization:</li> <li>Uniform Scaling: Ensures all features are scaled to a similar range, preventing dominance by features with larger scales.</li> <li>Improved Convergence: Facilitates faster convergence in optimization algorithms by making the loss surface more symmetric.</li> <li> <p>Interpretability: Easier interpretation as values are on a consistent scale, aiding in comparison and understanding of feature importance.</p> </li> <li> <p>Data Standardization:</p> </li> <li>Mean Centering: Transforms data to have a mean of 0 and a standard deviation of 1, simplifying interpretation of coefficients in linear models.</li> <li>Handling Different Scales: Useful when features have different scales or units, making them directly comparable.</li> <li>Reducing Sensitivity to Outliers: Less affected by outliers compared to normalization, leading to more robust models.</li> <li>Maintaining Information: Preserves relative relationships between data points without altering the distribution shape.</li> </ol>"},{"location":"Machine%20Learning/73_normalization_and_standardization/#which-one","title":"Which one?What's on your mind? Put it in the comments!","text":"<p>The choice between normalization and standardization depends on your data and the requirements of your analysis. Here are some guidelines to help you decide:</p> <ol> <li>Normalization:</li> <li>Use normalization when the scale of features is meaningful and should be preserved.</li> <li>Normalize data when you're working with algorithms that require input features to be on a similar scale, such as algorithms using distance metrics like k-nearest neighbors or clustering algorithms like K-means.</li> <li> <p>If the distribution of your data is not Gaussian and you want to scale the features to a fixed range, normalization might be a better choice.</p> </li> <li> <p>Standardization:</p> </li> <li>Use standardization when the distribution of your data is Gaussian or when you're unsure about the distribution.</li> <li>Standardization is less affected by outliers compared to normalization, making it more suitable when your data contains outliers.</li> <li>If you're working with algorithms that assume your data is normally distributed, such as linear regression, logistic regression, standardization is typically preferred.</li> </ol> <p>In some cases, you might experiment with both approaches and see which one yields better results for your specific dataset and analysis. Additionally, it's always a good practice to understand your data and the underlying assumptions of the algorithms you're using to make informed decisions about data preprocessing techniques.</p>"},{"location":"Machine%20Learning/75_bonus/","title":"ANOVA for Feature Selection","text":"<p>In this notebook, we demonstrate how ANOVA (Analysis of Variance) can be used to identify better features for machine learning models. We'll use the Fantasy Premier League (FPL) dataset to show how ANOVA helps in selecting features that best differentiate categories.</p> <pre><code># Uncomment the line below if you need to install the dataidea package\n# !pip install -U dataidea\n</code></pre> <p>First, we'll import the necessary packages: <code>scipy</code> for performing ANOVA, <code>dataidea</code> for loading the FPL dataset, and <code>SelectKBest</code> from <code>scikit-learn</code> for univariate feature selection based on statistical tests.</p> <pre><code>import scipy as sp\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport dataidea as di\n</code></pre> <p>Let's load the FPL dataset and preview the top 5 rows.</p> <pre><code># Load FPL dataset\nfpl = di.loadDataset('fpl') \n\n# Preview the top 5 rows\nfpl.head(n=5)\n</code></pre> First_Name Second_Name Club Goals_Scored Assists Total_Points Minutes Saves Goals_Conceded Creativity Influence Threat Bonus BPS ICT_Index Clean_Sheets Red_Cards Yellow_Cards Position 0 Bruno Fernandes MUN 18 14 244 3101 0 36 1414.9 1292.6 1253 36 870 396.2 13 0 6 MID 1 Harry Kane TOT 23 14 242 3083 0 39 659.1 1318.2 1585 40 880 355.9 12 0 1 FWD 2 Mohamed Salah LIV 22 6 231 3077 0 41 825.7 1056.0 1980 21 657 385.8 11 0 0 MID 3 Heung-Min Son TOT 17 11 228 3119 0 36 1049.9 1052.2 1046 26 777 315.2 13 0 0 MID 4 Patrick Bamford LEE 17 11 194 3052 0 50 371.0 867.2 1512 26 631 274.6 10 0 3 FWD <p>ANOVA helps us determine if there's a significant difference between the means of different groups. We use it to select features that best show the difference between categories. Features with higher F-statistics are preferred.</p>"},{"location":"Machine%20Learning/75_bonus/#anova-for-goals-scored","title":"ANOVA for Goals Scored","text":"<p>We will create groups of goals scored by each player position (forwards, midfielders, defenders, and goalkeepers) and run an ANOVA test.</p> <pre><code># Create groups of goals scored for each player position\nforwards_goals = fpl[fpl.Position == 'FWD']['Goals_Scored']\nmidfielders_goals = fpl[fpl.Position == 'MID']['Goals_Scored']\ndefenders_goals = fpl[fpl.Position == 'DEF']['Goals_Scored']\ngoalkeepers_goals = fpl[fpl.Position == 'GK']['Goals_Scored']\n\n# Perform the ANOVA test for the groups\nf_statistic, p_value = sp.stats.f_oneway(forwards_goals, midfielders_goals, defenders_goals, goalkeepers_goals)\nprint(\"F-statistic:\", f_statistic)\nprint(\"p-value:\", p_value)\n</code></pre> <pre><code>F-statistic: 33.281034594400445\np-value: 3.9257634156019246e-20\n</code></pre> <p>We observe an F-statistic of <code>33.281</code> and a p-value of <code>3.926e-20</code>, indicating a significant difference at multiple confidence levels.</p>"},{"location":"Machine%20Learning/75_bonus/#anova-for-assists","title":"ANOVA for Assists","text":"<p>Next, we'll create groups for assists and run an ANOVA test.</p> <pre><code># Create groups of assists for each player position\nforwards_assists = fpl[fpl.Position == 'FWD']['Assists']\nmidfielders_assists = fpl[fpl.Position == 'MID']['Assists']\ndefenders_assists = fpl[fpl.Position == 'DEF']['Assists']\ngoalkeepers_assists = fpl[fpl.Position == 'GK']['Assists']\n\n# Perform the ANOVA test for the groups\nf_statistic, p_value = sp.stats.f_oneway(forwards_assists, midfielders_assists, defenders_assists, goalkeepers_assists)\nprint(\"F-statistic:\", f_statistic)\nprint(\"p-value:\", p_value)\n</code></pre> <pre><code>F-statistic: 19.263717036430815\np-value: 5.124889288362087e-12\n</code></pre> <p>We observe an F-statistic of <code>19.264</code> and a p-value of <code>5.125e-12</code>, again indicating significance.</p>"},{"location":"Machine%20Learning/75_bonus/#comparing-results","title":"Comparing Results","text":"<p>Both features show significant F-statistics, but goals scored has a higher value, indicating it is a better feature for differentiating player positions.</p>"},{"location":"Machine%20Learning/75_bonus/#using-selectkbest-for-feature-selection","title":"Using SelectKBest for Feature Selection","text":"<p>We can also use <code>SelectKBest</code> from <code>scikit-learn</code> to automate this process.</p> <pre><code># Use scikit-learn's SelectKBest (with f_classif)\ntest = SelectKBest(score_func=f_classif, k=1)\n\n# Fit the model to the data\nfit = test.fit(fpl[['Goals_Scored', 'Assists']], fpl.Position)\n\n# Get the F-statistics\nscores = fit.scores_\n\n# Select the best feature\nfeatures = fit.transform(fpl[['Goals_Scored', 'Assists']])\n\n# Get the indices of the selected features (optional)\nselected_indices = test.get_support(indices=True)\n\n# Print indices and scores\nprint('Feature Scores: ', scores)\nprint('Selected Features Indices: ', selected_indices)\n</code></pre> <pre><code>Feature Scores:  [33.28103459 19.26371704]\nSelected Features Indices:  [0]\n</code></pre> <p>The <code>0th</code> feature (Goals Scored) is selected as the best feature based on the F-statistics.</p>"},{"location":"Machine%20Learning/75_bonus/#summary","title":"Summary","text":"<p>In this notebook, we demonstrated how to use ANOVA for feature selection in the Fantasy Premier League dataset. By comparing the F-statistics of different features, we identified that 'Goals Scored' is a more significant feature than 'Assists' for differentiating player positions. Using <code>SelectKBest</code> from <code>scikit-learn</code>, we confirmed that 'Goals Scored' is the best feature among the two. This method can be applied to other datasets and features to enhance the performance of machine learning models.</p> What's on your mind? Put it in the comments!"},{"location":"Machine%20Learning/76_handling_missing_data/","title":"Handling Missing Data","text":""},{"location":"Machine%20Learning/76_handling_missing_data/#introduction","title":"Introduction:","text":"<p>Missing data is a common hurdle in data analysis, impacting the reliability of insights drawn from datasets. Python offers a range of solutions to address this issue, some of which we discussed in the earlier weeks. In this notebook, we look into the top four missing data imputation methods:</p> <ul> <li>SimpleImputer</li> <li>KNNImputer</li> <li>IterativeImputer </li> <li>Datawig</li> </ul> <p>We'll explore these essential techniques, using sklearn and the weather dataset.</p>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code># install the libraries for this demonstration\n# ! pip install -U dataidea\n</code></pre> <pre><code>import pandas as pd\nimport dataidea as di\n</code></pre> <p><code>loadDataset</code> allows us to load datasets inbuilt in the dataidea library</p> <pre><code>weather = di.loadDataset('weather') \nweather\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 NaN 9.0 Sunny 2 05/01/2017 28.0 NaN Snow 3 06/01/2017 NaN 7.0 NaN 4 07/01/2017 32.0 NaN Rain 5 08/01/2017 NaN NaN Sunny 6 09/01/2017 NaN NaN NaN 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <pre><code>weather.isna().sum()\n</code></pre> <pre><code>day            0\ntemperature    4\nwindspead      4\nevent          2\ndtype: int64\n</code></pre> <p>Let's demonstrate how to use the top three missing data imputation methods\u2014SimpleImputer, KNNImputer, and IterativeImputer\u2014using the simple weather dataset.</p> <pre><code># select age from the data\ntemp_wind = weather[['temperature', 'windspead']].copy()\n</code></pre> <pre><code>temp_wind_imputed = temp_wind.copy()\n</code></pre> <p></p>"},{"location":"Machine%20Learning/76_handling_missing_data/#simpleimputer-from-scikit-learn","title":"SimpleImputer from scikit-learn:","text":"<ul> <li>Usage: SimpleImputer is a straightforward method for imputing missing values by replacing them with a constant, mean, median, or most frequent value along each column.</li> <li>Pros:<ul> <li>Easy to use and understand.</li> <li>Can handle both numerical and categorical data.</li> <li>Offers flexibility with different imputation strategies.</li> </ul> </li> <li>Cons:<ul> <li>It doesn't consider relationships between features.</li> <li>May not be the best choice for datasets with complex patterns of missingness.</li> </ul> </li> <li>Example:</li> </ul> <pre><code>from sklearn.impute import SimpleImputer\n\nsimple_imputer = SimpleImputer(strategy='mean')\ntemp_wind_simple_imputed = simple_imputer.fit_transform(temp_wind)\n\ntemp_wind_simple_imputed_df = pd.DataFrame(temp_wind_simple_imputed, columns=temp_wind.columns)\n</code></pre> <p>Let's have a look at the outcome</p> <pre><code>temp_wind_simple_imputed_df\n</code></pre> temperature windspead 0 32.0 6.0 1 33.2 9.0 2 28.0 8.4 3 33.2 7.0 4 32.0 8.4 5 33.2 8.4 6 33.2 8.4 7 34.0 8.0 8 40.0 12.0"},{"location":"Machine%20Learning/76_handling_missing_data/#exercise","title":"Exercise:","text":"<ol> <li>Try out the SimpleImputer with different imputation strategies like mode, constant </li> <li>Choose and try some imputation techniques on categorical data</li> </ol>"},{"location":"Machine%20Learning/76_handling_missing_data/#knnimputer-from-scikit-learn","title":"KNNImputer from scikit-learn:","text":"<ul> <li>Usage: <ul> <li>KNNImputer imputes missing values using k-nearest neighbors, replacing them with the mean value of the nearest neighbors.</li> <li>You can read more about the KNNImputer from the sklearn official docs site</li> </ul> </li> <li>Pros:<ul> <li>Considers relationships between features, making it suitable for datasets with complex patterns of missingness.</li> <li>Can handle both numerical and categorical data.</li> </ul> </li> <li>Cons:<ul> <li>Computationally expensive for large datasets.</li> <li>Requires careful selection of the number of neighbors (k).</li> </ul> </li> </ul> Note!<p>By default, the KNNImputer uses 'nan' values as missing data and the 'nan_euclidean' metric to calculate the distances between values.</p> <ul> <li>Example:</li> </ul> <pre><code>from sklearn.impute import KNNImputer\n\nknn_imputer = KNNImputer(n_neighbors=2)\ntemp_wind_knn_imputed = knn_imputer.fit_transform(temp_wind)\n\ntemp_wind_knn_imputed_df = pd.DataFrame(temp_wind_knn_imputed, columns=temp_wind.columns)\n</code></pre> <p>If we take a look at the outcome</p> <pre><code>weather\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 NaN 9.0 Sunny 2 05/01/2017 28.0 NaN Snow 3 06/01/2017 NaN 7.0 NaN 4 07/01/2017 32.0 NaN Rain 5 08/01/2017 NaN NaN Sunny 6 09/01/2017 NaN NaN NaN 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny <p></p>"},{"location":"Machine%20Learning/76_handling_missing_data/#filling-a-single-column-independently-using-the-knnimputer","title":"Filling a single column independently using the <code>KNNImputer</code>","text":"<p>To use the KNNImputer for a single independ column, you can use the index as the other column instead, this will result into equal euclidean distances resulting into the use of the physical neighbors in the data table.</p> <pre><code>from sklearn.impute import KNNImputer\n\nknn_imputer = KNNImputer(n_neighbors=2)\nwindspead_imputed = knn_imputer.fit_transform(weather[['windspead']].reset_index())\n\nwindspead_imputed\n</code></pre> <pre><code>array([[ 0. ,  6. ],\n       [ 1. ,  9. ],\n       [ 2. ,  8. ],\n       [ 3. ,  7. ],\n       [ 4. ,  8. ],\n       [ 5. ,  7.5],\n       [ 6. , 10. ],\n       [ 7. ,  8. ],\n       [ 8. , 12. ]])\n</code></pre> <pre><code># we can fill it back in the weather data\nweather['windspead'] = windspead_imputed[:, 1]\n\n# now looking at the data\nweather\n</code></pre> day temperature windspead event 0 01/01/2017 32.0 6.0 Rain 1 04/01/2017 NaN 9.0 Sunny 2 05/01/2017 28.0 8.0 Snow 3 06/01/2017 NaN 7.0 NaN 4 07/01/2017 32.0 8.0 Rain 5 08/01/2017 NaN 7.5 Sunny 6 09/01/2017 NaN 10.0 NaN 7 10/01/2017 34.0 8.0 Cloudy 8 11/01/2017 40.0 12.0 Sunny"},{"location":"Machine%20Learning/76_handling_missing_data/#exercise_1","title":"Exercise","text":"<ul> <li>Try out the KNNImputer with different numbers of neighbors and compare the results</li> <li>Findo out how to use KNNImputer to fill categorical data</li> </ul>"},{"location":"Machine%20Learning/76_handling_missing_data/#iterativeimputer-from-scikit-learn","title":"IterativeImputer from scikit-learn:","text":"<ul> <li>Usage: IterativeImputer models each feature with missing values as a function of other features and uses that estimate for imputation. It iteratively estimates the missing values.</li> <li>Pros:<ul> <li>Takes into account relationships between features, making it suitable for datasets with complex missing patterns.</li> <li>More robust than SimpleImputer for handling missing data.</li> </ul> </li> <li>Cons:<ul> <li>Can be computationally intensive and slower than SimpleImputer.</li> <li>Requires careful tuning of model parameters.</li> </ul> </li> <li>Example:</li> </ul> <pre><code>from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\niterative_imputer = IterativeImputer()\ntemp_wind_iterative_imputed = iterative_imputer.fit_transform(temp_wind)\n\ntemp_wind_iterative_imputed_df = pd.DataFrame(temp_wind_iterative_imputed, columns=temp_wind.columns)\n\ntemp_wind_iterative_imputed_df\n</code></pre> temperature windspead 0 32.000000 6.0 1 33.967053 9.0 2 28.000000 8.0 3 31.410210 7.0 4 32.000000 8.0 5 32.049421 7.5 6 35.245474 10.0 7 34.000000 8.0 8 40.000000 12.0 <p>You can also choose an estimator of your choice, let's try a <code>Linear Regression</code> model</p> <pre><code>from sklearn.linear_model import LinearRegression\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# set estimator to an instance of a model\niterative_imputer = IterativeImputer(estimator=LinearRegression())\ntemp_wind_iterative_imputed = iterative_imputer.fit_transform(temp_wind)\n\ntemp_wind_iterative_imputed_df = pd.DataFrame(temp_wind_iterative_imputed, columns=temp_wind.columns)\n\ntemp_wind_iterative_imputed_df\n</code></pre> temperature windspead 0 32.000000 6.0 1 34.125000 9.0 2 28.000000 8.0 3 31.041667 7.0 4 32.000000 8.0 5 31.812500 7.5 6 35.666667 10.0 7 34.000000 8.0 8 40.000000 12.0 <p></p>"},{"location":"Machine%20Learning/76_handling_missing_data/#datawig","title":"Datawig:","text":"<p>Datawig is a library specifically designed for imputing missing values in tabular data using deep learning models.</p> <pre><code># import datawig\n\n# # Impute missing values\n# df_imputed = datawig.SimpleImputer.complete(weather)\n</code></pre> <p>These top imputation methods offer different trade-offs in terms of computational complexity, handling of missing data patterns, and ease of use. The choice between them depends on the specific characteristics of the dataset and the requirements of the analysis.</p>"},{"location":"Machine%20Learning/76_handling_missing_data/#homework","title":"HomeworkWhat's on your mind? Put it in the comments!","text":"<ul> <li>Try out these techniques for categorical data</li> </ul> <p> Don't miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it's easy and safe. </p>"},{"location":"Machine%20Learning/80_classification_metrics/","title":"SciKit-Learn Classification Metrics","text":"<p>In scikit-learn, classification metrics are essential tools to evaluate the performance of a classification model. </p> <p>They provide insights into how well the model is performing and where it may need improvements. </p> <p>Here are some commonly used classification metrics along with examples of how to implement them using scikit-learn:</p> <pre><code>import pandas as pd\n</code></pre> <pre><code># True labels\ny_true = [0, 1, 1, 0, 1]\n# Predicted labels\ny_pred = [1, 1, 0, 0, 1]\n</code></pre>"},{"location":"Machine%20Learning/80_classification_metrics/#accuracy","title":"Accuracy:","text":"<ul> <li>Accuracy measures the ratio of correctly predicted instances to the total instances.</li> <li>Formula:</li> </ul> <p><code>Accuracy = Number of Correct Predictions / Total Number of Predictions</code></p> <p></p>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code>from sklearn.metrics import accuracy_score\n\n# Calculate accuracy\naccuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\nprint(\"Accuracy:\", accuracy)\n</code></pre> <pre><code>Accuracy: 0.6\n</code></pre>"},{"location":"Machine%20Learning/80_classification_metrics/#precision","title":"Precision:","text":"<ul> <li>Precision measures the ability of the classifier not to label as positive a sample that is negative.</li> <li>Precision is simply the models ability to not make a mistake</li> <li>Formula:</li> </ul> <p><code>Precision = True Positives / (True Positives + False Positives)</code></p> <p></p> <pre><code>from sklearn.metrics import precision_score\n\n# Calculate precision\nprecision = precision_score(y_true, y_pred)\nprint(\"Precision:\", precision)\n</code></pre> <pre><code>Precision: 0.6666666666666666\n</code></pre>"},{"location":"Machine%20Learning/80_classification_metrics/#recall-also-known-as-sensitivity-or-true-positive-rate","title":"Recall (also known as Sensitivity or True Positive Rate):","text":"<ul> <li>Recall measures the ability of the classifier to find all the positive samples.</li> <li>Formula:</li> </ul> <p><code>Recall = True Positives / (True Positives + False Negatives)</code></p> <p></p> <pre><code>from sklearn.metrics import recall_score\n\n# Calculate recall\nrecall = recall_score(y_true, y_pred)\nprint(\"Recall:\", recall)\n</code></pre> <pre><code>Recall: 0.6666666666666666\n</code></pre>"},{"location":"Machine%20Learning/80_classification_metrics/#f1-score","title":"F1 Score:","text":"<ul> <li>F1 Score is the harmonic mean of precision and recall. </li> <li>It provides a balance between precision and recall.</li> <li>Formula:</li> </ul> <p><code>F1 Score = (2 x Precision x Recall) / (Precision + Recall)</code></p> <pre><code>from sklearn.metrics import f1_score\n\n# Calculate F1 Score\nf1 = f1_score(y_true, y_pred)\nprint(\"F1 Score:\", f1)\n</code></pre> <pre><code>F1 Score: 0.6666666666666666\n</code></pre> <p>In classification tasks, metrics like precision, recall, and F1-score are commonly used to evaluate the performance of a model. When dealing with multi-class classification, you often need a way to aggregate these metrics across all classes. Three common methods for doing this are micro-average, macro-average, and weighted average.</p>"},{"location":"Machine%20Learning/80_classification_metrics/#micro-average","title":"Micro-average:","text":"<ul> <li>Calculate metrics globally by counting the total true positives, false negatives, and false positives.</li> <li>This method gives equal weight to each individual prediction, regardless of class imbalance.</li> </ul> <pre><code># Calculate micro-average\nmicro_precision = precision_score(y_true, y_pred, average='micro')\nmicro_recall = recall_score(y_true, y_pred, average='micro')\nmicro_f1 = f1_score(y_true, y_pred, average='micro')\n\nprint('Micro Precision:', micro_precision)\nprint('Micro Recall:', micro_recall)\nprint('Micro F1:', micro_f1)\n</code></pre> <pre><code>Micro Precision: 0.6\nMicro Recall: 0.6\nMicro F1: 0.6\n</code></pre>"},{"location":"Machine%20Learning/80_classification_metrics/#macro-average","title":"Macro-average:","text":"<ul> <li>Calculate metrics for each class individually and then average them.</li> <li>This method treats all classes equally, giving each class the same weight.</li> <li>To obtain macro-averaged precision, recall, and F1-score:<ul> <li>Calculate precision, recall, and F1-score for each class.</li> <li>Average the precision, recall, and F1-score across all classes.</li> </ul> </li> </ul> <pre><code># Calculate macro-average\nmacro_precision = precision_score(y_true, y_pred, average='macro')\nmacro_recall = recall_score(y_true, y_pred, average='macro')\nmacro_f1 = f1_score(y_true, y_pred, average='macro')\n\nprint('Macro Precision:', macro_precision)\nprint('Macro Recall:', macro_recall)\nprint('Macro F1:', macro_f1)\n</code></pre> <pre><code>Macro Precision: 0.5833333333333333\nMacro Recall: 0.5833333333333333\nMacro F1: 0.5833333333333333\n</code></pre>"},{"location":"Machine%20Learning/80_classification_metrics/#weighted-average","title":"Weighted average:","text":"<ul> <li>Calculate metrics for each class individually and then average them, weighted by the number of true instances for each class.</li> <li>This method considers class imbalance by giving more weight to classes with more instances.</li> <li>To obtain weighted-averaged precision, recall, and F1-score:<ul> <li>Calculate precision, recall, and F1-score for each class.</li> <li>Weighted average is calculated as the <code>sum of (metric * class_weight) / total_number_of_samples</code>, where class_weight is the ratio of the number of true instances in the given class to the total number of true instances.</li> </ul> </li> </ul> <pre><code># Calculate weighted-average\nweighted_precision = precision_score(y_true, y_pred, average='weighted')\nweighted_recall = recall_score(y_true, y_pred, average='weighted')\nweighted_f1 = f1_score(y_true, y_pred, average='weighted')\n\nprint('Weighted Precision:', weighted_precision)\nprint('Weighted Recall:', weighted_recall)\nprint('Weighted F1:', weighted_f1)\n</code></pre> <pre><code>Weighted Precision: 0.6\nWeighted Recall: 0.6\nWeighted F1: 0.6\n</code></pre> <p>Weighted_precision:</p> <p><code>(Precision_A * N_A + Precision_B * N_B, ... , Precision_n * N_n) / (N_A + N_B + ... + N_n)</code></p> <ul> <li>Micro-average is useful when overall performance across all classes is important</li> <li>Macro-average is helpful when you want to evaluate the model's performance on smaller classes equally.</li> <li>Weighted average is suitable when you want to account for class imbalance.</li> </ul>"},{"location":"Machine%20Learning/80_classification_metrics/#the-classification-report","title":"The Classification Report","text":"<p>The classification report in scikit-learn provides a comprehensive summary of different classification metrics for each class in the dataset. It includes precision, recall, F1-score, and support (the number of true instances for each label). Here's how you can generate a classification report:</p> <pre><code>from sklearn.metrics import classification_report\n\n# Generate classification report\nclass_report_dict = classification_report(y_true, y_pred, output_dict=True)\nclass_report_df = pd.DataFrame(class_report_dict).transpose()\n\nprint(\"Classification Report:\\n\")\nclass_report_df\n</code></pre> <pre><code>Classification Report:\n</code></pre> precision recall f1-score support 0 0.500000 0.500000 0.500000 2.0 1 0.666667 0.666667 0.666667 3.0 accuracy 0.600000 0.600000 0.600000 0.6 macro avg 0.583333 0.583333 0.583333 5.0 weighted avg 0.600000 0.600000 0.600000 5.0 <pre><code>(0.666667 + 0.5)/2\n</code></pre> <pre><code>0.5833335\n</code></pre>"},{"location":"Machine%20Learning/80_classification_metrics/#confusion-matrix","title":"Confusion Matrix:","text":"<ul> <li>A confusion matrix is a table that is often used to describe the performance of a classification model.</li> <li>It presents a summary of the model's predictions on the classification problem, showing correct predictions as well as types of errors made.</li> </ul> <pre><code>from sklearn.metrics import confusion_matrix\nimport pandas as pd\n\n# Calculate confusion matrix\nconf_matrix = confusion_matrix(y_true, y_pred)\n\nconf_matrix = pd.DataFrame(conf_matrix, index=[1, 0], columns=[1, 0])\n# print(\"Confusion Matrix:\\n\", conf_matrix)\n\nconf_matrix\n</code></pre> 1 0 1 1 1 0 1 2 <p>These are just a few of the many classification metrics available in scikit-learn. Depending on your specific problem and requirements, you may want to explore other metrics as well.</p> <p>Understanding these metrics and how they are computed can provide valuable insights into the performance of a classification model and help in making informed decisions about its improvement.</p> What's on your mind? Put it in the comments!"},{"location":"Machine%20Learning/81_regression_metrics/","title":"Regression Metrics","text":"<p>In regression tasks, the goal is to predict continuous numerical values. Scikit-learn provides several metrics to evaluate the performance of regression models. In this notebook, we will look at the following</p> <ul> <li>Mean Absolute Error</li> <li>Mean Square Error</li> <li>Root Mean Squared Error</li> <li>R-Squared (Coefficient of Determination)</li> </ul>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code># True labels\ny_true = [2.5, 3.7, 5.1, 4.2, 6.8]\n# Predicted labels\ny_pred = [2.3, 3.5, 4.9, 4.0, 6.5]\n</code></pre>"},{"location":"Machine%20Learning/81_regression_metrics/#mean-absolute-error-mae","title":"Mean Absolute Error (MAE):","text":"<ul> <li>MAE measures the average absolute errors between predicted values and actual values.</li> <li>Imagine you're trying to hit a target with darts. The MAE is like calculating the average distance between where your darts hit and the bullseye. You just sum up how far each dart landed from the center (without caring if it was too short or too far) and then find the average. The smaller the MAE, the closer your predictions are to the actual values.</li> <li>Formula: \\(\\(\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_{\\text{true}} - y_{\\text{pred}}|\\)\\)</li> </ul> <pre><code>from sklearn.metrics import mean_absolute_error\n\n# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_true, y_pred)\nprint(\"Mean Absolute Error (MAE):\", mae)\n</code></pre> <pre><code>Mean Absolute Error (MAE): 0.21999999999999992\n</code></pre>"},{"location":"Machine%20Learning/81_regression_metrics/#mean-squared-error-mse","title":"Mean Squared Error (MSE):","text":"<ul> <li>MSE measures the average of the squares of the errors between predicted values and actual values.</li> <li>This is similar to MAE, but instead of just adding up the distances, you square them before averaging. Squaring makes bigger differences more noticeable (by making them even bigger), so MSE penalizes larger errors more than smaller ones.</li> <li>Formula: \\(\\(\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{true}} - y_{\\text{pred}})^2\\)\\)</li> </ul> <pre><code>from sklearn.metrics import mean_squared_error\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_true, y_pred)\nprint(\"Mean Squared Error (MSE):\", mse)\n</code></pre> <pre><code>Mean Squared Error (MSE): 0.04999999999999997\n</code></pre>"},{"location":"Machine%20Learning/81_regression_metrics/#root-mean-squared-error-rmse","title":"Root Mean Squared Error (RMSE):","text":"<ul> <li>RMSE is the square root of the MSE, providing a more interpretable scale since it's in the same units as the target variable.</li> <li>It's just like MSE, but we take the square root of the result. This brings the error back to the same scale as the original target variable, which makes it easier to interpret. RMSE gives you an idea of how spread out your errors are in the same units as your data.</li> <li>Formula: \\(\\(\\text{RMSE} = \\sqrt{\\text{MSE}}\\)\\)</li> </ul> <pre><code>from sklearn.metrics import root_mean_squared_error\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse = root_mean_squared_error(y_true, y_pred,)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\n</code></pre> <pre><code>Root Mean Squared Error (RMSE): 0.2236067977499789\n</code></pre>"},{"location":"Machine%20Learning/81_regression_metrics/#r-squared-coefficient-of-determination","title":"R-squared (Coefficient of Determination)**:What's on your mind? Put it in the comments!","text":"<ul> <li>R-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variables.</li> <li>This tells you how well your model's predictions match the actual data compared to a simple average. If R-squared is 1, it means your model perfectly predicts the target variable. If it's 0, it means your model is no better than just predicting the mean of the target variable. So, the closer R-squared is to 1, the better your model fits the data.</li> <li> <p>Formula: \\(\\(R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_{\\text{true}} - y_{\\text{pred}})^2}{\\sum_{i=1}^{n} (y_{\\text{true}} - \\bar{y}_{\\text{true}})^2}\\)\\)</p> </li> <li> <p>where $$ \\bar{y}_{\\text{true}}$$ is the mean of the observed data.</p> </li> </ul> <pre><code>from sklearn.metrics import r2_score\n\n# Calculate R-squared (Coefficient of Determination)\nr2 = r2_score(y_true, y_pred)\nprint(\"R-squared (R2 Score):\", r2)\n</code></pre> <pre><code>R-squared (R2 Score): 0.975896644812958\n</code></pre> <p>Understanding these metrics can help you assess the performance of your regression model and make necessary adjustments to improve its accuracy.</p>"},{"location":"Machine%20Learning/82_sklearn_pipeline/","title":"Pipeline","text":""},{"location":"Machine%20Learning/82_sklearn_pipeline/#pipeline","title":"Pipeline","text":"<p>A pipeline is a series of data processing steps that are chained together sequentially. Each step in the pipeline typically performs some transformation on the data. In this notebook we are gonna be looking at the following steps in a pipeline: </p> <ul> <li>Preprocessing</li> <li>Feature extraction</li> <li>Feature selection</li> <li>Model fitting</li> </ul>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Machine%20Learning/82_sklearn_pipeline/#lets-redefine-a-model","title":"Let's redefine a model","text":"<p>In week 4, we introduced ourselves to Machine Learning Concepts, in week 5 we learned some statistical tests and we applied them in week 7 to find the best feature and transform them to efficient forms. In this section, we will build on top of those concepts to redefine what a Machine Learning model is and hence come up with a more efficient way of developing good Machine Learning models</p> <p>First, let's install the <code>dataidea</code> package, which will help us with loading packages and datasets with much more ease</p> <pre><code>## install the version of dataidea used for this notebook\n# !pip install --upgrade dataidea\n</code></pre> <pre><code># Let's import some packages\n\nimport numpy as np\nimport pandas as pd\nimport dataidea as di\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsRegressor\n</code></pre> <pre><code># loading the data set\n\ndata = di.loadDataset('boston')\n</code></pre> <p>The Boston Housing Dataset</p> <p>The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of  Boston MA. The following describes the dataset columns:</p> <ul> <li>CRIM - per capita crime rate by town</li> <li>ZN - proportion of residential land zoned for lots over 25,000 sq.ft.</li> <li>INDUS - proportion of non-retail business acres per town.</li> <li>CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)</li> <li>NOX - nitric oxides concentration (parts per 10 million)</li> <li>RM - average number of rooms per dwelling</li> <li>AGE - proportion of owner-occupied units built prior to 1940</li> <li>DIS - weighted distances to five Boston employment centres</li> <li>RAD - index of accessibility to radial highways</li> <li>TAX - full-value property-tax rate per $10,000</li> <li>PTRATIO - pupil-teacher ratio by town</li> <li>B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town</li> <li>LSTAT - % lower status of the population</li> <li>MEDV - Median value of owner-occupied homes in $1000's</li> </ul> <pre><code># looking at the top part\n\ndata.head()\n</code></pre> CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18.0 2.31 0 0.538 6.575 65.2 4.0900 1 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0 0.469 6.421 78.9 4.9671 2 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0 0.469 7.185 61.1 4.9671 2 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0 0.458 6.998 45.8 6.0622 3 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0 0.458 7.147 54.2 6.0622 3 222.0 18.7 396.90 5.33 36.2"},{"location":"Machine%20Learning/82_sklearn_pipeline/#training-our-first-model","title":"Training our first model","text":"<p>In week 4, we learned that to train a model (for supervised machine learning), we needed to have a set of X variables (also called independent, predictor etc), and then, we needed a y variable (also called dependent, outcome, predicted etc).</p> <pre><code># Selecting our X set and y\n\nX = data.drop('MEDV', axis=1)\ny = data.MEDV\n</code></pre> <p>Now we can train the <code>KNeighborsRegressor</code> model, this model naturally makes predictions by averaging the values of the 5 neighbors to the point that you want to predict</p> <pre><code># lets traing the KNeighborsRegressor\n\nknn_model = KNeighborsRegressor() # instanciate the model class\nknn_model.fit(X, y) # train the model on X, y\nscore = knn_model.score(X, y) # obtain the model score on X, y\npredicted_y = knn_model.predict(X) # make predictions on X\n\nprint('score:', score)\n</code></pre> <pre><code>score: 0.716098217736928\n</code></pre> <p>Now lets go ahead and try to visualize the performance of the model. The scatter plot is of true labels against predicted labels. Do you think the model is doing well?</p> <pre><code># looking at the performance \n\nplt.scatter(y, predicted_y)\nplt.title('Model Performance')\nplt.xlabel('Predicted y')\nplt.ylabel('True y')\nplt.show()\n</code></pre> <p></p>"},{"location":"Machine%20Learning/82_sklearn_pipeline/#some-feature-selection","title":"Some feature selection.","text":"<p>Feature selection is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested.</p> <p>In week 7 we learned that having irrelevant features in your data can decrease the accuracy of many models. In the code below, we try to find out the best features that best contribute to the outcome variable</p> <pre><code>from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression # score function for ANOVA with continuous outcome\n</code></pre> <pre><code># lets do some feature selection using ANOVA\n\ndata_num = data.drop(['CHAS','RAD'], axis=1) # dropping categorical\nX = data_num.drop(\"MEDV\", axis=1) \ny = data_num.MEDV\n\n# using SelectKBest\ntest_reg = SelectKBest(score_func=f_regression, k=6) \nfit_boston = test_reg.fit(X, y)\nindexes = fit_boston.get_support(indices=True)\n\nprint(fit_boston.scores_)\nprint(indexes)\n</code></pre> <pre><code>[ 89.48611476  75.2576423  153.95488314 112.59148028 471.84673988\n  83.47745922  33.57957033 141.76135658 175.10554288  63.05422911\n 601.61787111]\n[ 2  3  4  7  8 10]\n</code></pre> <p>From above, we can see from above that the best features for now are those in indexes <code>[ 2  3  4  7  8 10]</code> in the <code>num_data</code> dataset. Lets find them in the <code>data</code> and add on our categorical ones to set up our new X set</p> <pre><code>data_num.sample()\n</code></pre> CRIM ZN INDUS NOX RM AGE DIS TAX PTRATIO B LSTAT MEDV 211 0.37578 0.0 10.59 0.489 5.404 88.6 3.665 277.0 18.6 395.24 23.98 19.3 <pre><code># redifining the X set \n\nnew_X = data[['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT', 'CHAS','RAD']]\n</code></pre>"},{"location":"Machine%20Learning/82_sklearn_pipeline/#training-our-second-model","title":"Training our second model","text":"<p>Now that we have selected out the features, X that we thing best contribute to the outcome, let's retrain our machine learning model and see if we are gonna get better results</p> <pre><code>knn_model = KNeighborsRegressor()\nknn_model.fit(new_X, y)\nnew_score = knn_model.score(new_X, y)\nnew_predicted_y = knn_model.predict(new_X)\n\nprint('Feature selected score:', new_score)\n</code></pre> <pre><code>Feature selected score: 0.8324963639640872\n</code></pre> <p>The model seems to score better with a significant increment in accuracy from <code>0.71</code> to <code>0.83</code>. As like last time, let us try to visualize the difference in performance</p> <pre><code>plt.scatter(y, new_predicted_y)\nplt.title('Model Performance')\nplt.xlabel('New Predicted y')\nplt.ylabel('True y')\nplt.show()\n</code></pre> <p></p> <p>I do not know about you, but as for me, I notice a meaningful improvement in the predictions made from the model considering this scatter plot</p>"},{"location":"Machine%20Learning/82_sklearn_pipeline/#transforming-the-data","title":"Transforming the data","text":"<p>In week 7, we learned some advantages of scaling our data like:</p> <ul> <li>preventing dominance by features with larger scales</li> <li>faster convergence in optimization algorithms</li> <li> <p>reduce the impact of outliers</p> </li> <li> <p>Numeric Transformer: </p> </li> </ul> <pre><code># importing the StandardScaler\nfrom sklearn.preprocessing import StandardScaler\n\n# instanciating the StandardScaler\nscaler = StandardScaler() \n</code></pre> <p>This initializes a <code>StandardScaler</code> which standardizes features by removing the mean and scaling to unit variance. It's applied to numeric columns to ensure they are on a similar scale.</p> <pre><code>standardized_data_num = scaler.fit_transform(\n    data[['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT']]\n    ) # rescaline numeric features\nstandardized_data_num_df = pd.DataFrame(\n    standardized_data_num, \n    columns=['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT'] \n    ) # converting the standardized to dataframe\n</code></pre> <pre><code>standardized_data_num_df.head()\n</code></pre> INDUS NOX RM TAX PTRATIO LSTAT 0 -1.287909 -0.144217 0.413672 -0.666608 -1.459000 -1.075562 1 -0.593381 -0.740262 0.194274 -0.987329 -0.303094 -0.492439 2 -0.593381 -0.740262 1.282714 -0.987329 -0.303094 -1.208727 3 -1.306878 -0.835284 1.016303 -1.106115 0.113032 -1.361517 4 -1.306878 -0.835284 1.228577 -1.106115 0.113032 -1.026501 <ol> <li>Categorical Transformer: </li> </ol> <pre><code># importing the OneHotEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n# instanciating the OneHotEncoder\none_hot_encoder = OneHotEncoder()\n</code></pre> <p>This initializes a <code>OneHotEncoder</code> which converts categorical variables into a format that can be provided to ML algorithms to do a better job in prediction.</p> <pre><code>encoded_data_cat = one_hot_encoder.fit_transform(data[['CHAS', 'RAD']])\nencoded_data_cat_array = encoded_data_cat.toarray()\n# Get feature names\nfeature_names = one_hot_encoder.get_feature_names_out(['CHAS', 'RAD'])\n\nencoded_data_cat_df = pd.DataFrame(\n    data=encoded_data_cat_array,\n    columns=feature_names\n)\n</code></pre> <pre><code>encoded_data_cat_df.head()\n</code></pre> CHAS_0 CHAS_1 RAD_1 RAD_2 RAD_3 RAD_4 RAD_5 RAD_6 RAD_7 RAD_8 RAD_24 0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 4 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 <p>Let us add that to the new X and form a standardized new X set</p> <pre><code>transformed_new_X = pd.concat(\n    [standardized_data_num_df, encoded_data_cat_df], \n    axis=1\n    )\n</code></pre> <pre><code>transformed_new_X.head()\n</code></pre> INDUS NOX RM TAX PTRATIO LSTAT CHAS_0 CHAS_1 RAD_1 RAD_2 RAD_3 RAD_4 RAD_5 RAD_6 RAD_7 RAD_8 RAD_24 0 -1.287909 -0.144217 0.413672 -0.666608 -1.459000 -1.075562 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 -0.593381 -0.740262 0.194274 -0.987329 -0.303094 -0.492439 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2 -0.593381 -0.740262 1.282714 -0.987329 -0.303094 -1.208727 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3 -1.306878 -0.835284 1.016303 -1.106115 0.113032 -1.361517 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 4 -1.306878 -0.835284 1.228577 -1.106115 0.113032 -1.026501 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0"},{"location":"Machine%20Learning/82_sklearn_pipeline/#training-our-third-model","title":"Training our third model","text":"<p>Now that we have the right features selected and standardized, let us train a new model and see if it is gonna beat the first models</p> <pre><code>knn_model = KNeighborsRegressor()\nknn_model.fit(transformed_new_X, y)\nnew_transformed_score = knn_model.score(transformed_new_X, y)\nnew_predicted_y = knn_model.predict(transformed_new_X)\n\nprint('Transformed score:', new_transformed_score)\n</code></pre> <pre><code>Transformed score: 0.8734524530397529\n</code></pre> <p>This new models appears to do better than the earlier ones with an improvement in score from <code>0.83</code> to <code>0.87</code>. Do you think this is now a good model?</p>"},{"location":"Machine%20Learning/82_sklearn_pipeline/#the-pipeline","title":"The Pipeline","text":"<p>It turns out the above efforts to improve the performance of the model add extra steps to pass before you can have a good model. But what about if we can put together the transformers into on object we do most of that stuff.</p> <p>The sklearn <code>Pipeline</code> allows you to sequentially apply a list of transformers to preprocess the data and, if desired, conclude the sequence with a final predictor for predictive modeling.</p> <p>Intermediate steps of the pipeline must be \u2018transforms\u2019, that is, they must implement fit and transform methods. The final estimator only needs to implement fit.</p> <p>Let us build a model that puts together transformation and modelling steps into one <code>pipeline</code> object</p> <pre><code># lets import the Pipeline from sklearn\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n</code></pre> <pre><code>numeric_cols = ['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT']\ncategorical_cols = ['CHAS', 'RAD']\n</code></pre> <pre><code># Preprocessing steps\nnumeric_transformer = StandardScaler()\ncategorical_transformer = OneHotEncoder()\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('numerical', numeric_transformer, numeric_cols),\n        ('categorical', categorical_transformer, categorical_cols)\n    ])\n\n# Pipeline\npipe = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', KNeighborsRegressor())\n])\n\n# display pipe\npipe\n</code></pre> <pre>Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('numerical', StandardScaler(),\n                                                  ['INDUS', 'NOX', 'RM', 'TAX',\n                                                   'PTRATIO', 'LSTAT']),\n                                                 ('categorical',\n                                                  OneHotEncoder(),\n                                                  ['CHAS', 'RAD'])])),\n                ('model', KNeighborsRegressor())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('numerical', StandardScaler(),\n                                                  ['INDUS', 'NOX', 'RM', 'TAX',\n                                                   'PTRATIO', 'LSTAT']),\n                                                 ('categorical',\n                                                  OneHotEncoder(),\n                                                  ['CHAS', 'RAD'])])),\n                ('model', KNeighborsRegressor())])</pre> \u00a0preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformer<pre>ColumnTransformer(transformers=[('numerical', StandardScaler(),\n                                 ['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO',\n                                  'LSTAT']),\n                                ('categorical', OneHotEncoder(),\n                                 ['CHAS', 'RAD'])])</pre> numerical<pre>['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT']</pre> \u00a0StandardScaler?Documentation for StandardScaler<pre>StandardScaler()</pre> categorical<pre>['CHAS', 'RAD']</pre> \u00a0OneHotEncoder?Documentation for OneHotEncoder<pre>OneHotEncoder()</pre> \u00a0KNeighborsRegressor?Documentation for KNeighborsRegressor<pre>KNeighborsRegressor()</pre> <p>The code above sets up a data preprocessing and modeling pipeline using the <code>scikit-learn</code> library. Let's break down each part:</p>"},{"location":"Machine%20Learning/82_sklearn_pipeline/#combine-preprocessing-steps","title":"Combine Preprocessing Steps","text":"<ol> <li>ColumnTransformer:    <pre><code>preprocessor = ColumnTransformer(\n    transformers=[\n        ('numerical', numeric_transformer, numeric_cols),\n        ('categorical', categorical_transformer, categorical_cols)\n    ])\n</code></pre></li> <li>The <code>ColumnTransformer</code> is used to apply different preprocessing steps to different columns of the data. It combines the <code>numeric_transformer</code> for numeric columns and the <code>categorical_transformer</code> for categorical columns.</li> <li><code>numeric_cols</code> and <code>categorical_cols</code> are lists containing the names of numeric and categorical columns respectively.</li> </ol>"},{"location":"Machine%20Learning/82_sklearn_pipeline/#pipeline_1","title":"Pipeline","text":"<ol> <li>Pipeline Setup:    <pre><code>pipe = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', KNeighborsRegressor())\n])\n</code></pre></li> <li>A <code>Pipeline</code> is created which sequentially applies a list of transforms and a final estimator. </li> <li>The <code>preprocessor</code> step applies the <code>ColumnTransformer</code> defined earlier.</li> <li>The <code>model</code> step applies a <code>KNeighborsRegressor</code>, which is a regression model that predicts the target variable based on the k-nearest neighbors in the feature space.</li> </ol> <p>Now we can instead fit the Pipeline and use it for making predictions</p> <pre><code># Fit the pipeline\npipe.fit(new_X, y)\n\n# Score the pipeline\npipe_score = pipe.score(new_X, y)\n\n# Predict using the pipeline\npipe_predicted_y = pipe.predict(new_X)\n\nprint('Pipe Score:', pipe_score)\n</code></pre> <pre><code>Pipe Score: 0.8734524530397529\n</code></pre> <pre><code>plt.scatter(y, pipe_predicted_y)\nplt.title('Pipe Performance')\nplt.xlabel('Pipe Predicted y')\nplt.ylabel('True y')\nplt.show()\n</code></pre> <p></p> <p>We can observe that the model still gets the same good score, but now all the transformation steps, both on numeric and categorical variables are in a single pipeline object together with the model.</p> What's on your mind? Put it in the comments!"},{"location":"Machine%20Learning/83_GridSearchCV/","title":"GridSearchCV","text":""},{"location":"Machine%20Learning/83_GridSearchCV/#what-is-gridsearchcv","title":"What is GridSearchCV","text":"<p><code>GridSearchCV</code> is a method in the scikit-learn library, which is a popular machine learning library in Python. It's used for hyperparameter optimization, which involves searching for the best set of hyperparameters for a machine learning model. In this notebook, we'll learn:</p> <ul> <li>how to setup a proper GridSearchCV and</li> <li>how to use it for hyperparameter optimization.</li> </ul>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Machine%20Learning/83_GridSearchCV/#lets-import-some-packages","title":"Let's import some packages","text":"<p>We begin by importing necessary packages and modules. The <code>KNeighborsRegressor</code> model is imported from the <code>sklearn.neighbors</code> module.  KNN regression is a non-parametric method that, in an intuitive manner, approximates the association between independent variables and the continuous outcome by averaging the observations in the same neighbourhood.  Read more about the KNN Regressor from this link </p> <pre><code># Let's import some packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport dataidea as di\nfrom sklearn.neighbors import KNeighborsRegressor\n</code></pre>"},{"location":"Machine%20Learning/83_GridSearchCV/#lets-import-necessary-components-from-sklearn","title":"Let's import necessary components from sklearn","text":"<p>We import essential components from <code>sklearn</code>, including <code>Pipeline</code>, which we'll use to create a <code>pipe</code> as from the previous section, <code>ColumnTransformer</code>, <code>StandardScaler</code>, and <code>OneHotEncoder</code> which we'll use to transform the numeric and categorical columns respectively to be good for modelling.</p> <pre><code># lets import the Pipeline from sklearn\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\n</code></pre>"},{"location":"Machine%20Learning/83_GridSearchCV/#loading-the-dataset","title":"Loading the dataset","text":"<p>We load the dataset named boston using the <code>loadDataset</code> function, which is inbuilt in the dataidea package. The loaded dataset is stored in the variable <code>data</code>. </p> <pre><code># loading the data set\ndata = di.loadDataset('boston')\n</code></pre> <pre><code># looking at the top part\ndata.head()\n</code></pre> CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18.0 2.31 0 0.538 6.575 65.2 4.0900 1 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0 0.469 6.421 78.9 4.9671 2 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0 0.469 7.185 61.1 4.9671 2 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0 0.458 6.998 45.8 6.0622 3 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0 0.458 7.147 54.2 6.0622 3 222.0 18.7 396.90 5.33 36.2 Reveal more about the Boston dataset  The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of [ Boston MA](http://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html). The following describes the dataset columns:  * CRIM - per capita crime rate by town * ZN - proportion of residential land zoned for lots over 25,000 sq.ft. * INDUS - proportion of non-retail business acres per town. * CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise) * NOX - nitric oxides concentration (parts per 10 million) * RM - average number of rooms per dwelling * AGE - proportion of owner-occupied units built prior to 1940 * DIS - weighted distances to five Boston employment centres * RAD - index of accessibility to radial highways * TAX - full-value property-tax rate per \\$10,000 * PTRATIO - pupil-teacher ratio by town * B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town * LSTAT - % lower status of the population * MEDV - Median value of owner-occupied homes in \\$1000's"},{"location":"Machine%20Learning/83_GridSearchCV/#selecting-features-x-and-target-variable-y","title":"Selecting features (X) and target variable (y)","text":"<p>We separate the features (X) from the target variable (y). Features are stored in <code>X</code>, excluding the target variable 'MEDV', which is stored in <code>y</code>.</p> <pre><code># Selecting our X set and y\nX = data.drop('MEDV', axis=1)\ny = data.MEDV\n</code></pre>"},{"location":"Machine%20Learning/83_GridSearchCV/#defining-numeric-and-categorical-columns","title":"Defining numeric and categorical columns","text":"<p>We define lists of column names representing numeric and categorical features in the dataset. We identified these columns as the best features from the previous section of this week. Click here to learn about feature selection</p> <pre><code># numeric columns\nnumeric_cols = [\n    'INDUS', 'NOX', 'RM', \n    'TAX', 'PTRATIO', 'LSTAT'\n    ]\n\n# categorical columns\ncategorical_cols = ['CHAS', 'RAD']\n</code></pre>"},{"location":"Machine%20Learning/83_GridSearchCV/#preprocessing-steps","title":"Preprocessing steps","text":"<p>We define transformers for preprocessing numeric and categorical features. <code>StandardScaler</code> is used for standardizing numeric features, while <code>OneHotEncoder</code> is used for one-hot encoding categorical features. These transformers are applied to respective feature types using <code>ColumnTransformer</code> as we learned in the previous section.</p> <pre><code># Preprocessing steps\nnumeric_transformer = StandardScaler()\ncategorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\n# Combine preprocessing steps\ncolumn_transformer = ColumnTransformer(\n    transformers=[\n        ('numeric', numeric_transformer, numeric_cols),\n        ('categorical', categorical_transformer, categorical_cols)\n    ])\n</code></pre>"},{"location":"Machine%20Learning/83_GridSearchCV/#defining-the-pipeline","title":"Defining the pipeline","text":"<p>We construct a machine learning pipeline using <code>Pipeline</code>. The pipeline consists of preprocessing steps (defined in <code>column_transformer</code>) and a <code>KNeighborsRegressor</code> model with 10 neighbors. Learn about Machine Learning Pipelining here</p> <pre><code># Pipeline\npipe = Pipeline([\n    ('column_transformer', column_transformer),\n    ('model', KNeighborsRegressor(n_neighbors=10))\n])\n\npipe\n</code></pre> <pre>Pipeline(steps=[('column_transformer',\n                 ColumnTransformer(transformers=[('numeric', StandardScaler(),\n                                                  ['INDUS', 'NOX', 'RM', 'TAX',\n                                                   'PTRATIO', 'LSTAT']),\n                                                 ('categorical',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['CHAS', 'RAD'])])),\n                ('model', KNeighborsRegressor(n_neighbors=10))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0Pipeline?Documentation for PipelineiNot fitted<pre>Pipeline(steps=[('column_transformer',\n                 ColumnTransformer(transformers=[('numeric', StandardScaler(),\n                                                  ['INDUS', 'NOX', 'RM', 'TAX',\n                                                   'PTRATIO', 'LSTAT']),\n                                                 ('categorical',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['CHAS', 'RAD'])])),\n                ('model', KNeighborsRegressor(n_neighbors=10))])</pre> \u00a0column_transformer: ColumnTransformer?Documentation for column_transformer: ColumnTransformer<pre>ColumnTransformer(transformers=[('numeric', StandardScaler(),\n                                 ['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO',\n                                  'LSTAT']),\n                                ('categorical',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 ['CHAS', 'RAD'])])</pre> numeric<pre>['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT']</pre> \u00a0StandardScaler?Documentation for StandardScaler<pre>StandardScaler()</pre> categorical<pre>['CHAS', 'RAD']</pre> \u00a0OneHotEncoder?Documentation for OneHotEncoder<pre>OneHotEncoder(handle_unknown='ignore')</pre> \u00a0KNeighborsRegressor?Documentation for KNeighborsRegressor<pre>KNeighborsRegressor(n_neighbors=10)</pre>"},{"location":"Machine%20Learning/83_GridSearchCV/#fitting-the-pipeline","title":"Fitting the pipeline","text":"<p>As we learned, the Pipeline has the <code>fit</code>, <code>score</code> and <code>predict</code> methods which we use to fit on the dataset (<code>X</code>, <code>y</code>) and evaluate the model's performance using the <code>score()</code> method, finally making predictions.</p> <pre><code># Fit the pipeline\npipe.fit(X, y)\n\n# Score the pipeline\npipe_score = pipe.score(X, y)\n\n# Predict using the pipeline\npipe_predicted_y = pipe.predict(X)\n\nprint('Pipe Score:', pipe_score)\n</code></pre> <pre><code>Pipe Score: 0.818140222027107\n</code></pre>"},{"location":"Machine%20Learning/83_GridSearchCV/#hyperparameter-tuning-using-gridsearchcv","title":"Hyperparameter tuning using GridSearchCV","text":"<p>We perform hyperparameter tuning using <code>GridSearchCV</code>. The pipeline (<code>pipe</code>) serves as the base estimator, and we define a grid of hyperparameters to search through.</p> <p>For this demonstration, we will focus on the number of neighbors for the KNN model.</p> <pre><code>from sklearn.model_selection import GridSearchCV\n</code></pre> <pre><code>model = GridSearchCV(\n    estimator=pipe,\n    param_grid={\n        'model__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    },\n    cv=3\n    )\n</code></pre>"},{"location":"Machine%20Learning/83_GridSearchCV/#fitting-the-model-for-hyperparameter-tuning","title":"Fitting the model for hyperparameter tuning","text":"<p>We fit the <code>GridSearchCV</code> model on the dataset to find the optimal hyperparameters. This involves preprocessing the data and training the model multiple times using cross-validation.</p> <pre><code>model.fit(X, y)\n</code></pre> <pre>GridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('column_transformer',\n                                        ColumnTransformer(transformers=[('numeric',\n                                                                         StandardScaler(),\n                                                                         ['INDUS',\n                                                                          'NOX',\n                                                                          'RM',\n                                                                          'TAX',\n                                                                          'PTRATIO',\n                                                                          'LSTAT']),\n                                                                        ('categorical',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['CHAS',\n                                                                          'RAD'])])),\n                                       ('model',\n                                        KNeighborsRegressor(n_neighbors=10))]),\n             param_grid={'model__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0GridSearchCV?Documentation for GridSearchCViFitted<pre>GridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('column_transformer',\n                                        ColumnTransformer(transformers=[('numeric',\n                                                                         StandardScaler(),\n                                                                         ['INDUS',\n                                                                          'NOX',\n                                                                          'RM',\n                                                                          'TAX',\n                                                                          'PTRATIO',\n                                                                          'LSTAT']),\n                                                                        ('categorical',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['CHAS',\n                                                                          'RAD'])])),\n                                       ('model',\n                                        KNeighborsRegressor(n_neighbors=10))]),\n             param_grid={'model__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})</pre> estimator: Pipeline<pre>Pipeline(steps=[('column_transformer',\n                 ColumnTransformer(transformers=[('numeric', StandardScaler(),\n                                                  ['INDUS', 'NOX', 'RM', 'TAX',\n                                                   'PTRATIO', 'LSTAT']),\n                                                 ('categorical',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['CHAS', 'RAD'])])),\n                ('model', KNeighborsRegressor(n_neighbors=10))])</pre> \u00a0column_transformer: ColumnTransformer?Documentation for column_transformer: ColumnTransformer<pre>ColumnTransformer(transformers=[('numeric', StandardScaler(),\n                                 ['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO',\n                                  'LSTAT']),\n                                ('categorical',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 ['CHAS', 'RAD'])])</pre> numeric<pre>['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT']</pre> \u00a0StandardScaler?Documentation for StandardScaler<pre>StandardScaler()</pre> categorical<pre>['CHAS', 'RAD']</pre> \u00a0OneHotEncoder?Documentation for OneHotEncoder<pre>OneHotEncoder(handle_unknown='ignore')</pre> \u00a0KNeighborsRegressor?Documentation for KNeighborsRegressor<pre>KNeighborsRegressor(n_neighbors=10)</pre>"},{"location":"Machine%20Learning/83_GridSearchCV/#extracting-and-displaying-cross-validation-results","title":"Extracting and displaying cross-validation results","text":"<p>We extract the results of cross-validation performed during hyperparameter tuning and present them in a tabular format using a DataFrame.</p> <pre><code>cv_results = pd.DataFrame(model.cv_results_)\ncv_results\n</code></pre> mean_fit_time std_fit_time mean_score_time std_score_time param_model__n_neighbors params split0_test_score split1_test_score split2_test_score mean_test_score std_test_score rank_test_score 0 0.006364 0.003203 0.003916 0.000833 1 {'model__n_neighbors': 1} 0.347172 0.561780 0.295295 0.401415 0.115356 10 1 0.004014 0.000250 0.003659 0.001165 2 {'model__n_neighbors': 2} 0.404829 0.612498 0.276690 0.431339 0.138369 9 2 0.003741 0.000159 0.003376 0.000710 3 {'model__n_neighbors': 3} 0.466325 0.590333 0.243375 0.433345 0.143552 8 3 0.004399 0.000464 0.002981 0.000075 4 {'model__n_neighbors': 4} 0.569672 0.619854 0.246539 0.478688 0.165428 4 4 0.003881 0.000336 0.002855 0.000071 5 {'model__n_neighbors': 5} 0.613900 0.600994 0.230320 0.481738 0.177857 2 5 0.004046 0.000582 0.003318 0.000555 6 {'model__n_neighbors': 6} 0.620587 0.607083 0.225238 0.484302 0.183269 1 6 0.003628 0.000127 0.002781 0.000018 7 {'model__n_neighbors': 7} 0.639693 0.583685 0.218612 0.480663 0.186704 3 7 0.003585 0.000059 0.002839 0.000093 8 {'model__n_neighbors': 8} 0.636143 0.567841 0.209472 0.471152 0.187125 5 8 0.003649 0.000175 0.002755 0.000031 9 {'model__n_neighbors': 9} 0.649335 0.542624 0.197917 0.463292 0.192639 6 9 0.003591 0.000071 0.002790 0.000060 10 {'model__n_neighbors': 10} 0.653370 0.535112 0.191986 0.460156 0.195674 7 <pre><code>model.score(X, y)\n</code></pre> <pre><code>0.8661624926868122\n</code></pre> Reveal the interpretation of the CV results  These are the results of a grid search cross-validation performed on our pipeline (`pipe`). Let's break down each column:  - `mean_fit_time`: The average time taken to fit the estimator on the training data across all folds. - `std_fit_time`: The standard deviation of the fitting time across all folds. - `mean_score_time`: The average time taken to score the estimator on the test data across all folds. - `std_score_time`: The standard deviation of the scoring time across all folds. - `param_model__n_neighbors`: The value of the `n_neighbors` parameter of the KNeighborsRegressor model in our pipeline for this particular grid search iteration. - `params`: A dictionary containing the parameters used in this grid search iteration. - `split0_test_score`, `split1_test_score`, `split2_test_score`: The test scores obtained for each fold of the cross-validation. Each fold corresponds to one entry here. - `mean_test_score`: The average test score across all folds. - `std_test_score`: The standard deviation of the test scores across all folds. - `rank_test_score`: The rank of this model configuration based on the mean test score. Lower values indicate better performance.  These results allow you to compare different parameter configurations and select the one that performs best based on the mean test score and other relevant metrics.  <p>From the results above, it appears that the best number of neighbors to is 6.</p> <p>From now on, I would like you to consider a GridSearchCV whenever you want to build a machine learning model.</p>"},{"location":"Machine%20Learning/83_GridSearchCV/#congratulations","title":"Congratulations!What's on your mind? Put it in the comments!","text":"<p>If you reached here, you have learned the following:</p> <ul> <li>Selecting Features</li> <li>Preprocessing data</li> <li>Creating a Machine Learning Pipeline</li> <li>Creating a GridSearchCV</li> <li>Using the GridSearchCV to find the best Hyperparameters for our Machine Learning model.</li> </ul>"},{"location":"Machine%20Learning/pipelining_quiz/","title":"Machine Learning Pipelining Quiz","text":"<p>Here are some multiple choice and true/false questions on machine learning pipelining:</p>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Machine%20Learning/pipelining_quiz/#multiple-choice-questions","title":"Multiple Choice Questions","text":"<ol> <li>What is the primary purpose of a machine learning pipeline?</li> <li>A. To visualize data</li> <li>B. To automate the workflow of data processing and model training</li> <li>C. To analyze data manually</li> <li>D. To store data securely</li> </ol> Reveal answer <p>B. To automate the workflow of data processing and model training</p> <ol> <li>Which of the following steps is typically the first in a machine learning pipeline?</li> <li>A. Model evaluation</li> <li>B. Data preprocessing</li> <li>C. Model deployment</li> <li>D. Hyperparameter tuning</li> </ol> Reveal answer <p>B. Data preprocessing</p> <ol> <li>In a scikit-learn pipeline, what does the <code>StandardScaler</code> do?</li> <li>A. Select features</li> <li>B. Scale features to a standard normal distribution</li> <li>C. Reduce the dimensionality of data</li> <li>D. Train the model</li> </ol> Reveal answer <p>B. Scale features to a standard normal distribution</p> <ol> <li>Which of the following is an advantage of using pipelines?</li> <li>A. They make code less readable</li> <li>B. They ensure reproducibility</li> <li>C. They slow down model training</li> <li>D. They increase the risk of data leakage</li> </ol> Reveal answer <p>B. They ensure reproducibility</p> <ol> <li>Which step in a machine learning pipeline is responsible for improving the model by adjusting its parameters?</li> <li>A. Data preprocessing</li> <li>B. Model training</li> <li>C. Hyperparameter tuning</li> <li>D. Model evaluation</li> </ol> Reveal answer <p>C. Hyperparameter tuning</p>"},{"location":"Machine%20Learning/pipelining_quiz/#true-or-false-questions","title":"True or False QuestionsWhat's on your mind? Put it in the comments!","text":"<ol> <li>Pipelines in scikit-learn can only include pre-built transformers and estimators.</li> </ol> Reveal answer <p>False</p> <ol> <li>Using a pipeline ensures that the same data transformations are applied during both training and testing phases.</li> </ol> Reveal answer <p>True</p> <ol> <li>You can use GridSearchCV with a pipeline to perform hyperparameter tuning on multiple steps simultaneously.</li> </ol> Reveal answer <p>True</p> <ol> <li>The steps in a machine learning pipeline must be specified in a particular order.</li> </ol> Reveal answer <p>True</p> <ol> <li>A machine learning pipeline can be saved to disk using joblib or pickle in Python.</li> </ol> Reveal answer <p>True</p> <ol> <li>Transformers in a pipeline are fit using the training data and then applied to the test data.</li> </ol> Reveal answer <p>True</p> <ol> <li>Model evaluation is typically done before model training in a pipeline.</li> </ol> Reveal answer <p>False</p> <ol> <li>A pipeline helps in avoiding data leakage by ensuring proper separation of training and testing data transformations.</li> </ol> Reveal answer <p>True</p> <ol> <li>Pipelines cannot be used for text data processing.</li> </ol> Reveal answer <p>False</p> <ol> <li>Feature extraction can be included as a step in a machine learning pipeline.</li> </ol> Reveal answer <p>True</p> End"},{"location":"Maths%20%26%20Statistics/1_eigen_values_vectors/","title":"Eigen Values and Vectors","text":""},{"location":"Maths%20%26%20Statistics/1_eigen_values_vectors/#lesson-understanding-eigenvalues-and-eigenvectors","title":"Lesson: Understanding Eigenvalues and Eigenvectors","text":""},{"location":"Maths%20%26%20Statistics/1_eigen_values_vectors/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this lesson, you will be able to:</p> <ul> <li>Define eigenvalues and eigenvectors.</li> <li>Compute eigenvalues and eigenvectors of a 2x2 matrix.</li> <li>Understand the significance of these concepts in linear algebra and data science.</li> </ul>"},{"location":"Maths%20%26%20Statistics/1_eigen_values_vectors/#what-are-eigenvalues-and-eigenvectors","title":"What Are Eigenvalues and Eigenvectors?","text":"<p>Let\u2019s consider a square matrix \\(A\\). An eigenvector \\(\\vec{v}\\) and its corresponding eigenvalue \\(\\lambda\\) satisfy the equation:</p> \\[ A \\vec{v} = \\lambda \\vec{v} \\] <p>Where:</p> <ul> <li>\\(A\\) is an \\(n \\times n\\) matrix.</li> <li>\\(\\vec{v}\\) is a non-zero vector.</li> <li>\\(\\lambda\\) is a scalar (just a number).</li> </ul> <p>\ud83d\udca1 Intuition: The vector \\(\\vec{v}\\) doesn't change its direction when multiplied by matrix \\(A\\); it only gets stretched or compressed by \\(\\lambda\\).</p>"},{"location":"Maths%20%26%20Statistics/1_eigen_values_vectors/#step-by-step-computation","title":"Step-by-Step Computation","text":"<p>Let\u2019s work with this matrix:</p> \\[ A = \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\end{bmatrix} \\]"},{"location":"Maths%20%26%20Statistics/1_eigen_values_vectors/#step-1-characteristic-equation","title":"Step 1: Characteristic Equation","text":"<p>To find the eigenvalues, we solve:</p> \\[ \\det(A - \\lambda I) = 0 \\] <p>Where \\(I\\) is the identity matrix. Subtract \\(\\lambda I\\) from \\(A\\):</p> \\[ A - \\lambda I =  \\begin{bmatrix} 2 - \\lambda &amp; 1 \\\\ 1 &amp; 2 - \\lambda \\end{bmatrix} \\] <p>Compute the determinant:</p> \\[ \\det = (2 - \\lambda)^2 - 1 = \\lambda^2 - 4\\lambda + 3 \\] <p>Solve the characteristic equation:</p> \\[ \\lambda^2 - 4\\lambda + 3 = 0 \\Rightarrow \\lambda = 3 \\text{ or } 1 \\]"},{"location":"Maths%20%26%20Statistics/1_eigen_values_vectors/#step-2-find-eigenvectors","title":"Step 2: Find Eigenvectors","text":""},{"location":"Maths%20%26%20Statistics/1_eigen_values_vectors/#for-lambda-3","title":"For \\(\\lambda = 3\\):","text":"<p>Solve \\((A - 3I)\\vec{v} = 0\\):</p> \\[ \\begin{bmatrix} -1 &amp; 1 \\\\ 1 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = 0 \\Rightarrow -x + y = 0 \\Rightarrow y = x \\] <p>So the eigenvector is any scalar multiple of:</p> \\[ \\vec{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\]"},{"location":"Maths%20%26%20Statistics/1_eigen_values_vectors/#for-lambda-1","title":"For \\(\\lambda = 1\\):","text":"<p>Solve \\((A - I)\\vec{v} = 0\\):</p> \\[ \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = 0 \\Rightarrow x + y = 0 \\Rightarrow y = -x \\] <p>So the eigenvector is any scalar multiple of:</p> \\[ \\vec{v}_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} \\]"},{"location":"Maths%20%26%20Statistics/1_eigen_values_vectors/#final-result","title":"Final Result","text":"<ul> <li>Eigenvalues:</li> </ul> <p>$$   \\lambda_1 = 3, \\quad \\lambda_2 = 1   $$</p> <ul> <li>Eigenvectors:</li> </ul> <p>$$   \\vec{v}_1 = \\begin{bmatrix} 1 \\ 1 \\end{bmatrix}, \\quad \\vec{v}_2 = \\begin{bmatrix} 1 \\ -1 \\end{bmatrix}   $$</p>"},{"location":"Maths%20%26%20Statistics/1_eigen_values_vectors/#why-it-matters-applications","title":"Why It Matters (Applications)","text":"<ul> <li>In Principal Component Analysis (PCA), we use eigenvectors of the covariance matrix to identify directions of maximum variance.</li> <li>In machine learning, eigenvalues help reduce dimensionality and noise.</li> <li>In differential equations, eigenvalues determine system stability.</li> </ul>"},{"location":"Maths%20%26%20Statistics/1_eigen_values_vectors/#step-3-implement-in-python","title":"Step 3: Implement in Python","text":"<p>Let\u2019s verify our work using Python and NumPy.</p> <pre><code>import numpy as np\n\n# Define the matrix A\nA = np.array([[2, 1],\n              [1, 2]])\n\n# Compute eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# Display results\nprint(\"Eigenvalues:\")\nprint(eigenvalues)\n\nprint(\"\\nEigenvectors (columns):\")\nprint(eigenvectors)\n</code></pre>"},{"location":"Maths%20%26%20Statistics/1_eigen_values_vectors/#expected-output","title":"Expected Output","text":"<p>The output should be:</p> <pre><code>Eigenvalues:\n[3. 1.]\n\nEigenvectors (columns):\n[[ 0.70710678 -0.70710678]\n [ 0.70710678  0.70710678]]\n</code></pre> <p>\ud83d\udca1 Note:</p> <ul> <li>The eigenvectors are normalized (unit vectors).</li> <li>The columns of the output matrix are the eigenvectors corresponding to each eigenvalue.</li> </ul>"},{"location":"Maths%20%26%20Statistics/1_eigen_values_vectors/#interpreting-the-output","title":"Interpreting the Output","text":"<ul> <li>Eigenvalues: <code>3</code> and <code>1</code> match our manual calculation.</li> <li> <p>Eigenvectors:</p> </li> <li> <p>The first column \\(\\begin{bmatrix} 0.707 \\\\ 0.707 \\end{bmatrix}\\) is equivalent to \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) normalized.</p> </li> <li>The second column \\(\\begin{bmatrix} -0.707 \\\\ 0.707 \\end{bmatrix}\\) corresponds to \\(\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\) normalized.</li> </ul>"},{"location":"Maths%20%26%20Statistics/1_eigen_values_vectors/#key-takeaway","title":"Key Takeaway","text":"<p>Python can be used to quickly and accurately compute eigenvalues and eigenvectors, which is especially helpful for larger matrices or data sets.</p>"},{"location":"Maths%20%26%20Statistics/50_maths_and_stats_for_ds/","title":"Maths and Statistics","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"Maths%20%26%20Statistics/50_maths_and_stats_for_ds/#linear-algebra-for-data-science","title":"Linear Algebra for Data Science","text":"<p>We'll cover essential linear algebra concepts, including Vectors and Matrices and Matrix Operations, with Python code examples using NumPy.</p>"},{"location":"Maths%20%26%20Statistics/50_maths_and_stats_for_ds/#1-vectors-and-matrices","title":"1. Vectors and Matrices","text":"<p>Vector: A vector is an ordered list of numbers, which can be represented as a row or column.</p> <pre><code># Creating a vector\nvector = np.array([3, 4])\nprint(\"Vector:\", vector)\n</code></pre> <pre><code>Vector: [3 4]\n</code></pre> <p>Matrix: A matrix is a two-dimensional array of numbers.</p> <pre><code># Creating a matrix\nmatrix = np.array([[1, 2], [3, 4]])\nprint(\"Matrix:\\n\", matrix)\n</code></pre> <pre><code>Matrix:\n [[1 2]\n [3 4]]\n</code></pre> <p>Applications of Vectors and Matrices   - Representing physical quantities like force, velocity, and acceleration   - Describing geometric shapes and transformations   - Organizing and modeling data in data science</p>"},{"location":"Maths%20%26%20Statistics/50_maths_and_stats_for_ds/#2-matrix-operations","title":"2. Matrix Operations","text":"<p>Addition and Subtraction</p> <pre><code>A = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\n\nprint(\"Matrix A:\\n\", A)\nprint(\"Matrix B:\\n\", B)\n</code></pre> <pre><code>Matrix A:\n [[1 2]\n [3 4]]\nMatrix B:\n [[5 6]\n [7 8]]\n</code></pre> <pre><code># Element-wise addition\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\n\nC = A + B\nprint(\"A + B:\\n\", C)\n</code></pre> <pre><code>A + B:\n [[ 6  8]\n [10 12]]\n</code></pre> <pre><code># Element-wise subtraction\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\n\nD = A - B\nprint(\"A - B:\\n\", D)\n</code></pre> <pre><code>A - B:\n [[-4 -4]\n [-4 -4]]\n</code></pre> <p>Matrix Multiplication</p> <pre><code># Element-wise multiplication\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\n\nE = A * B\nprint(\"Element-wise A * B:\\n\", E)\n</code></pre> <pre><code>Element-wise A * B:\n [[ 5 12]\n [21 32]]\n</code></pre> <pre><code># Dot product (Matrix multiplication)\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\n\nF = np.dot(A, B)\nprint(\"Dot Product A @ B:\\n\", F)\n</code></pre> <pre><code>Dot Product A @ B:\n [[19 22]\n [43 50]]\n</code></pre> <pre><code># **Transpose of a Matrix**\nA = np.array([[1, 2], [3, 4]])\n\nG = A.T\nprint(\"Transpose of A:\\n\", G)\n</code></pre> <pre><code>Transpose of A:\n [[1 3]\n [2 4]]\n</code></pre> <p>Research:</p> <ul> <li>Inverse Matrix</li> <li>Determinant</li> <li>Eigenvalues and Eigenvectors</li> </ul>"},{"location":"Maths%20%26%20Statistics/50_maths_and_stats_for_ds/#derivatives-and-gradients-in-calculus","title":"Derivatives and Gradients in Calculus","text":"<p>Derivatives measure the rate of change of a function. </p> <pre><code>from sympy import symbols, diff\n</code></pre> <pre><code># Define symbol x for differentiation\nx = symbols('x')\nf = x**2 + 3*x + 2\n# get derivative\nf_derivative = diff(f, x)\nprint(\"Derivative of f(x) = x^2 + 3x + 2 is:\", f_derivative)\n</code></pre> <pre><code>Derivative of f(x) = x^2 + 3x + 2 is: 2*x + 3\n</code></pre>"},{"location":"Maths%20%26%20Statistics/50_maths_and_stats_for_ds/#gradient","title":"Gradient","text":"<p>The gradient of a function represents the direction and rate of steepest increase of that function at any given point.</p> <p>Applications of Gradient</p> <ul> <li>Optimization algorithms</li> </ul>"},{"location":"Maths%20%26%20Statistics/50_maths_and_stats_for_ds/#probability-distributions","title":"Probability Distributions","text":"<p>Probability distributions describe how values are distributed.  Here, we explore some common ones: Uniform, Normal, Binomial.</p>"},{"location":"Maths%20%26%20Statistics/50_maths_and_stats_for_ds/#uniform-distribution","title":"Uniform Distribution","text":"<p>In a uniform distribution, all values within a range are equally likely.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n</code></pre> <pre><code># Discrete uniform distribution for a fair coin (2 outcomes: heads or tails)\noutcomes = ['Heads', 'Tails']\nprobabilities = [0.5, 0.5]\n\n# Plotting\nplt.bar(outcomes, probabilities, color=['blue', 'orange'])\nplt.title('Fair Coin Toss Distribution')\nplt.xlabel('Outcome')\nplt.ylabel('Probability')\nplt.ylim(0, 1)\nplt.show()\n</code></pre> <p></p> <p>Examples of uniform distributions</p> <ul> <li>Rolling a fair die</li> <li>Flipping a fair coin</li> <li>Drawing a card from a well-shuffled deck</li> </ul>"},{"location":"Maths%20%26%20Statistics/50_maths_and_stats_for_ds/#normal-distribution","title":"Normal Distribution","text":"<p>A normal (Gaussian) distribution is symmetric, centered around the mean.</p> <pre><code>from scipy.stats import norm\n\n# Simulating Nobel Prize winner ages (mean = 60, std dev = 10)\nmu, sigma = 60, 10\nages = np.random.normal(mu, sigma, 1000)  # Generate 1000 random ages\n\n# Plotting the histogram of ages\nplt.hist(ages, bins=30, density=True, alpha=0.6, color='g', label='Histogram of Ages')\n\n# Plot the normal distribution PDF\nx = np.linspace(ages.min(), ages.max(), 100)\ny = norm.pdf(x, mu, sigma)\nplt.plot(x, y, 'r-', label='Normal Distribution')\n\nplt.title('Age Distribution of Nobel Prize Winners')\nplt.xlabel('Age')\nplt.ylabel('Probability Density')\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Examples of normal distributions</p> <ul> <li>Heights of people</li> <li>IQ scores</li> <li>Measurement errors</li> <li>Stock price fluctuations</li> </ul>"},{"location":"Maths%20%26%20Statistics/50_maths_and_stats_for_ds/#binomial-distribution","title":"Binomial Distribution","text":"<p>The binomial distribution models the number of successes in <code>n</code> trials.</p> <pre><code>from scipy.stats import binom\n\n# Parameters\nn, p = 10, 0.5  # Number of trials and probability of success (fair coin)\nx = np.arange(0, n + 1)  # Possible number of heads (successes)\ny = binom.pmf(x, n, p)  # Binomial PMF (probability mass function)\n\n# Plotting the binomial distribution\nplt.bar(x, y, color='blue', alpha=0.7)\nplt.title('Binomial Distribution for Fair Coin Toss (n=10, p=0.5)')\nplt.xlabel('Number of Heads')\nplt.ylabel('Probability')\nplt.xticks(np.arange(0, n + 1))\nplt.ylim(0, 0.3)  # Limiting y-axis for better visualization\nplt.show()\n</code></pre> <p></p> <p>Examples of binomial distributions</p> <ul> <li>Number of heads in coin flips</li> <li>Number of defective items in a batch</li> <li>Number of customers who click on an ad</li> <li>Number of students who pass an exam</li> </ul>"},{"location":"Maths%20%26%20Statistics/50_maths_and_stats_for_ds/#expectation-and-variance","title":"Expectation and Variance","text":"<p>Expectation (mean) of a random variable X is E[X] = \u03a3 x * P(x)</p> <p>Variance measures spread: Var(X) = E[X^2] - (E[X])^2</p> <pre><code># Example with a discrete random variable\nvalues = np.array([1, 2, 3, 4, 5])\nprobs = np.array([0.1, 0.2, 0.3, 0.2, 0.2])  # Probabilities sum to 1\nexpectation = np.sum(values * probs)\nvariance = np.sum((values**2) * probs) - expectation**2\nprint(f\"Expectation (E[X]): {expectation}\")\nprint(f\"Variance (Var[X]): {variance}\")\n\n# For a normal distribution\nmean, std_dev = 5, 2\nexpectation_norm = mean\nvariance_norm = std_dev ** 2\nprint(f\"Normal Distribution - Expectation: {expectation_norm}, Variance: {variance_norm}\")\n</code></pre> <pre><code>Expectation (E[X]): 3.2\nVariance (Var[X]): 1.5599999999999987\nNormal Distribution - Expectation: 5, Variance: 4\n</code></pre> <p>Applications of Expectation and Variance</p> <ul> <li>Portfolio management</li> <li>Insurance risk assessment</li> <li>Machine Learning model performance</li> <li>Hypothesis testing</li> </ul>"},{"location":"Maths%20%26%20Statistics/50_maths_and_stats_for_ds/#statistics-in-data-science","title":"Statistics in Data Science","text":"<p>This notebook covers Descriptive Statistics and Regression Analysis with examples in Python.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n</code></pre>"},{"location":"Maths%20%26%20Statistics/50_maths_and_stats_for_ds/#descriptive-statistics","title":"Descriptive Statistics","text":"<pre><code># Descriptive Statistics\ndata = [12, 15, 14, 10, 8, 10, 12, 15, 18, 20, 20, 21, 19, 18]\nprint(f\"Dataset: {data}\")\n\nmean_value = np.mean(data)\nmedian_value = np.median(data)\nmode_value = stats.mode(data).mode\nstd_dev = np.std(data)\n\nprint(f\"Mean: {mean_value}\")\nprint(f\"Median: {median_value}\")\nprint(f\"Mode: {mode_value}\")\nprint(f\"Standard Deviation: {std_dev}\")\n</code></pre> <pre><code>Dataset: [12, 15, 14, 10, 8, 10, 12, 15, 18, 20, 20, 21, 19, 18]\nMean: 15.142857142857142\nMedian: 15.0\nMode: 10\nStandard Deviation: 4.120630029101703\n</code></pre>"},{"location":"Maths%20%26%20Statistics/50_maths_and_stats_for_ds/#regression-analysis","title":"Regression Analysis","text":"<pre><code># Regression Analysis\nnp.random.seed(42)\nX = np.random.rand(100, 1) * 10  # Independent variable\ny = 2.5 * X + np.random.randn(100, 1) * 2 + 5  # Dependent variable with noise\n# Splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Applying Linear Regression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# Model Performance\nprint(f\"Intercept: {model.intercept_[0]}\")\nprint(f\"Slope: {model.coef_[0][0]}\")\nprint(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred)}\")\nprint(f\"R-squared Score: {r2_score(y_test, y_pred)}\")\n\n# Plotting the Regression Line\nplt.scatter(X_test, y_test, color='blue', label='Actual Data')\nplt.plot(X_test, y_pred, color='red', linewidth=2, label='Regression Line')\nplt.title(\"Linear Regression Example\")\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>Intercept: 5.285826638917127\nSlope: 2.419729462992111\nMean Squared Error: 2.6147980548680128\nR-squared Score: 0.9545718935323326\n</code></pre> <p>Applications of Regression Analysis</p> <ul> <li>Predictive modeling</li> <li>Time series forecasting</li> <li>Causal inference</li> <li>Feature engineering</li> </ul>"},{"location":"Maths%20%26%20Statistics/51_descriptive_statistics/","title":"Descriptive Statistics","text":"<pre><code>#| hide\n#| default_exp statistics\n</code></pre> <pre><code>## Uncomment and run this cell to install the packages\n# !pip install --upgrade dataidea\n</code></pre>"},{"location":"Maths%20%26%20Statistics/51_descriptive_statistics/#descriptive-statistics-and-summary-metrics","title":"Descriptive Statistics and Summary Metrics","text":"<p>In this notebook, we will learn to obtain important values that describe our data including:</p> <ul> <li> Measures of central tendency</li> <li> Measures of variability</li> <li> Measures of distribution shape</li> <li> Measures of association</li> </ul> <pre><code>import pandas as pd\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\n</code></pre> <p>This notebook has been modified to use the Nobel Price Laureates Dataset which you can download from opendatasoft</p> <pre><code># load the dataset (modify the path to point to your copy of the dataset)\ndata = pd.read_csv('../assets/nobel_prize_year.csv')\ndata = data[data.Gender != 'org'] # removing organizations\ndata.sample(n=5)\n</code></pre> Year Gender Category birth_year age 505 2014 male Physics 1929 85 318 1952 male Literature 1885 67 883 1933 male Literature 1870 63 481 1995 male Peace 1908 87 769 2005 male Peace 1942 63  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Maths%20%26%20Statistics/51_descriptive_statistics/#what-is-descriptive-statistics","title":"What is Descriptive Statistics","text":"<p>Descriptive statistics is a branch of statistics that deals with the presentation and summary of data in a meaningful and informative way. Its primary goal is to describe and summarize the main features of a dataset. </p> <p>Commonly used measures in descriptive statistics include:</p> <ol> <li> <p>Measures of central tendency: These describe the center or average of a dataset and include metrics like mean, median, and mode.</p> </li> <li> <p>Measures of variability: These indicate the spread or dispersion of the data and include metrics like range, variance, and standard deviation.</p> </li> <li> <p>Measures of distribution shape: These describe the distribution of data points and include metrics like skewness and kurtosis.</p> </li> <li> <p>Measures of association: These quantify the relationship between variables and include correlation coefficients.</p> </li> </ol> <p>Descriptive statistics provide simple summaries about the sample and the observations that have been made.</p>"},{"location":"Maths%20%26%20Statistics/51_descriptive_statistics/#1-measures-of-central-tendency-ie-mean-median-mode","title":"1. Measures of central tendency ie Mean, Median, Mode:","text":"<p>The Center of the Data:</p> <p>The center of the data is where most of the values are concentrated.</p> <p> Mean: It is the average value of a dataset calculated by summing all values(numerical) and dividing by the total count.</p> <pre><code>mean_value = np.mean(data.age)\nprint(\"Mean:\", mean_value)\n</code></pre> <pre><code>Mean: 60.21383647798742\n</code></pre> <p> Median: It is the middle value of a dataset when arranged in ascending order. If there is an even number of observations, the median is the average of the two middle values.</p> <pre><code>median_value = np.median(data.age)\nprint(\"Median:\", median_value)\n</code></pre> <pre><code>Median: 60.0\n</code></pre> <p> Mode: It is the value that appears most frequently in a dataset.</p> <pre><code>mode_value = sp.stats.mode(data.age)[0]\nprint(\"Mode:\", mode_value)\n</code></pre> <pre><code>Mode: 56\n</code></pre> <p>Homework:</p> <p> Other ways to find mode (ie using pandas and numpy)</p>"},{"location":"Maths%20%26%20Statistics/51_descriptive_statistics/#2-measures-of-variability","title":"2. Measures of variability","text":"<p>The Variation of the Data:</p> <p>The variation of the data is how spread out the data are around the center.</p> <p>a) Variance and Standard Deviation:</p> <p> Variance: It measures the spread of the data points around the mean.</p> <pre><code># how to implement the variance and standard deviation using numpy\nvariance_value = np.var(data.age)\nprint(\"Variance:\", variance_value)\n</code></pre> <pre><code>Variance: 159.28551085795658\n</code></pre> <p> Standard Deviation: It is the square root of the variance, providing a measure of the average distance between each data point and the mean.</p> <pre><code>std_deviation_value = np.std(data.age)\nprint(\"Standard Deviation:\", std_deviation_value)\n</code></pre> <pre><code>Standard Deviation: 12.620836377116873\n</code></pre>  Summary<p>In summary, variance provides a measure of dispersion in squared units, while standard deviation provides a measure of dispersion in the original units of the data</p>  Note!<p>Smaller variances and standard deviation values mean that the data has values similar to each other and closer to the mean and the vice versa is true</p> <pre><code>plt.hist(x=data.age, bins=20, edgecolor='black')\n# add standard deviation lines\nplt.axvline(mean_value, color='red', linestyle='--', label='Mean')\nplt.axvline(mean_value+std_deviation_value, color='orange', linestyle='--', label='1st std Dev')\nplt.axvline(mean_value-std_deviation_value, color='orange', linestyle='--')\nplt.title('Age of Nobel Prize Winners')\nplt.ylabel('Frequency')\nplt.xlabel('Age')\n# Adjust the position of the legend\nplt.legend(loc='upper left')\n\nplt.show()\n</code></pre> <p></p> <p>b) Range and Interquartile Range (IQR):</p> <p> Range: It is the difference between the maximum and minimum values in a dataset. It is simplest measure of variation</p> <pre><code># One way to obtain range\nmin_age = min(data.age)\nmax_age = max(data.age)\nage_range = max_age - min_age\nprint('Range:', age_range)\n</code></pre> <pre><code>Range: 80\n</code></pre> <pre><code># Calculating the range using numpy\nrange_value = np.ptp(data.age)\nprint(\"Range:\", range_value)\n</code></pre> <pre><code>Range: 80\n</code></pre> <p> Interquartile Range (IQR): It is the range between the first quartile (25th percentile) and the third quartile (75th percentile) of the dataset.</p> <p>Quartiles:</p> <p>Calculating Quartiles</p> <p>The quartiles (Q0,Q1,Q2,Q3,Q4) are the values that separate each quarter.</p> <p>Between Q0 and Q1 are the 25% lowest values in the data. Between Q1 and Q2 are the next 25%. And so on.</p> <ul> <li> Q0 is the smallest value in the data.</li> <li> Q1 is the value separating the first quarter from the second quarter of the data.</li> <li> Q2 is the middle value (median), separating the bottom from the top half.</li> <li> Q3 is the value separating the third quarter from the fourth quarter</li> <li> Q4 is the largest value in the data.</li> </ul> <pre><code># Calculate the quartile\nquartiles = np.quantile(a=data.age, q=[0, 0.25, 0.5, 0.75, 1])\n\nprint('Quartiles:', quartiles)\n</code></pre> <pre><code>Quartiles: [17. 51. 60. 69. 97.]\n</code></pre> <p>Percentiles:</p> <p>Percentiles are values that separate the data into 100 equal parts.</p> <p>For example, The 95th percentile separates the lowest 95% of the values from the top 5%</p> <ul> <li> The 25th percentile (P25%) is the same as the first quartile (Q1).</li> <li> The 50th percentile (P50%) is the same as the second quartile (Q2) and the median.</li> <li> The 75th percentile (P75%) is the same as the third quartile (Q3)</li> </ul> <p>Calculating Percentiles with Python</p> <p>To get all the percentile values, we can use <code>np.percentile()</code> method and pass in the data, and the list of the percentiles as showed below.</p> <pre><code># Getting many percentiles\npercentiles = np.percentile(data.age, [25, 50, 75])\nprint(f'Percentiles: {percentiles}')\n</code></pre> <pre><code>Percentiles: [51. 60. 69.]\n</code></pre> <p>To get a single percentile value, we can again use the <code>np.percentile()</code> method and pass in the data, and a the specicific percentile you're interested in eg:</p> <pre><code># Getting one percentile at a time\nfirst_quartile = np.percentile(a=data.age, q=25) # 25th percentile\nmiddle_percentile = np.percentile(data.age, 50)\nthird_quartile = np.percentile(data.age, 75) # 75th percentile\n\nprint('Q1: ', first_quartile)\nprint('Q2: ', middle_percentile)  \nprint('Q3: ', third_quartile)\n</code></pre> <pre><code>Q1:  51.0\nQ2:  60.0\nQ3:  69.0\n</code></pre> Note!<p>Note also that we can be able to use the `np.quantile()` method  to calculate the percentiles which makes logical sense as all the values mark a fraction(percentage) of the data</p> <pre><code>percentiles = np.quantile(a=data.age, q=[0.25, 0.50, 0.75])\nprint('Percentiles:', percentiles)\n</code></pre> <pre><code>Percentiles: [51. 60. 69.]\n</code></pre> <p>Now we can be able to obtain the interquartile range as the difference between the third and first quartiles as predefined.</p> <pre><code># obtain the interquartile\niqr_value = third_quartile - first_quartile\nprint('Interquartile range: ', iqr_value)\n</code></pre> <pre><code>Interquartile range:  18.0\n</code></pre> <p>Note: Quartiles and percentiles are both types of quantiles</p>  Summary<p>While the range gives an overview of the entire spread of the data from lowest to highest, the interquartile range focuses s`pecifically on the spread of the middle portion of the data, making it more robust against outliers.</p>"},{"location":"Maths%20%26%20Statistics/51_descriptive_statistics/#3-measures-of-distribution-shape-ie-skewness-and-kurtosis","title":"3. Measures of distribution shape ie Skewness and Kurtosis:","text":"<p>The shape of the Data:</p> <p>The shape of the data refers to how the data are bounded on either side of the center.</p> <p> Skewness: It measures the asymmetry of the distribution.</p> <pre><code># let's get skew from scipy\nskewness_value = sp.stats.skew(data.age)\nprint(\"Skewness:\", skewness_value)\n</code></pre> <pre><code>Skewness: -0.028324578326524283\n</code></pre> <p>How to interpret Skewness:</p> <p> Positive skewness (&gt; 0) indicates that the tail on the right side of the distribution is longer than the left side (right skewed). </p> <p> Negative skewness (&lt; 0) indicates that the tail on the left side of the distribution is longer than the right side (left skewed). </p> <pre><code># Plot the histogram\n# Set density=True for normalized histogram\nplt.hist(x=data.age, bins=20, density=True, edgecolor='black')  \n\n# Create a normal distribution curve\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = sp.stats.norm.pdf(x, mean_value, std_deviation_value)\nplt.plot(x, p, 'k', linewidth=2)  \n# 'k' indicates black color, you can change it to any color\n\n# Labels and legend\nplt.xlabel('Age')\nplt.ylabel('Probability Density')\nplt.title('Histogram with Normal Distribution Curve')\nplt.legend(['Normal Distribution', 'Histogram'])\n\nplt.show()\n</code></pre> <p></p> <p> Kurtosis: It measures the peakedness or flatness of the distribution.</p> <pre><code># let's get kurtosis from scipy\nkurtosis_value = sp.stats.kurtosis(data.age)\nprint(\"Kurtosis:\", kurtosis_value)\n</code></pre> <pre><code>Kurtosis: -0.3811155702676823\n</code></pre> <p>How to interpret Kurtosis:</p> <p> A kurtosis of 3 indicates the normal distribution (mesokurtic), also known as Gaussian distribution.</p> <p> Positive kurtosis (&gt; 3) indicates a distribution with heavier tails and a sharper peak than the normal distribution. This is called leptokurtic.</p> <p> Negative kurtosis (&lt; 3) indicates a distribution with lighter tails and a flatter peak than the normal distribution. This is called platykurtic.</p>  Note!<p>In simple terms, skewness tells you if your data is leaning more to one side or the other, while kurtosis tells you if your data has heavy or light tails and how sharply it peaks.</p>"},{"location":"Maths%20%26%20Statistics/51_descriptive_statistics/#4-measures-of-association","title":"4. Measures of association","text":"<p>a). Correlation</p> <p> Correlation measures the relationship between two numerical variables.</p> <p>Correlation Matrix</p> <p> A correlation matrix is simply a table showing the correlation coefficients between variables</p> <p>Correlation Matrix in Python</p> <p>We can use the <code>corrcoef()</code> function in Python to create a correlation matrix.</p> <pre><code># Generate example data\nx = np.array([1, 1, 3, 5, 15])\ny = np.array([2, 4, 6, 8, 10])\n\ncorrelation_matrix = np.corrcoef(x, y)\n\ncorrelation_matrix_df = pd.DataFrame(\n    correlation_matrix, \n    columns=['x', 'y'], \n    index=['x', 'y']\n    )\ncorrelation_matrix_df\n</code></pre> x y x 1.000000 0.867722 y 0.867722 1.000000 <p>Correlation Coefficient:</p> <p> The correlation coefficient measures the strength and direction of the linear relationship between two continuous variables.</p> <p> t ranges from -1 to 1, where:</p> <ul> <li> 1 indicates a perfect positive linear relationship, eg complementary good bread and blueband, battery and torch, fuel and car</li> <li> -1 indicates a perfect negative linear relationship, eg substitute goods like tea and coffee</li> <li> 0 indicates no linear relationship, eg phones and socks, house and mouse</li> </ul> <pre><code># Calculate correlation coefficient\ncorrelation = np.corrcoef(x, y)[0, 1]\nprint(\"Correlation Coefficient:\", correlation)\n</code></pre> <pre><code>Correlation Coefficient: 0.8677218312746245\n</code></pre> <p>Correlation vs Causality:</p> <p>Correlation measures the numerical relationship between two varaibles</p> <p>A high correlation coefficient (close to 1), does not mean that we can for sure conclude an actual relationship between two variables.</p> <p>A classic example:</p> <ul> <li> During the summer, the sale of ice cream at a beach increases</li> <li> Simultaneously, drowning accidents also increase as well</li> </ul> <p>Does this mean that increase of ice cream sale is a direct cause of increased drowning accidents?</p> <p>Measures of Association for Categorical Variables</p> <p>b) Contingency Tables and Chi-square Test for Independence:</p> <p> Contingency tables are used to summarize the relationship between two categorical variables by counting the frequency of observations for each combination of categories.</p> <p> Chi-square test for independence determines whether there is a statistically significant association between the two categorical variables.</p> <pre><code>demo_data = data[['Gender', 'Category']]\n\n# We drop all the missing values just for demonstration purposes\ndemo_data = demo_data.dropna()\n</code></pre> <p>Obtain the cross tabulation of Gender and Category. The cross tabulation is also known as the contingency table</p> <pre><code># cross tab\ngender_category_tab = pd.crosstab(\n    demo_data.Gender, \n    demo_data.Category\n    )\n\n# Let's have a look at the outcome\ngender_category_tab\n</code></pre> Category Chemistry Economics Literature Medicine Peace Physics Gender female 8 2 17 13 18 5 male 181 87 102 212 90 219"},{"location":"Maths%20%26%20Statistics/51_descriptive_statistics/#test-of-independence","title":"Test of Independence:","text":"<p>This test is used to determine whether there is a significant association between two categorical variables.</p> <p>Formula:    \\(\\(\u03c7\u00b2 = \\sum \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\)\\)    where:    - \\(O_{ij}\\) = Observed frequency for each cell in the contingency table    - \\(E_{ij}\\) = Expected frequency for each cell under the assumption of independence</p> <pre><code>chi2_stat, p_value, dof, expected = sp.stats.chi2_contingency(gender_category_tab)\n\nprint('Chi-square Statistic:', chi2_stat)\nprint('p-value:', p_value)\nprint('Degrees of freedom (dof):', dof)\n# print('Expected:', expected)\n</code></pre> <pre><code>Chi-square Statistic: 40.7686907732235\np-value: 1.044840181761602e-07\nDegrees of freedom (dof): 5\n</code></pre> <p>Interpretation of Chi2 Test Results:</p> <ul> <li> The Chi-square statistic measures the difference between the observed frequencies in the contingency table and the frequencies that would be expected if the variables were independent.</li> <li> The p-value is the probability of obtaining a Chi-square statistic as extreme as, or more extreme than, the one observed in the sample, assuming that the null hypothesis is true (i.e., assuming that there is no association between the variables).</li> <li> A low p-value indicates strong evidence against the null hypothesis, suggesting that there is a significant association between the variables.</li> <li> A high p-value indicates weak evidence against the null hypothesis, suggesting that there is no significant association between the variables.</li> </ul> <p>c. Measures of Association for Categorical Variables:</p> <ul> <li> Measures like Cramer's V or phi coefficient quantify the strength of association between two categorical variables.</li> <li> These measures are based on chi-square statistics and the dimensions of the contingency table.</li> </ul>"},{"location":"Maths%20%26%20Statistics/51_descriptive_statistics/#the-formula-for-cramers-v-is","title":"The formula for Cramer's V is:","text":"\\[V = \\sqrt{\\frac{\u03c7\u00b2}{n(k - 1)}}\\] <p>Where: - \\(\u03c7\u00b2\\) is the chi-square statistic from the chi-square test of independence. - \\(n\\) is the total number of observations in the contingency table. - \\(k\\) is the minimum of the number of rows and the number of columns in the contingency table.</p> <p>Cramer's V is a normalized measure of association, making it easier to interpret compared to the raw chi-square statistic. A larger value of Cramer's V indicates a stronger association between the variables.</p> <pre><code>#| hide\n#| export\n\nimport numpy as np\n\ndef cramersV(contingency_table):\n\n    chi2_statistic = sp.stats.chi2_contingency(contingency_table)[0]\n    total_observations = contingency_table.sum().sum()\n    phi2 = chi2_statistic / total_observations\n    rows, columns = contingency_table.shape\n\n    return np.sqrt(phi2/ min(rows-1, columns-1))\n</code></pre> <pre><code>from dataidea.statistics import cramersV\n</code></pre> <pre><code>cramersV(contingency_table=gender_category_tab)\n</code></pre> <pre><code>0.20672318859163366\n</code></pre> <pre><code>#| hide\n#| export\n\nimport pandas as pd\nimport scipy as sp\n\ndef cramersVCorrected(contingency_table):\n\n    chi2_statistic = sp.stats.chi2_contingency(contingency_table)[0]\n    total_observations = contingency_table.sum().sum()\n    phi2 = chi2_statistic / total_observations\n    rows, columns = contingency_table.shape\n    phi2_corrected = max(0, phi2 - ((columns-1)*(rows-1))/(total_observations-1))\n    rows_corrected = rows - ((rows-1)**2)/(total_observations-1)\n    columns_corrected = columns - ((columns-1)**2)/(total_observations-1)\n\n    return np.sqrt(phi2_corrected / min((columns_corrected-1), (rows_corrected-1)))\n</code></pre> <pre><code>from dataidea.statistics import cramersVCorrected\n</code></pre> <pre><code>cramersVCorrected(gender_category_tab)\n</code></pre> <pre><code>0.19371955249110775\n</code></pre> <p>Cramer's V is measure of association between two categorical variables. It ranges from 0 to 1 where: </p> <ul> <li> 0 indicates no association between the variables</li> <li> 1 indicates a perfect association between the variables</li> </ul> <p>Here's an interpretation of the Cramer's V:</p> <ul> <li> Small effect: Around 0.1</li> <li> Medium effect: Around 0.3</li> <li> Large effect: Around 0.5 or greater</li> </ul>"},{"location":"Maths%20%26%20Statistics/51_descriptive_statistics/#frequency-tables","title":"Frequency Tables","text":"<p>Frequency means the number of times a value appears in the data. A table can quickly show us how many times each value appears. If the data has many different values, it is easier to use intervals of values to present them in a table.</p> <p>Here's the age of the 934 Nobel Prize winners up until the year 2020. IN the table, each row is an age interval of 10 years</p> Age Interval Frequency 10-19 1 20-29 2 30-39 48 40-49 158 50-59 236 60-69 262 70-79 174 80-89 50 90-99 3 <p>Note: The intervals for the values are also called bin</p>"},{"location":"Maths%20%26%20Statistics/51_descriptive_statistics/#further-reading","title":"Further Reading","text":"<p>Chapter 3 of An Introduction to Statistical Methods and Data Analysis 7th Edition_New</p> <pre><code>#| hide\nimport nbdev; nbdev.nbdev_export()\n</code></pre> What's on your mind? Put it in the comments!"},{"location":"Maths%20%26%20Statistics/52_inferencial_statistics/","title":"Inferencial Statistics","text":""},{"location":"Maths%20%26%20Statistics/52_inferencial_statistics/#hypothesis-testing-tutorial","title":"Hypothesis Testing Tutorial","text":"<p>Objective: This tutorial introduces hypothesis testing, one of the key statistical concepts used to make decisions based on data. We will use Python and the SciPy library to conduct hypothesis testing.</p>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Maths%20%26%20Statistics/52_inferencial_statistics/#1-what-is-hypothesis-testing","title":"1. What is Hypothesis Testing?","text":"<p>Hypothesis Testing is a statistical method used to make decisions or inferences about a population parameter based on sample data. It helps determine if there is enough evidence to reject a null hypothesis in favor of an alternative hypothesis.</p>"},{"location":"Maths%20%26%20Statistics/52_inferencial_statistics/#2-types-of-hypotheses","title":"2. Types of Hypotheses","text":"<ol> <li>Null Hypothesis (H\u2080): This is the default assumption that there is no effect or no difference. For example, \"There is no difference in mean height between men and women.\"</li> <li>Alternative Hypothesis (H\u2081): This opposes the null hypothesis, stating that there is an effect or difference. For example, \"There is a difference in mean height between men and women.\"</li> </ol>"},{"location":"Maths%20%26%20Statistics/52_inferencial_statistics/#3-steps-in-hypothesis-testing","title":"3. Steps in Hypothesis Testing","text":"<ol> <li>Define Hypotheses: Set the null and alternative hypotheses.</li> <li>Select Significance Level (\u03b1): The probability threshold (commonly 0.05) below which the null hypothesis is rejected.</li> <li>Choose the Test Statistic: Based on the type of data and sample size, e.g., t-test, chi-square test.</li> <li>Compute p-value: This is the probability of observing the data if the null hypothesis is true.</li> <li>Make a Decision:</li> <li>If p-value &lt; \u03b1, reject the null hypothesis.</li> <li>If p-value \u2265 \u03b1, fail to reject the null hypothesis.</li> </ol>"},{"location":"Maths%20%26%20Statistics/52_inferencial_statistics/#4-common-hypothesis-tests","title":"4. Common Hypothesis Tests","text":"<ol> <li>One-sample t-test: Tests whether the mean of a sample is significantly different from a known or hypothesized value.</li> <li>Two-sample t-test: Compares the means of two independent samples.</li> <li>Chi-square test: Tests for association between categorical variables.</li> </ol>"},{"location":"Maths%20%26%20Statistics/52_inferencial_statistics/#5-example-hypothesis-testing-in-python","title":"5. Example: Hypothesis Testing in Python","text":""},{"location":"Maths%20%26%20Statistics/52_inferencial_statistics/#a-import-libraries","title":"(a) Import Libraries","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"Maths%20%26%20Statistics/52_inferencial_statistics/#b-one-sample-t-test","title":"(b) One-sample t-test","text":"<p>Scenario: You want to test whether the average height of people in a town is 170 cm. You take a sample of 30 people and record their heights.</p> <pre><code># Sample data (heights of 30 people)\nnp.random.seed(42)  # For reproducibility\nsample_heights = np.random.normal(168, 5, 30)  # mean=168, std=5\n\n# Perform one-sample t-test\nt_stat, p_value = stats.ttest_1samp(sample_heights, 170)\n\nprint(f\"t-statistic: {t_stat:.4f}, p-value: {p_value:.4f}\")\n\n# Significance level\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"Reject the null hypothesis: The average height is not 170 cm.\")\nelse:\n    print(\"Fail to reject the null hypothesis: The average height is 170 cm.\")\n</code></pre> <pre><code>t-statistic: -3.5793, p-value: 0.0012\nReject the null hypothesis: The average height is not 170 cm.\n</code></pre>"},{"location":"Maths%20%26%20Statistics/52_inferencial_statistics/#c-two-sample-t-test","title":"(c) Two-sample t-test","text":"<p>Scenario: You want to compare the average heights of men and women in the same town. You have the height data for 20 men and 20 women.</p> <pre><code># Heights of men and women (sample data)\nnp.random.seed(42)\nmen_heights = np.random.normal(175, 6, 20)\nwomen_heights = np.random.normal(165, 5, 20)\n\n# Perform two-sample t-test\nt_stat, p_value = stats.ttest_ind(men_heights, women_heights)\n\nprint(f\"t-statistic: {t_stat:.4f}, p-value: {p_value:.4f}\")\n\n# Decision\nif p_value &lt; alpha:\n    print(\"Reject the null hypothesis: There is a significant difference between men's and women's heights.\")\nelse:\n    print(\"Fail to reject the null hypothesis: No significant difference in heights.\")\n</code></pre> <pre><code>t-statistic: 6.1236, p-value: 0.0000\nReject the null hypothesis: There is a significant difference between men's and women's heights.\n</code></pre>"},{"location":"Maths%20%26%20Statistics/52_inferencial_statistics/#d-chi-square-test","title":"(d) Chi-square test","text":"<p>Scenario: You want to test whether gender and preference for a product (Yes/No) are independent in a survey.</p> <pre><code># Contingency table (sample data)\n# Rows: Gender (Male, Female), Columns: Preference (Yes, No)\ndata = [[30, 10], [25, 15]]\n\n# Perform chi-square test\nchi2_stat, p_value, dof, expected = stats.chi2_contingency(data)\n\nprint(f\"Chi-square statistic: {chi2_stat:.4f}, p-value: {p_value:.4f}\")\n\n# Decision\nif p_value &lt; alpha:\n    print(\"Reject the null hypothesis: Gender and preference are not independent.\")\nelse:\n    print(\"Fail to reject the null hypothesis: Gender and preference are independent.\")\n</code></pre> <pre><code>Chi-square statistic: 0.9309, p-value: 0.3346\nFail to reject the null hypothesis: Gender and preference are independent.\n</code></pre>"},{"location":"Maths%20%26%20Statistics/52_inferencial_statistics/#e-anova-analysis-of-variance","title":"(e) ANOVA (Analysis of Variance)","text":"<p>Scenario: You want to compare the average heights of people from three different cities to see if there is a statistically significant difference in their means.</p> <p>Hypotheses: - H\u2080 (Null Hypothesis): The means of all groups are equal. - H\u2081 (Alternative Hypothesis): At least one group has a different mean.</p> <pre><code># Simulate data for three cities (heights)\nnp.random.seed(42)\ncity1_heights = np.random.normal(168, 5, 30)\ncity2_heights = np.random.normal(170, 6, 30)\ncity3_heights = np.random.normal(165, 4, 30)\n\n# Combine data into a pandas DataFrame\ndf = pd.DataFrame({\n    'city1': city1_heights,\n    'city2': city2_heights,\n    'city3': city3_heights\n})\n\n# Visualize the data\ndf.boxplot()\nplt.title('Height Distributions for Three Cities')\nplt.ylabel('Height (cm)')\nplt.show()\n\n# Perform ANOVA test\nf_stat, p_value = stats.f_oneway(city1_heights, city2_heights, city3_heights)\n\nprint(f\"F-statistic: {f_stat:.4f}, p-value: {p_value:.4f}\")\n\n# Decision\nalpha = 0.05\nif p_value &lt; alpha:\n    print(\"Reject the null hypothesis: At least one city has a different mean height.\")\nelse:\n    print(\"Fail to reject the null hypothesis: All cities have the same mean height.\")\n</code></pre> <p></p> <pre><code>F-statistic: 5.9711, p-value: 0.0037\nReject the null hypothesis: At least one city has a different mean height.\n</code></pre>"},{"location":"Maths%20%26%20Statistics/52_inferencial_statistics/#explanation-of-anova","title":"Explanation of ANOVA","text":"<p> F-statistic: A ratio of the variance between the group means to the variance within the groups. A higher F-statistic indicates a greater disparity between group means.</p>"},{"location":"Maths%20%26%20Statistics/52_inferencial_statistics/#conclusion","title":"ConclusionWhat's on your mind? Put it in the comments!","text":"<p>Congratulations on completing this tutorial. Hypothesis testing is a powerful statistical tool used to make data-driven decisions. This tutorial demonstrated how to conduct hypothesis testing in Python with the:</p> <ul> <li> one-sample t-test</li> <li> two-sample t-test</li> <li> chi-square test</li> <li> ANOVA</li> </ul>"},{"location":"Maths%20%26%20Statistics/53_implementing_statistical_models/","title":"Statistical Models","text":"<pre><code>## Uncomment and run this cell to install the packages\n# !pip install pandas numpy statsmodels\n</code></pre>"},{"location":"Maths%20%26%20Statistics/53_implementing_statistical_models/#implementing-statistical-models-in-python","title":"Implementing Statistical Models in Python","text":""},{"location":"Maths%20%26%20Statistics/53_implementing_statistical_models/#statistical-models","title":"Statistical models","text":"<p>Statistical models are mathematical representations of relationships between variables in a dataset. These models are used to make predictions, infer causal relationships, and understand patterns in data. Statistical modeling involves formulating hypotheses about the data generating process, estimating model parameters from observed data, and evaluating the fit of the model to the data.</p> <p>The statsmodels.api library in Python provides a wide range of tools for statistical modeling and inference. It allows users to build, estimate, and analyze various statistical models using a simple and intuitive interface.</p> <pre><code>import statsmodels.api as sm\nimport numpy as np\n</code></pre> <ol> <li>Linear Regression:</li> <li>Linear regression is used to model the relationship between one or more independent variables and a continuous dependent variable.</li> </ol> <pre><code># Generate example data\nnp.random.seed(0)\nX = np.random.rand(100, 2)  # Two independent variables\n</code></pre> <pre><code># Dependent variable with noise\n\ny = 2 * X[:, 0] + 3 * X[:, 1] + np.random.normal(0, 1, 100)  \n</code></pre> <pre><code># Add constant term for intercept\nX = sm.add_constant(X)\n</code></pre> <pre><code># Fit linear regression model\nmodel = sm.OLS(y, X).fit()\n</code></pre> <pre><code># Print model summary\nprint(model.summary())\n</code></pre> <pre><code>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.440\nModel:                            OLS   Adj. R-squared:                  0.428\nMethod:                 Least Squares   F-statistic:                     38.06\nDate:                Wed, 17 Apr 2024   Prob (F-statistic):           6.31e-13\nTime:                        12:22:26   Log-Likelihood:                -140.25\nNo. Observations:                 100   AIC:                             286.5\nDf Residuals:                      97   BIC:                             294.3\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.2554      0.277      0.922      0.359      -0.294       0.805\nx1             1.4260      0.356      4.011      0.000       0.720       2.132\nx2             2.8054      0.351      8.004      0.000       2.110       3.501\n==============================================================================\nOmnibus:                        1.210   Durbin-Watson:                   2.349\nProb(Omnibus):                  0.546   Jarque-Bera (JB):                0.703\nSkew:                           0.122   Prob(JB):                        0.704\nKurtosis:                       3.330   Cond. No.                         5.58\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</code></pre> <pre><code>0.2554/0.277 \n</code></pre> <pre><code>0.9220216606498195\n</code></pre> <ul> <li>R-squared measures how well the independant variables explain the variability of the dependant variable</li> <li>F-Statistic measures the significance of the regression model</li> <li> <p>t-statistic for each coefficient measures the significance level of each independent variable</p> </li> <li> <p>Logistic Regression:</p> </li> <li>Logistic regression is used when the dependent variable is binary (e.g., 0 or 1, True or False).</li> </ul> <pre><code># Generate example data for logistic regression\nnp.random.seed(0)\nX = np.random.rand(100, 2)  # Two independent variables\n# Generate binary outcome variable based on a threshold\nthreshold = 0.6\ny = (2 * X[:, 0] + 3 * X[:, 1] &gt; threshold).astype(int)\n\n# Add constant term for intercept\nX = sm.add_constant(X)\n\n# Fit logistic regression model\nlogit_model = sm.Logit(y, X).fit()\n\n# Print model summary\nprint(logit_model.summary())\n</code></pre> <pre><code>Warning: Maximum number of iterations has been exceeded.\n         Current function value: 0.000000\n         Iterations: 35\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                      y   No. Observations:                  100\nModel:                          Logit   Df Residuals:                       97\nMethod:                           MLE   Df Model:                            2\nDate:                Wed, 17 Apr 2024   Pseudo R-squ.:                   1.000\nTime:                        13:05:57   Log-Likelihood:            -1.1958e-06\nconverged:                      False   LL-Null:                       -9.8039\nCovariance Type:            nonrobust   LLR p-value:                 5.524e-05\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        -70.8342   4.95e+04     -0.001      0.999   -9.71e+04     9.7e+04\nx1           196.7613   1.92e+05      0.001      0.999   -3.76e+05    3.76e+05\nx2           488.7691   7.23e+05      0.001      0.999   -1.42e+06    1.42e+06\n==============================================================================\n\nComplete Separation: The results show that there iscomplete separation or perfect prediction.\nIn this case the Maximum Likelihood Estimator does not exist and the parameters\nare not identified.\n\n\n/home/jumashafara/venvs/dataanalysis/lib/python3.10/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n</code></pre> <p>These examples demonstrate how to implement linear regression and logistic regression using <code>statsmodels.api</code>. The summary output provides detailed information about the model parameters, goodness-of-fit measures, and statistical significance of predictors. This can be useful for interpreting the results and assessing the performance of the models.</p> What's on your mind? Put it in the comments!"},{"location":"Maths%20%26%20Statistics/hypothesis_testing/","title":"Hypothesis Testing Quiz","text":"<ol> <li>What is the primary purpose of inferential statistics?</li> <li>A. To describe the main features of a dataset</li> <li>B. To draw conclusions about a population based on a sample</li> <li>C. To organize and summarize data</li> <li> <p>D. To determine the reliability of data</p> </li> <li> <p>Which of the following is an example of a parameter?</p> </li> <li>A. Sample mean</li> <li>B. Sample standard deviation</li> <li>C. Population mean</li> <li> <p>D. Sample variance</p> </li> <li> <p>Which statistical test is used to compare the means of two independent groups?</p> </li> <li>A. Paired t-test</li> <li>B. Independent t-test</li> <li>C. ANOVA</li> <li> <p>D. Chi-square test</p> </li> <li> <p>A Type I error occurs when:</p> </li> <li>A. A true null hypothesis is incorrectly rejected</li> <li>B. A false null hypothesis is incorrectly rejected</li> <li>C. A true null hypothesis is incorrectly accepted</li> <li> <p>D. A false null hypothesis is incorrectly accepted</p> </li> <li> <p>The p-value represents:</p> </li> <li>A. The probability that the null hypothesis is true</li> <li>B. The probability of observing the data, or something more extreme, assuming the null hypothesis is true</li> <li>C. The probability of making a Type I error</li> <li> <p>D. The power of the test</p> </li> <li> <p>Which of the following is true about the confidence interval?</p> </li> <li>A. It provides the exact value of the population parameter</li> <li>B. It indicates the range within which the sample mean will fall 95% of the time</li> <li>C. It provides a range of values within which the population parameter is likely to fall</li> <li> <p>D. It determines the sample size needed for a study</p> </li> <li> <p>What is the central limit theorem?</p> </li> <li>A. The distribution of sample means will be approximately normally distributed, regardless of the distribution of the population, given a sufficiently large sample size</li> <li>B. The sum of a large number of independent and identically distributed random variables will be approximately normally distributed</li> <li>C. The probability distribution of a continuous random variable will be normal if the sample size is large enough</li> <li> <p>D. The mean of a large sample will equal the mean of the population</p> </li> <li> <p>When should you use a chi-square test?</p> </li> <li>A. To compare the means of more than two groups</li> <li>B. To test the association between two categorical variables</li> <li>C. To compare the variances of two samples</li> <li> <p>D. To analyze the relationship between a continuous and a categorical variable</p> </li> <li> <p>If the 95% confidence interval for a mean difference includes zero, what does this imply?</p> </li> <li>A. The difference is statistically significant</li> <li>B. The null hypothesis cannot be rejected</li> <li>C. The sample size is too small</li> <li> <p>D. The p-value is less than 0.05</p> </li> <li> <p>The power of a statistical test is:</p> <ul> <li>A. The probability of rejecting the null hypothesis when it is true</li> <li>B. The probability of accepting the null hypothesis when it is false</li> <li>C. The probability of rejecting the null hypothesis when it is false</li> <li>D. The probability of making a Type I error</li> </ul> </li> <li> <p>In hypothesis testing, the null hypothesis is:</p> <ul> <li>A. A statement of no effect or no difference</li> <li>B. A statement that there is an effect or a difference</li> <li>C. Always the hypothesis that the researcher wants to prove</li> <li>D. Less important than the alternative hypothesis</li> </ul> </li> <li> <p>The sample statistic that is most commonly used to estimate a population parameter is called:</p> <ul> <li>A. Estimator</li> <li>B. Point estimate</li> <li>C. Confidence interval</li> <li>D. Hypothesis</li> </ul> </li> <li> <p>Which method is used to determine the sample size for a study?</p> <ul> <li>A. Power analysis</li> <li>B. T-test</li> <li>C. Chi-square test</li> <li>D. Correlation coefficient</li> </ul> </li> <li> <p>The F-test in ANOVA is used to:</p> <ul> <li>A. Test for the equality of two variances</li> <li>B. Test for the equality of three or more means</li> <li>C. Test for the equality of proportions</li> <li>D. Test for independence between two categorical variables</li> </ul> </li> <li> <p>Which of the following is true regarding the normal distribution?</p> <ul> <li>A. It is skewed to the right</li> <li>B. It is defined by its mean and standard deviation</li> <li>C. It can only take on positive values</li> <li>D. It is bimodal</li> </ul> </li> </ol>"},{"location":"Maths%20%26%20Statistics/hypothesis_testing/#correct-answers","title":"Correct Answers","text":"<ol> <li>B</li> <li>C</li> <li>B</li> <li>A</li> <li>B</li> <li>C</li> <li>A</li> <li>B</li> <li>B</li> <li>C</li> <li>A</li> <li>B</li> <li>A</li> <li>B</li> <li>B</li> </ol> What's on your mind? Put it in the comments!"},{"location":"Python/11_python_tutorial/","title":"Python Quick Review","text":""},{"location":"Python/11_python_tutorial/#introduction","title":"Introduction","text":"<p>Python is a great general-purpose programming language on its own, but with the help of a few popular libraries (numpy, scipy, matplotlib) it becomes a powerful environment for scientific computing.</p> <p>Whether you're a total beginner or seasoned programmer, this lesson will serve as a Python Quick Crash Course to refresh your Python knowledge</p> <p>In this tutorial, we will cover:</p> <p> Basic Python: Basic data types (Containers, Lists, Dictionaries, Sets, Tuples), Functions, Classes</p>"},{"location":"Python/11_python_tutorial/#a-brief-note-on-python-versions","title":"A Brief Note on Python Versions","text":"<p>We'll be using Python 3.10 for this iteration of the course. You can check your Python version at the command line by running <code>python --version</code>.</p> <pre><code># checking python version\n!python --version\n</code></pre> <pre><code>Python 3.12.3\n</code></pre> Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Python/11_python_tutorial/#basics-of-python","title":"Basics of Python","text":"<p>Python is a high-level, dynamically typed multiparadigm programming language. Python code is often said to be almost like pseudocode, since it allows you to express very powerful ideas in very few lines of code while being very readable. As an example, here is an implementation of the classic quicksort algorithm in Python:</p> <pre><code>def quicksort(array):\n    if len(array) &lt;= 1:\n        return array\n    pivot = array[len(array) // 2]\n    left = [number for number in array if number &lt; pivot]\n    middle = [number for number in array if number == pivot]\n    right = [number for number in array if number &gt; pivot]\n    return quicksort(left) + middle + quicksort(right)\n\nquicksort([3,6,8,10,1,2,1])\n</code></pre> <pre><code>[1, 1, 2, 3, 6, 8, 10]\n</code></pre> <pre><code>sorted('listen', reverse=True)\n</code></pre> <pre><code>['t', 's', 'n', 'l', 'i', 'e']\n</code></pre>"},{"location":"Python/11_python_tutorial/#variables","title":"Variables","text":"<p>Variables are stores of value</p> <pre><code>name = 'Juma'\nage = 19\nid_number = 190045\n</code></pre>"},{"location":"Python/11_python_tutorial/#rules-to-consider","title":"Rules to consider","text":"<ul> <li> Variable names should be meaningful eg `number` instead of `x`</li> <li> Variable names should contain only alpha-numberic characters, and maybe under_scores</li> <li> Variable names can only start with letters or an underscore</li> <li> Variable name cannot contain special characters</li> <li> Variables names are case sensitive</li> </ul>"},{"location":"Python/11_python_tutorial/#examples-of-variables","title":"Examples of variables","text":"<p>For this course, we'll use snake case for quick variables</p> <pre><code>name = 'Eva'\nlist_of_names = ['Eva', 'Shafara', 'Bob'] # snake case\n</code></pre> <p>we'll use camel case for function variables</p> <pre><code>def calculateBMI(weight_kg, height_m): # camel case\n    bmi = weight_kg / height_m ** 2\n    rounded_bmi = round(bmi, 3)\n    return rounded_bmi\n</code></pre> <p>finally, we'll use pascal case for class variables</p> <pre><code>class MathFunction:    # Pascal Case\n    def __init__(self, number):\n        self.number = number\n\n    def square(self):\n        return self.number ** 2\n\n    def cube(self):\n        return self.number ** 3\n</code></pre>"},{"location":"Python/11_python_tutorial/#basic-data-types","title":"Basic data types","text":""},{"location":"Python/11_python_tutorial/#numbers","title":"Numbers","text":"<p>Integers and floats work as you would expect from other languages:</p> <pre><code>number = 3\nprint('Number: ', number)\nprint('Type: ', type(number))\n</code></pre> <pre><code>Number:  3\nType:  &lt;class 'int'&gt;\n</code></pre> <pre><code># Quick number arithmetics\n\nprint(number + 1)   # Addition\nprint(number - 1)   # Subtraction\nprint(number * 2)   # Multiplication\nprint(number ** 2)  # Enumberponentiation\n</code></pre> <pre><code>4\n2\n6\n9\n</code></pre> <pre><code># Some compound assingment operators\n\nnumber += 1 # number = number + 1\nprint(number)\nnumber *= 2\nprint(number)\nnumber /= 1 # number = number / 1\nprint(number)\nnumber -= 2\nprint(number)\n</code></pre> <pre><code>4\n8\n8.0\n6.0\n</code></pre> <pre><code>number = 2.5\nprint(type(number))\nprint(number, number + 1, number * 2, number ** 2)\n</code></pre> <pre><code>&lt;class 'float'&gt;\n2.5 3.5 5.0 6.25\n</code></pre> <pre><code># complex numbers\nvector = 2 + 6j\ntype(vector)\n</code></pre> <pre><code>complex\n</code></pre>"},{"location":"Python/11_python_tutorial/#booleans","title":"Booleans","text":"<p>Python implements all of the usual operators for Boolean logic, but uses English words rather than symbols:</p> <pre><code>t, f = True, False\ntype(t)\n</code></pre> <pre><code>bool\n</code></pre> <p>Now we let's look at the operations:</p> <pre><code># Logical Operators\n\nprint(t and f) # Logical AND;\nprint(t or f)  # Logical OR;\nprint(not t)   # Logical NOT;\nprint(t != f)  # Logical XOR;\n</code></pre> <pre><code>False\nTrue\nFalse\nTrue\n</code></pre>"},{"location":"Python/11_python_tutorial/#strings","title":"Strings","text":"<p>A string is a sequence of characters under some quotes. Eg.</p> <pre><code>hello = 'hello'   # String literals can use single quotes\nworld = \"world\"   # or double quotes; it does not matter\nprint(hello, len(hello))\n</code></pre> <pre><code>hello 5\n</code></pre> <pre><code># We can string in python\nfull = hello + ' ' + world  # String concatenation\nprint(full)\n</code></pre> <pre><code>hello world\n</code></pre> <pre><code>hw12 = '{} {} {}'.format(hello, world, 12)  # string formatting\nprint(hw12)\n</code></pre> <pre><code>hello world 12\n</code></pre> <pre><code>statement = 'I love to code in {}'\nmodified = statement.format('JavaScript')\nprint(modified)\n</code></pre> <pre><code>I love to code in JavaScript\n</code></pre> <pre><code># formatting by indexing\nstatement = '{0} loves to code in {2} and {1}'\nstatement.format('Juma', 'Python', 'JavaScript')\n</code></pre> <pre><code>'Juma loves to code in JavaScript and Python'\n</code></pre> <pre><code># formatting by name\nstatement = '{name} loves to code in {language1} and {language2}'\nstatement.format(language2='Python', name='Juma', language1='JavaScript')\n</code></pre> <pre><code>'Juma loves to code in JavaScript and Python'\n</code></pre> <pre><code># String Literal Interpolation\nname = 'Juma'\nlanguage1 = 'JavaScript'\nlanguage2 = 'Python'\n\nstatement = f'{name} loves to code in {language1} and {language2}'\n\nprint(statement)\n</code></pre> <pre><code>Juma loves to code in JavaScript and Python\n</code></pre> <p>String objects have a bunch of useful methods; for example:</p> <pre><code>string_ = \"hello\"\nprint(string_.capitalize())  # Capitalize a string\nprint(string_.upper())       # Convert a string to uppercase; prints \"HELLO\"\nprint(string_.rjust(7))      # Right-justify a string, padding with spaces\nprint(string_.center(7))     # Center a string, padding with spaces\nprint(string_.replace('l', '(ell)'))  # Replace all instances of one substring with another\nprint('  world '.strip())  # Strip leading and trailing whitespace\n</code></pre> <pre><code>Hello\nHELLO\n  hello\n hello \nhe(ell)(ell)o\nworld\n</code></pre> <pre><code>statement = 'i love to code in Python '\n\ncapitalized = statement.capitalize()\nupped = statement.upper()\nreplaced = statement.replace('Python', 'javascript')\nstatement.strip()\n</code></pre> <pre><code>'i love to code in Python'\n</code></pre> <p>You can find a list of all string methods in the documentation.</p>"},{"location":"Python/11_python_tutorial/#containers","title":"Containers","text":"<ul> <li> Python containers (collections) are objects that we use to group other objects</li> <li> Python includes several built-in container types: lists, dictionaries, sets, and tuples.</li> </ul>"},{"location":"Python/11_python_tutorial/#lists","title":"Lists","text":"<p>A list is an ordered collection of python objects or elements. A list can contain objects of different data types</p> <pre><code>list_of_numbers = [3, 1, 2]   # Create a list\nprint(list_of_numbers)\nprint(list_of_numbers[2])\nprint(list_of_numbers[-1])     # Negative indices count from the end of the list; prints \"2\"\n</code></pre> <pre><code>[3, 1, 2]\n2\n2\n</code></pre> <pre><code>list_of_numbers[2] = 'foo'    # replacing a specific value in a list\nprint(list_of_numbers)\n</code></pre> <pre><code>[3, 1, 'foo']\n</code></pre> <pre><code>list_of_numbers.append('bar') # Add a new element to the end of the list\nprint(list_of_numbers)  \n</code></pre> <pre><code>[3, 1, 'foo', 'bar']\n</code></pre> <pre><code>last_item = list_of_numbers.pop()     # Remove and return the last element of the list\nprint(last_item)    # returns the last item \nprint(list_of_numbers) # Modifies the original list\n</code></pre> <pre><code>bar\n[3, 1, 'foo']\n</code></pre> <p>Research on:</p> <ul> <li> `del` </li> <li> `remove()`</li> </ul> <p>As usual, you can find all the gory details about lists in the documentation.</p>"},{"location":"Python/11_python_tutorial/#slicing","title":"Slicing","text":"<p>In addition to accessing list elements one at a time, Python provides concise syntax to access a range of values in a list; this is known as slicing:</p> <pre><code>list_of_numbers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nprint(list_of_numbers)\n</code></pre> <pre><code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n</code></pre> <pre><code>print(list_of_numbers)         # Prints \"[0, 1, 2, 3, 4]\"\nprint(list_of_numbers[2:4])    # Get a slice from index 2 to 4 (exclusive); prints \"[2, 3]\"\nprint(list_of_numbers[2:])     # Get a slice from index 2 to the end; prints \"[2, 3, 4]\"\nprint(list_of_numbers[:2])     # Get a slice from the start to index 2 (exclusive); prints \"[0, 1]\"\nprint(list_of_numbers[:])      # Get a slice of the whole list; prints [\"0, 1, 2, 3, 4]\"\nprint(list_of_numbers[:-1])    # Slice indices can be negative; prints [\"0, 1, 2, 3]\"\nlist_of_numbers[2:4] = [8, 9] # Assign a new sublist to a slice\nprint(list_of_numbers)         # Prints \"[0, 1, 8, 9, 4]\"\n</code></pre> <pre><code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n[2, 3]\n[2, 3, 4, 5, 6, 7, 8, 9]\n[0, 1]\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n[0, 1, 2, 3, 4, 5, 6, 7, 8]\n[0, 1, 8, 9, 4, 5, 6, 7, 8, 9]\n</code></pre>"},{"location":"Python/11_python_tutorial/#loops","title":"Loops","text":"<p>A <code>for loop</code> is used to loop through (or iterate) over a sequence of objects (iterable objects). Iterable objects in python include strings, lists, sets etc </p> <p>You can loop over the elements of a list like this:</p> <pre><code>list_of_animals = ['cat', 'dog', 'monkey']\n\nfor animal in list_of_animals:\n    print(animal)\n</code></pre> <pre><code>cat\ndog\nmonkey\n</code></pre> <pre><code>list_of_numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 0]\nlist_of_squared_numbers = []\n\nfor number in list_of_numbers:\n    list_of_squared_numbers.append(pow(number, 2))\n\nlist_of_squared_numbers\n</code></pre> <pre><code>[1, 4, 9, 16, 25, 36, 49, 64, 81, 0]\n</code></pre> <p>If you want access to the index of each element within the body of a loop, use the built-in <code>enumerate</code> function:</p> <pre><code>animals = ['cat', 'dog', 'monkey']\n\nfor index, animal in enumerate(animals):\n    print(f'{index}: {animal}')\n</code></pre> <pre><code>0: cat\n1: dog\n2: monkey\n</code></pre>"},{"location":"Python/11_python_tutorial/#list-comprehensions","title":"List comprehensions:","text":"<pre><code>numbers = [0, 1, 2, 3, 4]\nsquares = []\n\nfor number in numbers:\n    squares.append(pow(number, 2))\n\nprint(squares)\n</code></pre> <pre><code>[0, 1, 4, 9, 16]\n</code></pre> <p>You can make this code simpler using a list comprehension:</p> <pre><code>list_of_numbers = [0, 1, 2, 3, 4]\n\nsquares = [pow(number, 2) for number in list_of_numbers]\n\nprint(squares)\n</code></pre> <pre><code>[0, 1, 4, 9, 16]\n</code></pre> <p>List comprehensions can also contain conditions:</p> <pre><code>numbers = [0, 1, 2, 3, 4]\n\neven_squares = [pow(number, 2) for number in numbers if number % 2 == 0]\n\nprint(even_squares)\n</code></pre> <pre><code>[0, 4, 16]\n</code></pre> <p>Research:  How to combine lists</p>"},{"location":"Python/11_python_tutorial/#dictionaries","title":"Dictionaries","text":"<ul> <li> A dictionary is an unordered and mutable collection of items</li> <li> A dictionary is created using curly brackets</li> <li> Each item in a dictionary contains a key/value pair</li> </ul> <pre><code># creating a dictionary\nperson = {\n    'first_name': 'Juma',\n    'last_name': 'Shafara',\n    'age': 51,\n    'married': True\n}\nperson\n</code></pre> <pre><code>{'first_name': 'Juma', 'last_name': 'Shafara', 'age': 51, 'married': True}\n</code></pre> <pre><code># accessing items in a dictionary\nfirst_name = person['first_name']\nlast_name = person['last_name']\nfull_name = first_name + ' ' + last_name\n\n# display\nfull_name\n</code></pre> <pre><code>'Juma Shafara'\n</code></pre> <pre><code># add items to a dictionary\nperson['hobby'] = 'Coding'\nperson\n</code></pre> <pre><code>{'first_name': 'Juma',\n 'last_name': 'Shafara',\n 'age': 51,\n 'married': True,\n 'hobby': 'Coding'}\n</code></pre> <pre><code>email = person.get('email', 'email not available')\nprint(email)\n</code></pre> <pre><code>email not available\n</code></pre> <pre><code># modifying a value in a dictionay\nperson['married'] = False\nperson\n</code></pre> <pre><code>{'first_name': 'Juma',\n 'last_name': 'Shafara',\n 'age': 51,\n 'married': False,\n 'hobby': 'Coding'}\n</code></pre> <pre><code># remove an item from a dictionary\nperson.pop('age')\nperson\n</code></pre> <pre><code>{'first_name': 'Juma',\n 'last_name': 'Shafara',\n 'married': False,\n 'hobby': 'Coding'}\n</code></pre> <p>Research:</p> <ul> <li> How to remove an item using the `del` method</li> <li> How to iterate over objects in a dictionary</li> <li> Imitate list comprehension with dictionaries</li> </ul> <p>You can find all you need to know about dictionaries in the documentation.</p>"},{"location":"Python/11_python_tutorial/#sets","title":"Sets","text":"<ul> <li> A set is an unordered, immutable collection of distinct elements. </li> <li> A set is created using curly braces</li> <li> The objects are placed inside the brackets and are separated by commas</li> <li> As a simple example, consider the following:</li> </ul> <pre><code>animals = {'cat', 'dog'}\n\nprint('cat' in animals)   # Check if an element is in a set; prints \"True\"\nprint('fish' not in animals)  # prints \"True\"\n</code></pre> <pre><code>True\nTrue\n</code></pre> <pre><code>animals.add('fish')      # Add an element to a set\n\nprint('fish' in animals) # Returns \"True\"\n\nprint(len(animals))       # Number of elements in a set;\n</code></pre> <pre><code>True\n3\n</code></pre> <pre><code>animals.add('cat')       # Adding an element that is already in the set does nothing\nprint(len(animals)) \n\nanimals.remove('cat')    # Remove an element from a set\nprint(len(animals))       \n</code></pre> <pre><code>3\n2\n</code></pre> <p>Research:</p> <ul> <li> How to remove with `discard()`</li> <li> How to remove with `pop()`</li> <li> How to combine sets/li&gt;  <li> How to get the difference between 2 sets</li> <li> What happens when we have repeated elements in a set</li> <p>Loops: Iterating over a set has the same syntax as iterating over a list; however since sets are unordered, you cannot make assumptions about the order in which you visit the elements of the set:</p> <pre><code>animals = {'cat', 'dog', 'fish'}\n\nfor index, animal in enumerate(animals):\n    print(f'{index}: {animal}')\n</code></pre> <pre><code>0: fish\n1: cat\n2: dog\n</code></pre> <p>Set comprehensions: Like lists and dictionaries, we can easily construct sets using set comprehensions:</p> <pre><code>from math import sqrt\n\nprint({int(sqrt(x)) for x in range(30)})\n</code></pre> <pre><code>{0, 1, 2, 3, 4, 5}\n</code></pre>"},{"location":"Python/11_python_tutorial/#tuples","title":"Tuples","text":"<ul> <li> A tuple is an (immutable) ordered list of values. </li> <li> A tuple is in many ways similar to a list; one of the most important differences is that tuples can be used as keys in dictionaries and as elements of sets, while lists cannot. Here is a trivial example:</li> </ul> <pre><code>d = {(x, x + 1): x for x in range(10)}  # Create a dictionary with tuple keys\nt = (5, 6)       # Create a tuple\nprint(type(t))\nprint(d[t])       \nprint(d[(1, 2)])\n</code></pre> <pre><code>&lt;class 'tuple'&gt;\n5\n1\n</code></pre> <pre><code># t[0] = 1\n</code></pre> <p>Research:</p> <ul> <li> Creating a tuple</li> <li> Access items in a tuple</li> <li> Negative indexing tuples</li> <li> Using range of indexes</li> <li> Getting the length of items in a tuple</li> <li> Looping through a tuple</li> <li> Checking if an item exists in a tuple</li> <li> How to combine tuples</li> <li> Prove that tuples are immutable</li> </ul>"},{"location":"Python/11_python_tutorial/#functions","title":"Functions","text":"<p> A function is a group of statements that performs a particular task</p> <p> Python functions are defined using the <code>def</code> keyword. For example:</p> <pre><code>def overWeightOrUnderweightOrNormal(weight_kg:float, height_m:float) -&gt; str:\n    '''\n    Tells whether someone is overweight or underweight or normal\n    '''\n    height_m2 = pow(height_m, 2)\n    bmi = weight_kg / height_m2\n    rounded_bmi = round(bmi, 3)\n    if bmi &gt; 24:\n        return 'Overweight'\n    elif bmi &gt; 18:\n        return 'Normal'\n    else:\n        return 'Underweight'\n\noverWeightOrUnderweightOrNormal(67, 1.7)\n</code></pre> <pre><code>'Normal'\n</code></pre> <p>We will often use functions with optional keyword arguments, like this:</p> <pre><code>bmi = calculateBMI(height_m=1.7, weight_kg=67)\n\nprint(bmi)\n</code></pre> <pre><code>23.183\n</code></pre> <pre><code>def greet(name:str='You')-&gt;str:\n    \"\"\"\n    This function greets people by name\n    Example1:\n    &gt;&gt;&gt; greet(name='John Doe')\n    &gt;&gt;&gt; 'Hello John Doe'\n    Example2:\n    &gt;&gt;&gt; greet()\n    &gt;&gt;&gt; 'Hello You'\n    \"\"\"\n    return f'Hello {name}'\n\n# greet('Eva')\n?greet\n</code></pre> <pre><code>\u001b[0;31mSignature:\u001b[0m \u001b[0mgreet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'You'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-&gt;\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m\nThis function greets people by name\nExample1:\n&gt;&gt;&gt; greet(name='John Doe')\n&gt;&gt;&gt; 'Hello John Doe'\nExample2:\n&gt;&gt;&gt; greet()\n&gt;&gt;&gt; 'Hello You'\n\u001b[0;31mFile:\u001b[0m      /tmp/ipykernel_25670/2049930273.py\n\u001b[0;31mType:\u001b[0m      function\n</code></pre>"},{"location":"Python/11_python_tutorial/#classes","title":"Classes","text":"<ul> <li> In python, everything is an object</li> <li> We use classes to help us create new object</li> <li> The syntax for defining classes in Python is straightforward:</li> </ul> <pre><code>class Person:\n    first_name = 'John'\n    last_name = 'Tong'\n    age = 20\n</code></pre> <pre><code># Instantiating a class\nobject1 = Person()\n\nprint(object1.first_name)\nprint(object1.last_name)\nprint(object1.age)\n\nprint(f'object1 type: {type(object1)}')\n</code></pre> <pre><code>John\nTong\n20\nobject1 type: &lt;class '__main__.Person'&gt;\n</code></pre> <pre><code># Instantiating a class\nobject2 = Person()\n\nprint(object2.first_name)\nprint(object2.last_name)\nprint(object2.age)\n</code></pre> <pre><code>John\nTong\n20\n</code></pre> <pre><code>class Person:\n    def __init__(self, first_name, last_name, age):\n        self.first_name = first_name\n        self.last_name = last_name\n        self.age = age\n\n    def greet(self, name):\n        return f'Hello {name}'\n</code></pre> <pre><code>object1 = Person('Juma', 'Shafara', 24)\nprint(object1.first_name)\nprint(object1.last_name)\nprint(object1.age)\nprint(type(object1))\n</code></pre> <pre><code>Juma\nShafara\n24\n&lt;class '__main__.Person'&gt;\n</code></pre> <pre><code>object2 = Person('Eva', 'Ssozi', 24)\nprint(object2.first_name)\nprint(object2.last_name)\nprint(object2.age)\nprint(object2.greet('Shafara'))\nprint(type(object2))\n</code></pre> <pre><code>Eva\nSsozi\n24\nHello Shafara\n&lt;class '__main__.Person'&gt;\n</code></pre> <pre><code>class Student(Person):\n    def __init__(self, first_name, last_name, age, id_number, subjects=[]):\n        super().__init__(first_name, last_name, age)\n        self.id_number = id_number\n        self.subjects = subjects\n\n    def addSubject(self, subject):\n        self.subjects.append(subject) \n</code></pre> <pre><code>student1 = Student('Calvin', 'Masaba', 34, '200045', ['math', 'science'])\n</code></pre> <pre><code>student1.addSubject('english')\n</code></pre> <pre><code>student1.subjects\n</code></pre> <pre><code>['math', 'science', 'english']\n</code></pre> <p>Research:</p> <p> Inheritance: This allows to create classes that inherit the attributes and methods of another class</p> What's on your mind? Put it in the comments!"},{"location":"Python/13_end_of_course_exercise/","title":"End of Course Exam","text":""},{"location":"Python/13_end_of_course_exercise/#objectives","title":"Objectives","text":"<ul> <li>Write and run Python code</li> <li>Practice problem solving using Python code.</li> <li>Learn how to detect programming errors through program testing.</li> <li>Correct/fix syntax errors and bugs.</li> </ul>"},{"location":"Python/13_end_of_course_exercise/#prerequisites","title":"Prerequisites","text":"<ul> <li>Students are expected to be familiar with a text editor.</li> <li>A basic understanding of running and linking in concept.</li> <li>Knowledge in area such as Python language syntax, input output function is recommended.</li> </ul>"},{"location":"Python/13_end_of_course_exercise/#questions","title":"Questions","text":"<ol> <li>Write a Python program to display the following text on the screen.</li> </ol> <pre><code>DATAIDEA\nSir Apollo Kagwa Road,\nKampala,\nUganda\n-------------------\nwww.dataidea.org\n</code></pre> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to assign the number 34.5678 to a variable named \u201cnumber\u201d then display the number rounded to the nearest integer value and next the number rounded to two decimal places.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to identify whether a number input from the keyboard is even or odd. If it is even, the program should display the message \u201cNumber is even\u201d, else it should display \u201cNumber is odd\u201d.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to display the student\u2019s grade based on the following table:</li> </ol> Marks Grade &gt;= 75 A &gt; 50 and &lt;=75 B &gt; 25 and &lt;=50 C &lt;= 25 D <pre><code># your solution\n</code></pre> <ol> <li>Write a program to convert a given character from uppercase to lowercase and vice versa.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program which accepts a number (an amount of money to be paid by a customer in rupees) entered from the keyboard. If the amount is greater than or equal to 1000 rupees, a 5% discount is given to the customer. Then display the final amount that the customer has to pay.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>A car increases it velocity from u ms-1 to v ms-1 within t seconds. Write a program to calculate the acceleration.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to input a temperature reading in either Celsius(c) or Fahrenheit(f) scale and convert it to the other scale. The temperature reading consists of a decimal number followed by letter \u201dF\u201d or \u201df\u201d if the temperature is in Fahrenheit scale or letter \u201dC\u201d or \u201dc\u201d if it is in Celsius scale. You may use a similar format for the output of the program. Note: <code>c = 5( f \u2212 32) / 9</code></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Develop a simple calculator to accept two floating point numbers from the keyboard. Then display a menu to the user and let him/her select a mathematical operation to be performed on those two numbers. Then display the answer. A sample run of you program should be similar to the following:</li> </ol> <pre><code>Enter number 1: 20\nEnter number 2: 12\nMathematical Operation\n-----------------------------------\n1 - Add\n2 - Subtract\n3 - Multiply\n4 - Divide\n-----------------------------------\nEnter your preference: 2\nAnswer : 8.00\n</code></pre> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to display all the integers from 100 to 200.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to calculate the sum of all the even numbers up to 100.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to compute the sum of all integers form 1 to 100.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to calculate the factorial of any given positive integer.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to compute the sum of all integers between any given two numbers. In this program both inputs should be given from the keyboard.</li> </ol> <pre><code># your solution\n</code></pre>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <ol> <li>Modify the above so that it works only for positive integers.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to display the following symbol pattern: <pre><code>*\n**\n***\n****\n*****\n******\n</code></pre></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to display the message \u201cHello World!\u201d 10000 times. The program should allow users to terminate the program at any time by pressing any key before it displays all the 10000 messages.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to display a sine table. The program should display all the sine values from 0 to 360 degrees (at 5 degrees increments) and it should display only 20 rows at a time</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to store marks of 5 students for 5 subjects given through the keyboard. Calculate the average of each students marks and the average of marks taken by all the students</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to calculate the circumference and area of a circle given its radius. Implement calculation of circumference and areas as separate functions.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to read the file \u201cmy file.txt\u201d which has the message: <pre><code>Hello World!\nThis is my first file\n</code></pre></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a Python program to store the message \u201cIntroduction Python Programming\u201d in a file named \u201cmessage.txt\u201d.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to display the following menu on the screen and let the user select a menu item. Based on the user\u2019s selection display the category of software that the user selected program belongs to. <pre><code>Menu\n-----------------------------------\n1 \u2013 Microsoft Word\n2 \u2013 Yahoo messenger\n3 \u2013 AutoCAD\n4 \u2013 Java Games\n-----------------------------------\nEnter number of your preference:\n</code></pre></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Develop a simple telephone directory which saves your friends contact information in a file named directory.txt. The program should have a menu similar to the following: <pre><code>----------------Menu-------------------------\n1. Add new friend.\n2. Display contact info.\n3. Exit\n------------------------------------------------\nEnter menu number:\n</code></pre> When you press \u201c1\u201d it should request you to enter following data: <pre><code>---------New friend info--------\nName : Saman\nPhone-No: 011-2123456\ne-Mail : saman@cse.mrt.ac.lk\n</code></pre> After adding new contact information it should again display the menu. When you press \u201c2\u201d it should display all the contact information stored in the directory.txt file as follows:</li> </ol> <pre><code>--------------Contact info---------------\nName            Tel-No          e-Mail\nKamala          077-7123123     kamala@yahoo.com\nKalani          033-4100101     kalani@gmail.com\nSaman           011-2123456     saman@cse.mrt.ac.lk\n-----------------------------------------\n</code></pre> <pre><code># your solution\n</code></pre> <ol> <li>Given a date as a triplet of numbers (y, m, d), with y indicating the year, m the month (m = 1 for January, m = 2 for February, etc.), and d the day of the month, the corresponding day of the week f (f = 0 for Sunday, f = 1 for Monday, etc.) can be found as follows: <pre><code>(a) if m &lt; 3\n(b) let m = m + 12 and let y = y - 1\n(c) let a = 2m + 6 (m + 1) / 10\n(d) let b = y + y/4 \u2013 y/100 + y/400\n(e) let f 1 = d + a + b + 1\n(t) let f = f 1 mod 7\n(g) stop.\n</code></pre> Write a program that will read a date and print the corresponding day of the week. All divisions indicated above are integer divisions.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to input a series of positive integers and determine whether they are prime. The program should terminate if a negative integer is given as the input. A prime number is a number that is divisible by only one and itself. However one is not considered a prime number.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to find out whether a given number is a perfect number. The program should terminate if a negative integer is given as the input. A perfect number is a number whose factors other than itself add up to itself.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to find and display the minimum and the maximum among 10 numbers entered from the keyboard. Use a single-dimensional array to store the numbers entered. The numbers can be non-integers. An example would be as follows: <pre><code>Enter 10 numbers: 5 7.8 9.6 54 3.4 1.2 3 7 8.8 5\nMinimum = 1.2\nMaximum = 54\n</code></pre></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Suppose there are 4 students each having marks of 3 subjects. Write a program to read the marks from the keyboard and calculate and display the total marks of each student. Use a 2D (two-dimensional) array to store the marks. An example would be as follows: <pre><code>Enter the marks of four students, on four rows:\n50 60 80\n60 75 90\n30 49 99\n66 58 67\n\nTotal marks of four students:\n190\n225\n178\n191\n</code></pre></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to read in two matrices A and B of dimensions 3\u00d74 and 4\u00d73 respectively, and compute and display their product AB (of dimensions 3\u00d73). Assume that the elements of the matrices are integers. Use functions to while implementing this program.</li> </ol> <pre><code># your solution\n</code></pre>"},{"location":"Python/13_end_of_course_exercise/#congratulation","title":"Congratulation!What's on your mind? Put it in the comments!","text":"<p>Congratulations on completing this Python Fundamentals Course</p>"},{"location":"Python/Basics/00_python_programming_outline/","title":"Python3 Outline","text":"Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Python/Basics/00_python_programming_outline/#basics","title":"Basics","text":"<ul> <li>Python Overview</li> <li>Introduction</li> <li>Installing</li> <li>Writing Code</li> <li>Displaying Output</li> <li>Statements</li> <li>Syntax</li> <li>Comments</li> <li>Exercise</li> </ul> <p>Get Started </p>"},{"location":"Python/Basics/00_python_programming_outline/#variables","title":"Variables","text":"<ul> <li>Variables</li> <li>Data Types</li> <li>Numbers</li> <li>Number Methods</li> <li>Strings</li> <li>Type Conversion</li> <li>Python Booleans</li> <li>Exercise</li> </ul> <p>Get Started </p>"},{"location":"Python/Basics/00_python_programming_outline/#operations","title":"Operations","text":"<ul> <li>Operators Intro</li> <li>Arithmetics</li> <li>Assignment</li> <li>Comparison</li> <li>Logical</li> <li>Identity</li> <li>Membership</li> <li>Exercise</li> </ul> <p>Get Started </p>"},{"location":"Python/Basics/00_python_programming_outline/#collections","title":"Collections","text":"<ul> <li>Containers</li> <li>List</li> <li>Tuple</li> <li>Set</li> <li>Dictionary</li> <li>Exercise</li> </ul> <p>Get Started </p>"},{"location":"Python/Basics/00_python_programming_outline/#flow-control","title":"Flow Control","text":"<ul> <li>Functions</li> <li>Lambda functions</li> <li>If else</li> <li>If else shorthand</li> <li>For Loop</li> <li>While Loop</li> <li>Break and Continue</li> <li>Pass</li> <li>Exercise</li> </ul> <p>Get Started </p>"},{"location":"Python/Basics/00_python_programming_outline/#advanced","title":"Advanced","text":"<ul> <li>Functions</li> <li>Classes and Objects</li> <li>Inheritance</li> <li>Variable Scope</li> <li>Formatting Strings</li> <li>Try \u2026 Except</li> <li>Iterators</li> <li>User Input</li> <li>Exercise</li> </ul> <p>Get Started </p>"},{"location":"Python/Basics/00_python_programming_outline/#modules","title":"Modules","text":"<ul> <li>Intro</li> <li>Math</li> <li>Random</li> <li>Date and Time</li> <li>JSON</li> <li>Regular Expressions</li> <li>Exercise</li> </ul> <p>Get Started </p>"},{"location":"Python/Basics/00_python_programming_outline/#working-with-files","title":"Working With Files What's on your mind? Put it in the comments!","text":"<ul> <li>Intro</li> <li>File Handling</li> <li>File Reading</li> <li>File Writing/Creating/Appending</li> <li>File Deleting</li> <li>Exercise</li> </ul> <p>Get Started </p>"},{"location":"Python/Basics/01_basics/","title":"Python Basics","text":""},{"location":"Python/Basics/01_basics/#overview","title":"Overview","text":"<p>This course will teach you the basics and advanced concepts of Python Programming. </p>"},{"location":"Python/Basics/01_basics/#table-of-contents","title":"Table of Contents","text":"<ul> <li> Python Overview</li> <li> Introduction</li> <li> Installing</li> <li> Writing Code</li> <li> Displaying Output</li> <li> Statements</li> <li> Syntax</li> <li> Comments</li> <li> Exercise</li> </ul> <p>You can watch the full crash course on our YouTube channel Python for Data Science </p>"},{"location":"Python/Basics/01_basics/#prerequisites","title":"Prerequisites","text":"<p>What do you need before learning Python?</p> <ol> <li>Computer Literacy</li> <li>Knowledge of installing a software</li> <li>A compiler</li> </ol>"},{"location":"Python/Basics/01_basics/#python-is-easy","title":"Python is Easy","text":"<ul> <li> To learn Python, you don't need any prior knowledge of experience on programming.</li> <li> Python is human readable, making it easier to understand.</li> <li>  Take alook at this example</li> </ul> <pre><code>x = 4\ny = 3\nsumm = x + y\nprint(summ)\n</code></pre> <pre><code>7\n</code></pre> <p>Although we have not taught you how to code in Python yet, you can still easily pick up what the code is doing</p>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Python/Basics/01_basics/#what-is-python","title":"What is Python","text":"<p> Python is a programming language.</p> <p> Python is one of the most popular programming languages</p>"},{"location":"Python/Basics/01_basics/#who-created-python","title":"Who created Python?","text":"<p>Python was created by Guido van Rossum and it was first implemented in 1989</p>"},{"location":"Python/Basics/01_basics/#what-is-python-used-for","title":"What is Python used for?","text":"<p>Python is used for: 1. Web Development 2. Machine Learning 3. Data Science 4. Scripting 5. And many more</p>"},{"location":"Python/Basics/01_basics/#what-is-the-latest-version-of-python","title":"What is the latest version of Python?","text":"<ul> <li> Python 3 is the latest version of Python</li> <li> This tutorial is based on the standards of Python 3</li> </ul>"},{"location":"Python/Basics/01_basics/#installing-python","title":"Installing Python","text":"<p>Before you can run Python on your PC, you need to install it first. </p> <p>To install Python in a PC, go to https://www.python.org/downloads/ then download the latest version.</p> <p>After that, install it just like how you install other apps.</p> <p>Make sure that you check \"Add Python to PATH\" for easier installation.</p>"},{"location":"Python/Basics/01_basics/#writing-python-code","title":"Writing Python Code","text":"<p>In order to learn Python, you need to be able to write and execute code.</p>"},{"location":"Python/Basics/01_basics/#python-console-shell","title":"Python Console (Shell)","text":"<p>Python console also known as shell allows you to execute Python code line by line</p> <p>Assuming that you have already installed Python on your PC, you can access the Python console by opening the command prompt and typing <code>python</code></p> <p>Let's start using the console</p> <p>Type the following and hit enter</p> <p><pre><code>name = 'Juma Shafara'\n</code></pre> Again, type the following and hit enter <pre><code>print(name)\n</code></pre> After that, you should see this <pre><code>Juma Shafara\n</code></pre></p> <p></p>"},{"location":"Python/Basics/01_basics/#python-files","title":"Python Files","text":"<p>Python files are saved with <code>.py</code> file extension</p> <p>You can use any text editor (even notepad) to create Python files</p> <p>Just make sure that you save them with the <code>.py</code> extension, forexample <code>hello.py</code>.</p> <p>Copy this code and save it as <code>hello.py</code>: <pre><code>print('Hello World!')\n</code></pre> To run this Python file on a PC, navigate to the folder where is is located using the command prompt.</p> <p>Then type the following and hit enter <pre><code>python hello.py\n</code></pre> The console should then output: <pre><code>Hello World!\n</code></pre></p>"},{"location":"Python/Basics/01_basics/#integrated-development-enviroment","title":"Integrated Development Enviroment","text":"<p>To continue practicing Python smoothly, I advice using an Integrated Development Environment. This is just a software that has features built in for you to get moving with your coding practice with ease. </p> <p>Examples include:</p> <ul> <li> PyCharm</li> <li> Jupyter Notebook</li> <li> Thonny</li> <li> Visual Studio Code</li> </ul> <p>Watch this video to see how to set up Visual Studio Code for Python Programming</p>"},{"location":"Python/Basics/01_basics/#python-displaying-output","title":"Python Displaying output","text":"<p>To display an output in Python, use the <code>print()</code> function.</p> <pre><code>print('Hello world!')\n</code></pre> <pre><code>Hello world!\n</code></pre> <pre><code>print(27)\n</code></pre> <pre><code>27\n</code></pre> <pre><code>print(3 + 27)\n</code></pre> <pre><code>30\n</code></pre>"},{"location":"Python/Basics/01_basics/#printing-two-objects","title":"Printing two objects","text":"<p>The <code>print()</code> function can be used to print two objects. Eg.</p> <pre><code>print('Hello', 'Juma')\n</code></pre> <pre><code>Hello Juma\n</code></pre> <pre><code>x = 3\ny = 7\nsumm = x + y\nprint('the sum is ', summ)\n</code></pre> <pre><code>the sum is  10\n</code></pre> <pre><code>x = 4; y = 3; print(x + y)\n</code></pre> <pre><code>7\n</code></pre>"},{"location":"Python/Basics/01_basics/#python-statements","title":"Python Statements","text":"<p>A python statement is used to write a value, compute a value, assign a value to a variable, call a functino and many more. Eg.</p> <pre><code>x = 5\ny = 3\nsumm = x + y\nprint(summ)\n</code></pre> <pre><code>8\n</code></pre> <p>In the example above, we have 4 lines of code. In python, each line typically contains one statement</p>"},{"location":"Python/Basics/01_basics/#multiple-statements-in-one-line","title":"Multiple statements in one line","text":"<p>You can also write multiple statements in a single of code. Simply separate the statements with semicolons <code>;</code></p> <pre><code>a = 4; b = 3; sum = a + b; print(sum) \n</code></pre> <pre><code>7\n</code></pre>"},{"location":"Python/Basics/01_basics/#python-syntax","title":"Python Syntax","text":"<p>When coding in Python, we follow a syntax. Syntax is the set of rules followed when writing programs</p>"},{"location":"Python/Basics/01_basics/#python-indentation","title":"Python indentation","text":"<ul> <li> In python, indentation indicates a block(group) of statements</li> <li> Tabs or leading whitespaces are used to compute the indentation level of a line</li> <li> It depends on you whether you want to use tabs or whitespaces, in the example below, we use 2 whitespaces</li> </ul> <pre><code>number1 = 4\nnumber2 = 3\n\nif number1 &gt; number2:\n  x = 'Hello, world'\n  print(x)\n</code></pre> <pre><code>Hello, world\n</code></pre>"},{"location":"Python/Basics/01_basics/#python-comments","title":"Python Comments","text":"<ul> <li>Comments in Python are used to clarify or explain codes</li> <li>Comments are not interpreted by Python, meaning comments will not be executed</li> </ul> <pre><code># this is a comment\nx = 4 \ny = 3\n\n# some comment\n# second comment\n# third comment\n\nprint(x + y) # prints out the sum\n</code></pre> <pre><code>7\n</code></pre>"},{"location":"Python/Basics/01_basics/#excercise","title":"Excercise","text":"<p>Write a Python program to display the following text on the screen.</p> <pre><code>DATAIDEA\nSir Apollo Kagwa Road,\nKampala,\nUganda\n-------------------\nwww.dataidea.org\n</code></pre>"},{"location":"Python/Basics/01_basics/#end-of-first-module","title":"End of first moduleWhat's on your mind? Put it in the comments!","text":"<p>The nice introduction ends here, in the next section, we will look at variables in Python.</p>"},{"location":"Python/Basics/02_variables/","title":"Python Variables","text":""},{"location":"Python/Basics/02_variables/#variables","title":"Variables","text":""},{"location":"Python/Basics/02_variables/#table-of-contents","title":"Table of Contents","text":"<ul> <li> Variables</li> <li> Data Types</li> <li> Numbers</li> <li> Number Methods</li> <li> Strings</li> <li> Type Conversion</li> <li> Python Booleans</li> <li> Exercise</li> </ul> <ul> <li> Varibles are used to store values.</li> <li> To create a variable, use the equal sign `(=)`.</li> </ul> <p>In the examples below, we create varibales name <code>fruit</code> and <code>name</code> and we assign them values <code>'mango'</code> and <code>'viola'</code> respectively</p> <pre><code>fruit = 'mango'\nname = 'voila'\n\n# printing out\nprint(name, ' likes ', fruit )\n</code></pre> <pre><code>voila  likes  mango\n</code></pre>"},{"location":"Python/Basics/02_variables/#rules-to-consider","title":"Rules to consider","text":"<p>Before choosing a variable name, consider the following.</p> <ul> <li> Spaces are not allowed in a variable name eg `my age = 34`</li> <li> A variable name can not start with number eg `1name = 'Chris'`</li> <li> Variable names can not have special characters eg `n@me = 'Comfort'`</li> <li> Are case sensitive. Ie Name and name are not the same!</li> </ul> <p>Examples of good variable names</p> <pre><code>character = 'Peter Griffin'\nmy_favorite_character = 'Stewie Griffin' # snake case\nmyFavoriteCharacter = 'Meg Griffin' # camel case\nMyFavoriteCharacter = 'Brian Griffin' # Pascal case\n</code></pre>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Python/Basics/02_variables/#data-types","title":"Data Types","text":"<p>In this section of the tutorial, you will learn the most basic data types in Python</p>"},{"location":"Python/Basics/02_variables/#numbers","title":"Numbers","text":"<p>These are two basic types of numbers and they are called:</p> <ul> <li> integer(numbers without decimal places)</li> <li> floating point numbers(numbers with decimal places)</li> </ul> <pre><code># python numbers\n# Integers\nage = 45\npopulation = 45000000\n</code></pre> <pre><code># Floating point numbers\nheight = 1.7\nweight = 147.45\n</code></pre> <p>Find More on Numbers Here </p>"},{"location":"Python/Basics/02_variables/#strings","title":"Strings","text":"<p>Strings are simply text. A string must be surrounded by single or double quotes</p> <pre><code># Strings\nname = 'Juma'\nother_name = \"Masaba Calvin\"\nstatement = 'I love coding'\n</code></pre> <p>Find More on Strings Here </p>"},{"location":"Python/Basics/02_variables/#booleans","title":"Booleans","text":"<p>Boolean data type can only have on fo these values: <code>True</code> or <code>False</code></p> Note!<p>The first letter of a Boolean is in upper case.</p> <p>Booleans are often the result of evaluated expressions.</p> <p>Forexample, when you compare two numbers, Python evaluates the expression and returns either <code>True</code> or <code>False</code></p> <pre><code>print(5 == 5)\nprint(10 &gt; 5)\nprint(20 &lt; 10)\n</code></pre> <pre><code>True\nTrue\nFalse\n</code></pre> <p>These are often used in <code>if</code> statments.</p> <p>In this example, if the value of the variable <code>age</code> is more than <code>18</code>, the program will tell the user that they are allowed to enter</p> <pre><code>age = 19\n\nif age &gt; 18:\n    print('You are allowed to enter.')\n</code></pre> <pre><code>You are allowed to enter.\n</code></pre>  Note!<p>You will learn more about if statements later in the course</p>"},{"location":"Python/Basics/02_variables/#checking-the-type-of-a-boolean","title":"Checking the Type of a Boolean","text":"<p>We can check the data type of a Boolean variable using the <code>type()</code> method.</p> <pre><code>married = True\nprint(type(married))\n</code></pre> <pre><code>&lt;class 'bool'&gt;\n</code></pre>"},{"location":"Python/Basics/02_variables/#lists","title":"Lists","text":"<ul> <li> A list is an ordered collection of data</li> <li> It can contain strings, numbers or even other lists</li> <li> Lists are written with square brackets (`[]`)</li> <li> The values in lists (also called elements) are separated by commas (`,`)</li> </ul> <pre><code># Lists\nnames = ['juma', 'john', 'calvin']\nprint(names)\n</code></pre> <pre><code>['juma', 'john', 'calvin']\n</code></pre> <p>Find More on Lists Here </p> <p>A list can contain mixed data types</p> <pre><code>other_stuff = ['mango', True, 38]\nprint(other_stuff)\n</code></pre> <pre><code>['mango', True, 38]\n</code></pre>"},{"location":"Python/Basics/02_variables/#checking-data-types","title":"Checking data types","text":"<p>To check the data type of an object in python, use <code>type(object)</code>, for example, below we get the data type of the object stored in names</p> <pre><code># Which data type\nprint(type(names))\n</code></pre> <pre><code>list\n</code></pre>"},{"location":"Python/Basics/02_variables/#converting-types","title":"Converting Types","text":""},{"location":"Python/Basics/02_variables/#convert-to-string","title":"Convert to String","text":"<p>The <code>str()</code> method returns the string version of a given object</p> <pre><code># convert an integer to a string\nage = 45\nmessage = 'Peter Griffin is '+ str(age) + ' years old'\nprint(message)\n</code></pre> <pre><code>Peter Griffin is 45 years old\n</code></pre>"},{"location":"Python/Basics/02_variables/#convert-to-integer","title":"Convert to Integer","text":"<p>The <code>int()</code> method returns the integer version of the given object.</p> <pre><code># Convert floating point to integer\npi = 3.14159\nprint(int(pi))\n</code></pre> <pre><code>3\n</code></pre>"},{"location":"Python/Basics/02_variables/#convert-to-float","title":"Convert to Float","text":"<p>The <code>float()</code> method returns the floating point version of the given object.</p> <pre><code>side = 4\nprint(float(side))\n</code></pre> <pre><code>4.0\n</code></pre>"},{"location":"Python/Basics/02_variables/#exercise","title":"ExerciseWhat's on your mind? Put it in the comments!","text":"<p>Write a program to assign the number 34.5678 to a variable named \u201cnumber\u201d then display the number rounded to the nearest integer value.</p>"},{"location":"Python/Basics/03_numbers/","title":"Python Numbers","text":""},{"location":"Python/Basics/03_numbers/#numbers","title":"Numbers","text":""},{"location":"Python/Basics/03_numbers/#table-of-contents","title":"Table of Contents","text":"<p>In this notebook, you will learn the following</p> <ul> <li> Number types </li> <li> Number arithmetics</li> <li> Number methods</li> </ul> <p>In python, there are three types of numbers</p> <ul> <li> Integer - `int`</li> <li> Floating Point - `float`</li> <li> Complex - `complex`</li> </ul>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Python/Basics/03_numbers/#types","title":"Types","text":""},{"location":"Python/Basics/03_numbers/#integer","title":"Integer","text":"<p>An integer is a number without decimals</p> <pre><code># Python Numbers: intgers\n\na = 3\nb = 4\nnumber = 5\n\nprint('a:', a)\nprint('b:', b)\nprint('number:', number)\n</code></pre> <pre><code>a: 3\nb: 4\nnumber: 5\n</code></pre>"},{"location":"Python/Basics/03_numbers/#floating-point","title":"Floating Point","text":"<p>A floating point number of just a float is a number with decimals</p> <pre><code># Python Numbers: floating point\na = 3.0\nb = 4.21\nnumber = 5.33\n\nprint('a:', a)\nprint('b:', b)\nprint('number:', number)\n</code></pre> <pre><code>a: 3.0\nb: 4.21\nnumber: 5.33\n</code></pre>"},{"location":"Python/Basics/03_numbers/#complex","title":"Complex","text":"<p>A comple number is an imaginary number. To yield a complex number, append a <code>j</code> o <code>J</code> to a numeric value</p> <pre><code># Python Numbers: complex\n\na = 3j\nb = 5.21j\nnumber = 4 + 5.33j\n\nprint('a:', a)\nprint('b:', b)\nprint('number:', number)\n</code></pre> <pre><code>a: 3j\nb: 5.21j\nnumber: (4+5.33j)\n</code></pre>"},{"location":"Python/Basics/03_numbers/#number-arthmetics","title":"Number Arthmetics","text":"<p>Below are some of the basic arithmetic operations that can be performed with numbers</p> <pre><code># Python Numbers: arthmetics\n\nsummation = 4 + 2\nprint('sum:', summation)\n\ndifference = 4 - 2\nprint('difference:', difference)\n\nproduct = 4 * 2\nprint('product:', product)\n\nquotient = 4 / 2\nprint('quotient:', quotient)\n</code></pre> <pre><code>sum: 6\ndifference: 2\nproduct: 8\nquotient: 2.0\n</code></pre>"},{"location":"Python/Basics/03_numbers/#number-methods","title":"Number Methods","text":"<p>Number methods are special inbuilt functions used to work with numbers</p> <pre><code># sum() can add many numbers at once\nsummation = sum([1,2,3,4,5,6,7,8,9,10])\nprint(summation) # 55\n</code></pre> <pre><code>55\n</code></pre> <pre><code># round() rounds a number to a \n# specified number of decimal places\npi = 3.14159265358979\nrounded_pi = round(pi, 3)\n\nprint('PI: ', pi) # 3.14159265358979\nprint('Rounded PI: ', rounded_pi) # 3.142\n</code></pre> <pre><code>PI:  3.14159265358979\nRounded PI:  3.142\n</code></pre> <pre><code># abs() returns the absolute value of a number\nnumber = -5\nabsolute_value = abs(number)\nprint(absolute_value) # 5\n</code></pre> <pre><code>absolute value of -5 is 5\n</code></pre> <pre><code># pow() returns the value of \n# x to the power of y\nfour_power_two = pow(4, 2)\nprint(four_power_two) # 16\n</code></pre> <pre><code>16\n</code></pre> <pre><code># divmod() returns the quotient and \n# remainder of a division\nquotient, remainder = divmod(10, 3)\nprint(quotient) # 3\nprint(remainder) # 1\n</code></pre> <pre><code>Quotient: 3\nRemainder: 1\n</code></pre>"},{"location":"Python/Basics/03_numbers/#exercise","title":"ExerciseWhat's on your mind? Put it in the comments!","text":"<p>Write a program to assign the number 64.8678 to a variable named \u201cnumber\u201d then display the number rounded to two decimal places.</p>"},{"location":"Python/Basics/03_strings/","title":"Python Strings","text":""},{"location":"Python/Basics/03_strings/#strings","title":"Strings","text":"<p>Strings are simply text. A string must be surrounded by single or double quotes</p> <p>In this notebook, you will learn the following about strings</p>"},{"location":"Python/Basics/03_strings/#table-of-contents","title":"Table of Contents","text":"<ul> <li> How to create strings</li> <li> String methods</li> <li> Exercise</li> </ul>"},{"location":"Python/Basics/03_strings/#house-to-create-strings","title":"House to create strings","text":"<p>We can create a string by writing any text and we enclose it in quotes. An example is as below</p> <pre><code># Strings\nname = 'Juma'\nother_name = \"Masaba Calvin\"\nstatement = 'I love coding'\n</code></pre>"},{"location":"Python/Basics/03_strings/#single-or-double-quotes","title":"Single or double quotes?","text":"<p>Use single quotes when your string contains double quotes, or the vice versa.</p> <pre><code># when to use which quotes\nreport = 'He said, \"I will not go home\"'\n</code></pre>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Python/Basics/03_strings/#string-methods","title":"String Methods","text":"<p>In this lesson, we will study some methods(functions) that can be used to work with strings.</p>"},{"location":"Python/Basics/03_strings/#capitalize-string","title":"Capitalize String","text":"<p>The <code>capitalize()</code> function returns the string where the first letter is in upper case. </p> <p>Note that it doesn't modify the original string. </p> <pre><code>text = 'shafara loves coding'\ncapitalized_text = text.capitalize()\nprint(capitalized_text)\n</code></pre> <pre><code>Shafara loves coding\n</code></pre>"},{"location":"Python/Basics/03_strings/#convert-to-upper-case","title":"Convert to Upper Case","text":"<p>The <code>upper()</code> function returns a copy of the given string but all the letters are in upper case.</p> <p>Note that this does not modify the original string.</p> <pre><code>text = 'JavaScript'\nupper_text = text.upper()\nprint(upper_text)\n</code></pre> <pre><code>JAVASCRIPT\n</code></pre>"},{"location":"Python/Basics/03_strings/#convert-to-lower-case","title":"Convert to Lower Case","text":"<p>The <code>lower()</code> function returns a copy of the given string but all the letter are in lower case.</p> <p>Note that it does not modify the original text/string</p> <pre><code>text = 'JavaScript'\nlower_text = text.lower()\nprint(lower_text)\n</code></pre> <pre><code>javascript\n</code></pre>"},{"location":"Python/Basics/03_strings/#get-the-lenght-of-a-string","title":"Get the lenght of a String","text":"<p>The length of a  string is the number of characters it contains</p> <p>The <code>len()</code> function returns the length of a string. It takes on paramter, the string.</p> <pre><code>text = 'JavaScript'\ntext_length = len(text)\nprint(text_length)\n</code></pre> <pre><code>10\n</code></pre>"},{"location":"Python/Basics/03_strings/#replace-parts-of-a-string","title":"Replace Parts of a String.","text":"<p>The <code>replace()</code> method/function replaces the occurrences of a specified substring with another substring.</p> <p>It doesn't modify the original string.</p> <pre><code>text = 'shafara is a good girl'\ncorrected_text = text.replace('shafara', 'viola')\nprint(corrected_text)\n</code></pre> <pre><code>viola is a good girl\n</code></pre>"},{"location":"Python/Basics/03_strings/#check-if-a-value-is-present-in-a-string","title":"Check if a Value is Present in a String","text":"<p>To check if a substring is present in a string, use the <code>in</code> keyword.</p> <p>It returns <code>True</code> if the substring is found, otherwise <code>False</code>.</p> <pre><code>text = 'shafara loves coding'\nprint('shafara' not in text)\n</code></pre> <pre><code>False\n</code></pre>"},{"location":"Python/Basics/03_strings/#exercise","title":"Exercise","text":"<p>Write a program to convert a given character from uppercase to lowercase and vice versa.</p> What's on your mind? Put it in the comments!"},{"location":"Python/Basics/04_operators/","title":"Python Operators","text":""},{"location":"Python/Basics/04_operators/#what-are-python-operators","title":"What are Python Operators","text":"<p>Operators are symbols that perform operations on operands. Operands can be variables, strings, numbers, booleans etc</p>"},{"location":"Python/Basics/04_operators/#table-of-contents","title":"Table of Contents","text":"<ul> <li> What are Python Operators</li> <li> Arithmetic</li> <li> Assignment</li> <li> Comparison</li> <li> The BMI Example</li> <li> Identity</li> <li> Logical</li> <li> Membership</li> <li> Exercise</li> </ul>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Python/Basics/04_operators/#arithmetic","title":"Arithmetic","text":"<p>Arithemators are symbols that perform mathematical operations on operands</p> Arithmetic Operator Description <code>+</code> Addition <code>-</code> Subraction <code>/</code> Division <code>*</code> Multiplication <code>**</code> Exponentiates <code>%</code> Remainder <code>//</code> Floor Division"},{"location":"Python/Basics/04_operators/#addition-operator","title":"Addition Operator","text":"<p>The addition operator returns the sum of its numerical operands</p> <pre><code># Addition\nx = 10 \ny = 5\n\nsummation = x + y\nprint(summation)\n</code></pre> <pre><code>15\n</code></pre> <p>It can also be used to concatenate or join strings together</p> <pre><code>first_name = 'Juma '\nlast_name = 'Shafara'\n\nfull_name = first_name + last_name\nprint(full_name)\n</code></pre> <pre><code>Juma Shafara\n</code></pre>"},{"location":"Python/Basics/04_operators/#subtraction-operator","title":"Subtraction Operator","text":"<p>The subtraction operator returns the difference of its numerical operands</p> <pre><code># Subraction\nx = 10 \ny = 5\n\ndifference = x - y\nprint(difference)\n</code></pre> <pre><code>5\n</code></pre>"},{"location":"Python/Basics/04_operators/#multiplication-operator","title":"Multiplication Operator","text":"<p>The multiplication operator returns the product of its numerical operands</p> <pre><code># Multiplication\nx = 10 \ny = 5\n\nproduct = x * y\nprint(product)\n</code></pre> <pre><code>50\n</code></pre>"},{"location":"Python/Basics/04_operators/#division-operator","title":"Division Operator","text":"<p>The division operator returns the quotient of its numerical operands.</p> <pre><code># Division\nx = 10 \ny = 5\n\nquotient = x / y\nprint(quotient)\n</code></pre> <pre><code>2.0\n</code></pre>"},{"location":"Python/Basics/04_operators/#exponentiation-operator","title":"Exponentiation Operator","text":"<p>The exponentiation operator raises the left operand to the power of the right operand</p> <pre><code># Exponentiation\nx = 10 \ny = 5\n\nexponent = x ** y\nprint(exponent)\n</code></pre> <pre><code>100000\n</code></pre>"},{"location":"Python/Basics/04_operators/#remainder-operators","title":"Remainder Operators","text":"<p>The remainder operator, also knows as the modulus operator returns the remainder after dividing the left operand by the right operand</p> <pre><code># Remainder\nx = 10 \ny = 5\n\nremainder = x % y\nprint(remainder)\n</code></pre> <pre><code>0\n</code></pre>"},{"location":"Python/Basics/04_operators/#floor-division-operator","title":"Floor Division Operator","text":"<p>The floor division rounds down the quotient of its numerical operands to the nearest whole number.</p> <pre><code># Floor Division\nx = 10 \ny = 5\n\nfloor = 10 // 4\nprint(floor)\n</code></pre> <pre><code>2\n</code></pre>"},{"location":"Python/Basics/04_operators/#operator-sequence","title":"Operator Sequence","text":"<p>Operator sequence describes the order of performed operations in an arithmetic expression.</p> <pre><code>answer = 10 * 3 / 2 + 1\nprint(answer)\n</code></pre> <pre><code>16.0\n</code></pre>"},{"location":"Python/Basics/04_operators/#assignment","title":"Assignment","text":"<p>Assignment operators are used to assign values to variables.</p> Name Operation Same As Assignment <code>x = y</code> <code>x = y</code> Addition Ass <code>x += y</code> <code>x = x + y</code> Subtraction Ass <code>x -= y</code> <code>x = x - y</code> Mult Ass <code>x *= y</code> <code>x = x * y</code> Division Ass <code>x /= y</code> <code>x = x / y</code> Expo Ass <code>x **= y</code> <code>x = x ** y</code> Remainder Ass <code>x %= y</code> <code>x = x % y</code> Floor Div Ass <code>x //= y</code> <code>x = x // y</code>"},{"location":"Python/Basics/04_operators/#assignment_1","title":"Assignment","text":"<p>The assignment operator assigns a value to a variable</p> <pre><code># Assignment\nnumber = 10\nprint(number)\n</code></pre> <pre><code>10\n</code></pre>"},{"location":"Python/Basics/04_operators/#addition-assignment","title":"Addition Assignment","text":"<p>The addition assignment operator adds the left and right operands and assigns the sum to the left operand (the variable)</p> <pre><code># Addition Ass\nx += 5 # x = x + 5 =&gt; x = 10 + 5 =&gt; x = 15\nprint(x)\n</code></pre> <pre><code>15\n</code></pre>"},{"location":"Python/Basics/04_operators/#subtraction-assignment","title":"Subtraction Assignment","text":"<p>The subtraction assignment operators deducts the right operand and assigns the difference to the left operand (the variable)</p> <pre><code># Subraction Ass\nx = 10\nx -= 5 # x = x - 5 =&gt; x = 10 - 5 =&gt; x = 5\nprint(x)\n</code></pre> <pre><code>5\n</code></pre>"},{"location":"Python/Basics/04_operators/#comparison","title":"Comparison","text":"<p>A comparison operator compares its operands and returns a Boolean value based on whether the comparison is  True of False</p> Name Operation Equality <code>==</code> Inequality <code>!=</code> Greater than <code>&gt;</code> Less than <code>&lt;</code> Greater or equal <code>&gt;=</code> Less or equal <code>&lt;=</code>"},{"location":"Python/Basics/04_operators/#equality","title":"Equality","text":"<p>The equality operator compares two values and returns <code>True</code> if the operands are equal, otherwise returns <code>False</code></p> <pre><code># Equality \nprint('Voila' == 'Viola') # False\nprint(5 == 5) # True\n</code></pre> <pre><code>False\nTrue\n</code></pre>"},{"location":"Python/Basics/04_operators/#inequality","title":"Inequality","text":"<p>The inequality operator compares two values and returns <code>True</code> if the operands are Not equal, otherwise returns <code>False</code></p> <pre><code># Inequality \nprint('Voila' != 'Viola')\nprint(10 != 10)\n</code></pre> <pre><code>True\nFalse\n</code></pre>"},{"location":"Python/Basics/04_operators/#greater-than","title":"Greater than","text":"<p>The greater than operator returns <code>True</code> if the left operand is greater than the right operand, otherwise returns <code>False</code>.</p> <pre><code>print(8 &gt; 4) # True\nprint(5 &gt; 10) # False\n</code></pre> <pre><code>True\nFalse\n</code></pre>"},{"location":"Python/Basics/04_operators/#less-than","title":"Less than","text":"<p>The less than operator returns <code>True</code> if the left operand is less than the right operand, otherwise returns <code>False</code></p> <pre><code>print(10 &lt; 15) # True\nprint(20 &lt; 10) # False\n</code></pre> <pre><code>True\nFalse\n</code></pre>"},{"location":"Python/Basics/04_operators/#greater-than-or-equal-to","title":"Greater than or equal to","text":"<p>The greater than or equal to operator returns <code>True</code> if the left operand is greater than or equal to the right operand, otherwise returns <code>False</code></p> <pre><code># Greater or Equal\n34 &gt;= 43\n</code></pre> <pre><code>False\n</code></pre>"},{"location":"Python/Basics/04_operators/#less-than-or-equal-to","title":"Less than or equal to","text":"<p>The less than or equal to operator returns <code>True</code> if the left operand is less than or equal to the right operand, otherwise returns <code>False</code></p> <pre><code>print(10 &lt;= 15) # True\nprint(10 &lt;= 10) # True\nprint(20 &lt;= 10) # False\n</code></pre> <pre><code>True\nTrue\nFalse\n</code></pre>"},{"location":"Python/Basics/04_operators/#the-body-mass-index-example","title":"The Body Mass Index Example","text":"<p>In this example, we use the operators to calculate the body mass index of a person.</p> <pre><code>weight = 56\nheight = 1.5\n\nbmi = weight / (height**2)\n\nprint('BMI: ', bmi)\n</code></pre> <pre><code>BMI:  24.88888888888889\n</code></pre>"},{"location":"Python/Basics/04_operators/#identity","title":"Identity","text":"<p>Identity operators are used to compare two values to determine if they point to the same object</p> Operator Name <code>is</code> The is operator <code>is not</code> The is not operator"},{"location":"Python/Basics/04_operators/#the-is-operator","title":"The <code>is</code> operator","text":"<p>This operator returns <code>True</code> if both operands point to the same object.</p> <pre><code># Example 1\nx = 5\ny = 4\n\nprint(x is y)\n</code></pre> <pre><code>False\n</code></pre> <pre><code># Example 2\nx = 5 \ny = 4\nz = x\n\nprint(x is y) # False\nprint(x is z) # True\n</code></pre> <pre><code>False\nTrue\n</code></pre>"},{"location":"Python/Basics/04_operators/#the-is-not-operator","title":"The <code>is not</code> operator","text":"<p>This operator return <code>True</code> if both operands do Not point to the same object in memory</p> <pre><code># Example\n# is\nx = 5\ny = 4\nz = x \n\nprint(x is not y) # True\nprint(x is not z) # False\n</code></pre> <pre><code>True\nFalse\n</code></pre>"},{"location":"Python/Basics/04_operators/#logical","title":"Logical","text":"<p>Logical operators are commonly used with Booleans. In Python, there are 3 logical operators</p> Operator Description <code>and</code> Logical and operator <code>or</code> Logical or <code>not</code> Logical not"},{"location":"Python/Basics/04_operators/#logical-and","title":"Logical <code>and</code>","text":"<p>The logical <code>and</code> operator returns <code>True</code> if both operands are <code>True</code></p> <pre><code># Example\n# Logical and\nx = 4\nprint(x &gt; 3 and 8 &lt; x)\n#               True and False =&gt; False\n</code></pre> <pre><code>False\n</code></pre>"},{"location":"Python/Basics/04_operators/#logical-or","title":"Logical <code>or</code>","text":"<p>The logical <code>or</code> operator returns <code>True</code> if one of the operands is <code>True</code></p> <pre><code># Logical or\ny = 7\nexpression_2 = 10 &gt; y or 4 &gt; y\n#                   True or False =&gt; True\nprint(expression_2)\n</code></pre> <pre><code>True\n</code></pre>"},{"location":"Python/Basics/04_operators/#logical-not","title":"Logical <code>not</code>","text":"<p>The logical <code>not</code> operator returns <code>True</code> if the operand is <code>False</code>, otherwise returns <code>False</code> if the operand is <code>True</code></p> <pre><code># Logical not\nz = 8\nexpression_3 = not(10 == z)\n#               not False =&gt; True\nprint(expression_3)\n</code></pre> <pre><code>True\n</code></pre>"},{"location":"Python/Basics/04_operators/#membership","title":"Membership","text":"<p>Membership operators are used to check if a sequence is present in an object like a string, list etc</p> Operator Name <code>in</code> The in operator <code>not in</code> The not in operator"},{"location":"Python/Basics/04_operators/#the-in-operator","title":"The <code>in</code> operator","text":"<p>The <code>in</code> operator returns <code>True</code> if a sequence or value is present in an object</p> <pre><code># Example\nname = 'Tinye Robert'\nprint('Robert' in name)\n</code></pre> <pre><code>True\n</code></pre>"},{"location":"Python/Basics/04_operators/#the-not-in-operator","title":"The <code>not in</code> operator","text":"<p>The <code>not in</code> operator returns <code>True</code> if a sequence or value is NOT present in an object</p> <pre><code># Example\nname = 'Tinye Robert'\nprint('Robert' not in name)\n</code></pre> <pre><code>False\n</code></pre>"},{"location":"Python/Basics/04_operators/#exercise","title":"Exercise","text":"<p>A car increases it velocity from u ms-1 to v ms-1 within t seconds. Write a program to calculate the acceleration.</p> What's on your mind? Put it in the comments!"},{"location":"Python/Basics/05_containers/","title":"Python Containers","text":""},{"location":"Python/Basics/05_containers/#table-of-contents","title":"Table of Contents","text":"<ul> <li> What is an object?</li> <li> Containers</li> <li> Lists</li> <li> Tuple</li> <li> Sets</li> <li> Dictionaries</li> <li> Free Dictionary Tip</li> <li> Exercise</li> </ul> <p>Containers in Python are objects that contain other objects</p>"},{"location":"Python/Basics/05_containers/#what-is-an-object","title":"What is an object?","text":"<p>In python, everything is an object. Even the simplest strings and numbers are considered as objects</p> <pre><code>x = 'My favorite number is '\ny = 21\n\nprint(x, y)\n</code></pre> <pre><code>My favorite number is  21\n</code></pre>"},{"location":"Python/Basics/05_containers/#containers","title":"Containers","text":"<p>Containers (also called collections) are objects that contain objects</p> <p>In the example below, the <code>fruits</code> variable contains three strings</p> <pre><code>fruits = ['bananas', 'apples', 'grapes']\n\nprint(fruits)\n</code></pre> <pre><code>['bananas', 'apples', 'grapes']\n</code></pre>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Python/Basics/05_containers/#lists","title":"Lists","text":"<ul> <li> A python list is an ordered container</li> <li> A list is created by using square brackets (`[]`)</li> <li> Objects are poaced inside those brackets and are separated by commas (`,`)</li> </ul> <pre><code>pets = ['dog', 'cat', 'rabbit', 'monkey']\nprint(pets)\nprint(type(pets))\n</code></pre> <pre><code>['dog', 'cat', 'rabbit', 'monkey']\n&lt;class 'list'&gt;\n</code></pre> <p>A list can contain mixed data types.</p> <pre><code>x = ['dog', 21, True]\nprint(x)\n</code></pre> <pre><code>['dog', 21, True]\n</code></pre>"},{"location":"Python/Basics/05_containers/#indexing","title":"Indexing","text":"<ul> <li>Indexing is used to access items of a list</li> <li>Indexing uses square brackets and numbers to access individual items of a list</li> <li>Where <code>0</code> refers to the first item, 1 refers to the second item, and so on</li> </ul> <pre><code># indexing\npets = ['dog', 'cat', 'rabbit', 'monkey']\nprint(pets[0])\nprint(pets[1])\nprint(pets[2])\n</code></pre> <pre><code>dog\ncat\nrabbit\n</code></pre>"},{"location":"Python/Basics/05_containers/#negative-indexing","title":"Negative Indexing","text":"<p>Negative indexing is used to access the items of a list using negative numbers.</p> <p>Where <code>-1</code> refers to the last item, <code>-2</code> refers to the second last item, and so on.</p> <pre><code># negative indexing\npets = ['dog', 'cat', 'rabbit', 'monkey']\nprint(pets[-1])\nprint(pets[-2])\nprint(pets[-3])\n</code></pre> <pre><code>monkey\nrabbit\ncat\n</code></pre>"},{"location":"Python/Basics/05_containers/#range-of-indexes","title":"Range of Indexes","text":"<p>By using a colon ie <code>:</code>, we can access a range of items at once.</p> <p>Simply separate two indexes using the colon.</p> <p>The first index is the start of the range, while the second index is the end of the range (not included)</p> <pre><code>#range of indexes\npets = ['dog', 'cat', 'rabbit', 'monkey']\nprint(pets[1:3])\n</code></pre> <pre><code>['cat', 'rabbit']\n</code></pre> <p>If you don't specify the last index, the range ends with the last item of the list</p> <p>In this case, the range includes the last item.</p> <pre><code>pets = ['dog', 'cat', 'rabbit', 'monkey']\nprint(pets[2:])\n</code></pre> <pre><code>['rabbit', 'monkey']\n</code></pre>"},{"location":"Python/Basics/05_containers/#adding-items-to-a-list","title":"Adding items to a list","text":"<p>The <code>append()</code> method adds an item to the end of the list.</p> <p>In this example, we will add <code>'fish'</code> to our pets list</p> <pre><code>pets = ['dog', 'cat']\npets.append('fish')\n\nprint(pets)\n</code></pre> <pre><code>['dog', 'cat', 'fish']\n</code></pre> <p>The <code>insert()</code> method inserts an item at the specified index.</p> <p>In this case, we will insert <code>'rabbit'</code> to the index 0 and <code>'hamster'</code> to index 2</p> <pre><code>pets = ['dog', 'cat', 'monkey']\npets.insert(0, 'rabbit')\npets.insert(2, 'hamster')\n\nprint(pets)\n</code></pre> <pre><code>['rabbit', 'dog', 'hamster', 'cat', 'monkey']\n</code></pre>"},{"location":"Python/Basics/05_containers/#deleting-items-from-a-list","title":"Deleting Items from a list","text":"<p>The <code>pop()</code> method removes the last item from a list </p> <pre><code>pets = ['dog', 'cat', 'rabbit', 'monkey']\npets.pop()\n\nprint(pets)\n</code></pre> <pre><code>['dog', 'cat', 'rabbit']\n</code></pre> <p>The <code>remove()</code> method removes the specified item value.</p> <pre><code>pets = ['dog', 'cat', 'rabbit', 'monkey']\npets.remove('rabbit')\n\nprint(pets)\n</code></pre> <pre><code>['dog', 'cat', 'monkey']\n</code></pre> <p>To delete a specified index, use the <code>del</code> keyword</p> <pre><code>pets = ['dog', 'cat', 'rabbit', 'monkey']\ndel pets [2]\n\nprint(pets)\n</code></pre> <pre><code>['dog', 'cat', 'monkey']\n</code></pre>"},{"location":"Python/Basics/05_containers/#getting-the-length-of-a-list","title":"Getting the length of a list","text":"<p>The length of a list refers to the number of items in a list, use the <code>len()</code> method</p>"},{"location":"Python/Basics/05_containers/#changing-an-items-value","title":"Changing an item's value","text":"<p>To change an item's value, access the index first and use the assignment operator.</p> <pre><code>pets = ['dog', 'cat', 'rabbit', 'monkey']\npets[2] = 'fish'\n\nprint(pets)\n</code></pre> <pre><code>['dog', 'cat', 'fish', 'monkey']\n</code></pre>"},{"location":"Python/Basics/05_containers/#homework","title":"Homework","text":"<ul> <li>Check if an item exist</li> </ul>"},{"location":"Python/Basics/05_containers/#extending-a-list","title":"Extending a list","text":"<p>The <code>extend()</code> methods adds all items from one list to another</p> <pre><code>pets = ['dog', 'cat']\nother_pets = ['rabbit', 'monkey']\npets.extend(other_pets)\nprint(pets)\n</code></pre> <pre><code>['dog', 'cat', 'rabbit', 'monkey']\n</code></pre>"},{"location":"Python/Basics/05_containers/#tuple","title":"Tuple","text":"<ul> <li> Python tuple is an ordered container</li> <li> Its the same as a list but the items of tuples cannot be changed</li> <li> We create a tuple using round brackets `()`</li> </ul> <pre><code>pets = ('dog', 'cat', 'rabbit')\nprint(pets)\nprint(type(pets))\n</code></pre> <pre><code>('dog', 'cat', 'rabbit')\n&lt;class 'tuple'&gt;\n</code></pre>"},{"location":"Python/Basics/05_containers/#sets","title":"Sets","text":"<ul> <li> A set is a container/collection that is unordered and immutable</li> <li> We create a set using `{}`</li> <li> The objects are placed inside those brackets and are separated by commas </li> </ul> <pre><code>pets = {'dog', 'cat', 'rabbit'}\nprint(pets)\n</code></pre> <pre><code>{'rabbit', 'dog', 'cat'}\n</code></pre> <p>A set can contain mixed data types, but can NOT contain mutable items like lists, sets and dictionaries</p> <pre><code># A set can contain objects of different data types\nmixed = {'dog', 21, True}\nprint(mixed)\nprint(type(mixed))\n</code></pre> <pre><code>{True, 'dog', 21}\n&lt;class 'set'&gt;\n</code></pre>"},{"location":"Python/Basics/05_containers/#accessing-set-elements","title":"Accessing set elements","text":"<ul> <li>Unlike lists and tuples, you cannot access the items in a set using indexes</li> <li>This is because a set is unordered and not indexed</li> <li>However, we can use a <code>for</code> loop to access all its items one-by-one</li> </ul> <p>Note: We'll discuss a for loop in the next chapter</p> <pre><code># Accessing\npets = {'dog', 'cat', 'rabbit'}\nfor pet in pets:\n    print(pet)\n</code></pre> <pre><code>rabbit\ndog\ncat\n</code></pre>"},{"location":"Python/Basics/05_containers/#adding-elements-to-a-set","title":"Adding elements to a set","text":"<p>To add items to a set, use the <code>add()</code> or <code>update()</code> method.</p> <p>The <code>add()</code> method adds one item to a set.</p> <pre><code># Adding items to a set\npets = {'dog', 'cat', 'rabbit'}\npets.add('fish')\nprint(pets)\n</code></pre> <pre><code>{'rabbit', 'dog', 'cat', 'fish'}\n</code></pre>"},{"location":"Python/Basics/05_containers/#changing-an-item","title":"Changing an item","text":"<p>The items of a set can NOT be changed because a set is immutable or unchangeable.</p>"},{"location":"Python/Basics/05_containers/#removing-set-elements","title":"Removing set elements","text":"<p>To remove an item from a set, use the <code>remove()</code> method.</p> <p>You should specify the value of the item you want to remove.</p> <pre><code># Removing items from a set\npets = {'dog', 'cat', 'rabbit'}\npets.remove('cat') # remove\nprint(pets)\n</code></pre> <pre><code>{'rabbit', 'dog'}\n</code></pre> <p>You can also use the <code>discard()</code> method.</p> <pre><code>pets = {'dog', 'cat', 'rabbit'}\npets.discard('rabbit') #discard\nprint(pets)\n</code></pre> <pre><code>{'dog', 'cat'}\n</code></pre> <p>The difference between the <code>remove()</code> and <code>discard()</code> methods is that the <code>discard()</code> method does not raise an error if the specified item is not present.</p> <p>You can also use the <code>pop()</code> method to remove an item.</p> <p>But we cannot determine which item will be removed because a set is unordered.</p> <pre><code>pets = {'dog', 'cat', 'rabbit'}\npets.pop() # pop removes the last item from the set\nprint(pets)\n</code></pre> <pre><code>{'dog', 'cat'}\n</code></pre>"},{"location":"Python/Basics/05_containers/#homework_1","title":"Homework","text":"<ul> <li>Find the length of a set</li> <li>Check if an element exists</li> <li>Combine sets</li> </ul>"},{"location":"Python/Basics/05_containers/#getting-the-difference-between-sets","title":"Getting the difference between sets","text":"<p>To get the difference between two sets, use the subtraction operator (<code>-</code>)</p> <pre><code># Getting the difference\nfirst_numbers = {1, 2, 3, 4}\nsecond_numbers = {3, 4, 5, 6}\n\ndifference = first_numbers - second_numbers\n# another way\ndifference2 = first_numbers.difference(second_numbers)\nprint(difference)\n</code></pre> <pre><code>{1, 2}\n</code></pre>"},{"location":"Python/Basics/05_containers/#dictionaries","title":"Dictionaries","text":"<p>A dictionary is an unordered and mutable colletion of items.</p> <p>A dictionary is written with curly brackets.</p> <p>Each item in a dictionary contains a <code>key/value</code> pair.</p> <pre><code># Creating \nperson = {\n    'first_name': 'Voila', \n    'last_name': 'Akullu',\n    'age': 16\n    }\n\nprint(person)\n</code></pre> <pre><code>{'first_name': 'Voila', 'last_name': 'Akullu', 'age': 16}\n</code></pre> <p>In the above example we have 3 items:</p> <ul> <li> The first item has a key name of `'first_name'`, and its value is `'Viola'`.</li> <li> The second item has a key name of `'last_name'`, and its value is `'Akullu'`</li> <li> The third item has a key name of `'age'`, and its value is `30`</li> </ul>"},{"location":"Python/Basics/05_containers/#accessing-items","title":"Accessing items","text":"<p>To access an item, specify the key name of an item inside square brackets.</p> <pre><code># Accessing items\nperson = {\n    'first_name': 'Voila', \n    'last_name': 'Akullu',\n    'age': 16\n    }\n\nprint(person['last_name'])\n</code></pre> <pre><code>Akullu\n</code></pre> <p>If you try to access an item using a key name that does not exist, an error will be raised </p>"},{"location":"Python/Basics/05_containers/#adding-items","title":"Adding items","text":"<p>To add new items, specify a new index key name inside the square brackets and assign a  value using the assignment operator.</p> <pre><code># Adding items \nperson = {\n    'first_name': 'Voila', \n    'last_name': 'Akullu',\n    'age': 16\n    }\nperson['middle_name'] = 'Vee'\n\nprint(person)\n</code></pre> <pre><code>{'first_name': 'Voila', 'last_name': 'Akullu', 'age': 16, 'middle_name': 'Vee'}\n</code></pre>"},{"location":"Python/Basics/05_containers/#changing-an-items-value_1","title":"Changing an item's value","text":"<p>To change an item, refer to its key name using square brackets and use the assignment operator.</p> <pre><code># Changing items \nperson = {\n    'first_name': 'Voila', \n    'last_name': 'Akullu',\n    'age': 16\n    }\nperson['last_name'] = 'Kibekityo'\n\nprint(person)\n</code></pre> <pre><code>{'first_name': 'Voila', 'last_name': 'Kibekityo', 'age': 16}\n</code></pre>"},{"location":"Python/Basics/05_containers/#removing-an-items-value","title":"Removing an Item's value","text":"<p>To remove an item, use the pop() method.</p> <p>The <code>pop()</code> method removes an item with the specified key name.</p> <pre><code># Remove items\nperson = {\n    'first_name': 'Voila', \n    'last_name': 'Akullu',\n    'age': 16\n    }\n\nperson.pop('age')\nprint(person)\n</code></pre> <pre><code>{'first_name': 'Voila', 'last_name': 'Akullu'}\n</code></pre> <p>Another way to remove an item is to use the <code>del</code> keyword.</p> <pre><code># del keyword\nperson = {\n    'first_name': 'Voila', \n    'last_name': 'Akullu',\n    'age': 16\n    }\n\ndel person['age']\nprint(person)\n</code></pre> <pre><code>{'first_name': 'Voila', 'last_name': 'Akullu'}\n</code></pre>"},{"location":"Python/Basics/05_containers/#homework_2","title":"Homework","text":"<ul> <li>Check if an element exists</li> <li>Find the length of a dictionary</li> </ul>"},{"location":"Python/Basics/05_containers/#nested-dictionary","title":"Nested Dictionary","text":"<p>A dictionary can contain another dictionary.</p> <pre><code># Nesting dictionaries\nemployees = {\n    'manager': {\n        'name': 'Akullu Viola',\n        'age': 29\n    },\n    'programmer': {\n        'name': 'Juma Shafara',\n        'age': 30\n    }\n}\n\nprint(employees)\n</code></pre> <pre><code>{'manager': {'name': 'Akullu Viola', 'age': 29}, 'programmer': {'name': 'Juma Shafara', 'age': 30}}\n</code></pre> <p>To access an item in a nested dictionary, access the key name of the dictionary then the key name of the item</p> <pre><code># Accessing nested dictionary\nemployees = {\n    'manager': {\n        'name': 'Akullu Viola',\n        'age': 29\n    },\n    'programmer': {\n        'name': 'Juma Shafara',\n        'age': 30\n    }\n}\n\nprogrammer = employees['programmer']\nprint(programmer['name'])\n</code></pre> <pre><code>Juma Shafara\n</code></pre>"},{"location":"Python/Basics/05_containers/#free-dictionary-tip","title":"Free Dictionary Tip","text":"<pre><code># Using a dictionary constructer\nnames = ('a1', 'b2', 'c3')\ndictionary = dict(names)\nprint(dictionary)\n</code></pre> <pre><code>{'a': '1', 'b': '2', 'c': '3'}\n</code></pre>"},{"location":"Python/Basics/05_containers/#exercise","title":"Exercise","text":"<p>Write a program to store marks of 5 students for 5 subjects given through the keyboard. Calculate the average of each students marks and the average of marks taken by all the students</p> What's on your mind? Put it in the comments!"},{"location":"Python/Basics/06_flow_control/","title":"Python Flow Control","text":"<p>Python control flow tools change the flow of how code is executed by the Python interpreter.</p> <p>Since the Python interpreter executes code in a line-by-line manner, Python control flow tools help dictate what line(s) of code should run in a Python program. There are different types of control flow tools available to us in Python and we will go through them in detail in this lesson.</p>"},{"location":"Python/Basics/06_flow_control/#table-of-contents","title":"Table of Contents","text":"<p>In this notebook, you'll learn the following</p> <ul> <li> Functions</li> <li> Loops</li> <li> Conditional Statements</li> </ul>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Python/Basics/06_flow_control/#functions","title":"Functions","text":"<p>A function in python is a group statements that perform a particular task</p> <p>The function below calculates and returns one's body mass index rounded to 2 decimal places</p> <pre><code># This function calculates Body Mass Index\ndef calculateBodyMassIndex(weight_kg, height_m):\n\n    body_mass_index = weight_kg / pow(height_m, 2)\n    rounded_bmi = round(body_mass_index, 2)\n\n    return rounded_bmi\n</code></pre> <pre><code># lets try\ncalculateBodyMassIndex(67, 1.6)\n</code></pre> <pre><code>26.17\n</code></pre>"},{"location":"Python/Basics/06_flow_control/#creating-a-function","title":"Creating a function","text":"<p>To create a function, we need the following:</p> <ul> <li> The `def` keyword</li> <li> A function name</li> <li> Round brackets `()` and a colon `:`</li> <li> A function body- a group of statements</li> </ul> <pre><code>def greeter():\n    message = 'Hello'\n    print(message)\n</code></pre>  Note!A group of statements must have the same indentation level, in the example above, 4 whitespaces were used to indent the function body"},{"location":"Python/Basics/06_flow_control/#calling-a-function","title":"Calling a Function","text":"<ul> <li> To execute a function, it needs to be called</li> <li> To call a function, use its function name with parentheses `()`</li> </ul> <pre><code>greeter()\n</code></pre> <pre><code>Hello\n</code></pre>"},{"location":"Python/Basics/06_flow_control/#function-parametersarguments","title":"Function Parameters/Arguments","text":"<p> When calling a function, we can pass data using parameters/arguments</p> <p> A parameter is a variable declared in the function. In the example below, <code>number1</code> and <code>number2</code> are parameter</p> <p> The argument is the value passed to the function when its called. In the example below <code>3</code> and <code>27</code> are the arguments</p> <pre><code># define the function\ndef addNumbers(number1, number2):\n    sum = number1 + number2\n    print(sum)\n\n# Call the function\naddNumbers(3, 27)\n</code></pre> <pre><code>30\n</code></pre> <pre><code># setting a default argument\ndef greet(name='you'):\n    message = 'Hello ' + name\n    print(message)\n\ngreet('Tinye')\ngreet()\n</code></pre> <pre><code>Hello Tinye\nHello you\n</code></pre>"},{"location":"Python/Basics/06_flow_control/#default-arguments","title":"Default Arguments:","text":"<p>A function can have default arguments.</p> <p>It can be done using the assignment operator (<code>=</code>).</p> <p>If you don't pass the argument, the default argument will be used instead.</p> <pre><code>def hello(name = 'Agaba'):\n    print('Hello ' + name)\n\nhello('John') # calling with John\nhello() # calling with no name\n</code></pre> <pre><code>Hello John\nHello Agaba\n</code></pre>"},{"location":"Python/Basics/06_flow_control/#return-statement","title":"Return Statement","text":"<p>The <code>return</code> statement is used to return a value to a function caller</p> <pre><code>def addNumbers(number1, number2):\n    sum = number1 + number2\n    return sum\n\nsummation = addNumbers(56, 4)\nprint(summation)\n</code></pre> <pre><code>60\n</code></pre>  Note!<p>The return statement stops the execution of a function.</p> <p>### lambda functions   <ul> <li> Lambda functions (also called anonymous functions) are functions that donot have names</li> <li> The body of a lambda function can only have one expression, but can have multiple arguments</li> <li> The result of the expression is automatically returned</li> </ul> </p> <p>Syntax: <pre><code>lambda parameters: expression\n</code></pre></p> <pre><code># Example of lambda function\ncalculateBMI = lambda weight_kg, height_m: round((weight_kg/(height_m ** 2)), 2)\n\n# Calling a labda function\ncalculateBMI(67, 1.7)\n</code></pre> <pre><code>23.18\n</code></pre>  Note!<p>In the example above, the body mass index is automatically return, even without using the return statement</p>"},{"location":"Python/Basics/06_flow_control/#practice-functions","title":"Practice functions","text":""},{"location":"Python/Basics/06_flow_control/#calculate-cgpa","title":"Calculate CGPA","text":"<pre><code># Assume 4 course units\n# 1. Math - A\n# 2. Science - B\n# 3. SST - B\n# 4. English - C\n\n\ndef calculate_CGPA(GPs_list, CUs_list):\n    length = len(GPs_list)\n    product_sum = 0\n\n    for item in range(length):\n        product_sum += GPs_list[item] * CUs_list[item]\n\n    CUs_sum = sum(CUs_list)\n\n    CGPA = product_sum / CUs_sum\n\n    return CGPA\n\n# calculate_CGPA(4, 5)\n</code></pre>"},{"location":"Python/Basics/06_flow_control/#get-someones-age-given-birth-month-and-year","title":"Get someones age given birth month and year","text":"<pre><code>def getAge(month, year):\n    month_diff = 12 - month\n    year_diff = 2023 - year\n\n    return str(year_diff) + ' years ' + str(month_diff) + ' months'  \n\nage = getAge(year=2000, month=10) # keyword argument\nage2 = getAge(10, 2000) # positional argument\n\nprint(age)\n</code></pre> <pre><code>23 years 2 months\n</code></pre>"},{"location":"Python/Basics/06_flow_control/#loops","title":"Loops","text":"<ul> <li> Loops are used to repetitively execute a group of statements</li> <li> we have 2 types, `for` and `while` loop</li> </ul>"},{"location":"Python/Basics/06_flow_control/#for-loop","title":"For Loop","text":"<p>A <code>for</code> loop is used to loop through or iterate over a sequence or iterable objects</p> <p>Syntax: <pre><code>for variable in sequence:\n    statements\n</code></pre></p>"},{"location":"Python/Basics/06_flow_control/#looping-through-a-list","title":"Looping through a list","text":"<p>The for loop is commonly used with lists.</p> <pre><code>pets = ['cat', 'dog', 'rabbit']\n# iterate through pets\nfor pet in pets:\n    print(pet)\n</code></pre> <pre><code>cat\ndog\nrabbit\n</code></pre> <p>Here's another example to convert many weights from kilograms(kgs) to pounds(pds)</p> <pre><code># convert all weights in list from kg to pounds\nweights_kg = [145, 100, 76, 80]\nweights_pds = []\n\nfor weight in weights_kg:\n    pounds = weight * 2.2\n    rounded_pds = round(pounds, 2)\n    weights_pds.append(rounded_pds)\n\nprint(weights_pds)\n</code></pre> <p>This example displays all letters in my name.</p> <pre><code># Display all letters in a name\nname = 'Shafara'\n\nfor letter in name:\n    print(letter)\n</code></pre> <pre><code>S\nh\na\nf\na\nr\na\n</code></pre>"},{"location":"Python/Basics/06_flow_control/#while-loop","title":"While loop","text":"<p>The <code>while</code> loop executes a given group of statements as long as the given expression is <code>True</code></p> <p>Syntax: <pre><code>while expression:\n    statements\n</code></pre></p> <p>In the example below, <code>Hello you</code> will be printed 5 times, that is for each time counter is still less than 5.</p> <pre><code>counter = 0\n\nwhile counter &lt; 5:\n    print('Hello you')\n    counter += 1\n</code></pre> <pre><code>Hello you\nHello you\nHello you\nHello you\nHello you\n</code></pre> <pre><code># Convert the weights in the list from kgs to pounds\nweights_kg = [145, 100, 76, 80]\nweights_pds = []\n\ncounter = 0\nend = len(weights_kg)\n\nwhile counter &lt; end:\n\n    pound = weights_kg[counter] * 2.2\n    rounded_pds = round(pound, 3)\n    weights_pds.append(rounded_pds)\n\n    counter += 1\n\nprint(weights_pds)\n</code></pre> <pre><code>[319.0, 220.0, 167.2, 176.0]\n</code></pre>"},{"location":"Python/Basics/06_flow_control/#conditional-statements","title":"Conditional Statements","text":"<p>Conditional statements in Python are fundamental building blocks for controlling the flow of a program based on certain conditions. They enable the execution of specific blocks of code when certain conditions are met. The primary conditional statements in Python include <code>if</code>, <code>elif</code>, and <code>else</code>.</p>"},{"location":"Python/Basics/06_flow_control/#basic-syntax","title":"Basic Syntax","text":""},{"location":"Python/Basics/06_flow_control/#if-statement","title":"If Statement","text":"<p>The <code>if</code> statement is used to test a condition. If the condition evaluates to <code>True</code>, the block of code inside the <code>if</code> statement is executed.</p> <pre><code>if condition:\n    # block of code\n</code></pre> <p>Example: <pre><code>x = 10\nif x &gt; 5:\n    print(\"x is greater than 5\")\n</code></pre></p>"},{"location":"Python/Basics/06_flow_control/#else-statement","title":"Else Statement","text":"<p>The <code>else</code> statement is used to execute a block of code if the condition in the <code>if</code> statement evaluates to <code>False</code>.</p> <pre><code>if condition:\n    # block of code if condition is True\nelse:\n    # block of code if condition is False\n</code></pre> <p>Example: <pre><code>x = 3\nif x &gt; 5:\n    print(\"x is greater than 5\")\nelse:\n    print(\"x is not greater than 5\")\n</code></pre></p>"},{"location":"Python/Basics/06_flow_control/#elif-statement","title":"Elif Statement","text":"<p>The <code>elif</code> (short for else if) statement allows you to check multiple conditions. If the first condition is <code>False</code>, it checks the next <code>elif</code> condition, and so on. If all conditions are <code>False</code>, the <code>else</code> block is executed.</p> <pre><code>if condition1:\n    # block of code if condition1 is True\nelif condition2:\n    # block of code if condition2 is True\nelse:\n    # block of code if none of the above conditions are True\n</code></pre> <p>Example: <pre><code>x = 7\nif x &gt; 10:\n    print(\"x is greater than 10\")\nelif x &gt; 5:\n    print(\"x is greater than 5 but less than or equal to 10\")\nelse:\n    print(\"x is 5 or less\")\n</code></pre></p>"},{"location":"Python/Basics/06_flow_control/#nested-conditional-statements","title":"Nested Conditional Statements","text":"<p>Conditional statements can be nested within each other to handle more complex decision-making processes.</p> <p>Example: <pre><code>x = 15\nif x &gt; 10:\n    if x &gt; 20:\n        print(\"x is greater than 20\")\n    else:\n        print(\"x is greater than 10 but not greater than 20\")\nelse:\n    print(\"x is 10 or less\")\n</code></pre></p>"},{"location":"Python/Basics/06_flow_control/#conditional-expressions-ternary-operator","title":"Conditional Expressions (Ternary Operator)","text":"<p>Python also supports conditional expressions, which allow for a more concise way to write simple <code>if-else</code> statements.</p> <pre><code>variable = value_if_true if condition else value_if_false\n</code></pre> <p>Example: <pre><code>x = 10\nresult = \"greater than 5\" if x &gt; 5 else \"5 or less\"\nprint(result)  # Output: greater than 5\n</code></pre></p>"},{"location":"Python/Basics/06_flow_control/#combining-conditions","title":"Combining Conditions","text":"<p>Multiple conditions can be combined using logical operators (<code>and</code>, <code>or</code>, <code>not</code>).</p> <p>Example: <pre><code>x = 8\nif x &gt; 5 and x &lt; 10:\n    print(\"x is between 5 and 10\")\n</code></pre></p>"},{"location":"Python/Basics/06_flow_control/#practical-usage","title":"Practical Usage","text":"<p>Conditional statements are used in a wide variety of scenarios, such as:</p> <ul> <li> Validating user input.</li> <li> Controlling the flow of loops.</li> <li> Implementing different behaviors in functions or methods.</li> <li> Handling exceptions or special cases in data processing.</li> </ul> <p>Understanding and effectively using conditional statements are crucial for writing efficient and readable code in Python. They enable developers to build programs that can make decisions and respond dynamically to different inputs and situations.</p>"},{"location":"Python/Basics/06_flow_control/#exercise","title":"Exercise","text":"<p>Write a program which accepts a number (an amount of money to be paid by a customer in rupees) entered from the keyboard. If the amount is greater than or equal to 1000 rupees, a 5% discount is given to the customer. Then display the final amount that the customer has to pay.</p> What's on your mind? Put it in the comments!"},{"location":"Python/Basics/07_advanced/","title":"Python Advanced","text":"<p>In this notebook, we will explore some fundamental concepts in Python that are essential for writing clean, efficient, and maintainable code. These concepts include:</p> <ul> <li> Classes and Objects</li> <li> Formatted Strings</li> <li> Handling Errors</li> <li> Variable Scopes</li> </ul>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Python/Basics/07_advanced/#classes-and-objects","title":"Classes and Objects","text":"<p>In Python, everything is an object. A class helps us create objects.</p>"},{"location":"Python/Basics/07_advanced/#creating-a-class","title":"Creating a Class","text":"<p>Use the <code>class</code> keyword to create a class</p> <p>Here is the syntax: <pre><code>class className:\n    statement(s)\n</code></pre></p> <p>Below is an example:</p> <pre><code>class Person:\n    first_name = \"Betty\"\n    last_name = \"Kawala\"\n    age = 30\n</code></pre>"},{"location":"Python/Basics/07_advanced/#instantiating-a-class","title":"Instantiating a class","text":"<p>Now we can ceate an object from the class by instantiating it.</p> <p>To instantiate a class, add round brackets to the class name.</p> <pre><code>person_obj1 = Person()\n\ntype(person_obj1)\n</code></pre> <pre><code>__main__.Person\n</code></pre> <p>After instantiating a class, we can now access the object's properties.</p> <pre><code># print attributes\nprint(person_obj1.first_name)\nprint(person_obj1.last_name)\nprint(person_obj1.age)\n</code></pre> <pre><code>Betty\nKawala\n30\n</code></pre>"},{"location":"Python/Basics/07_advanced/#class-attributes","title":"Class Attributes","text":"<p>A class can have attributes. Forexample the Person Class can have attributes like the <code>name</code>, <code>height</code> and <code>feet</code></p> <pre><code>class Person:\n    def __init__(self, name, height, feet):\n        self.name = name\n        self.height = height\n        self.feet = feet\n</code></pre>  Note!<p>For now, focus on the syntax. Later we will explain the <code>__init__()</code> function and the <code>self</code> parameter.</p> <p>Now that our class is ready, we can now instantiate it and provide values to it's attributes.</p> <p>This process can also be called \"creating an instance of a class\".</p> <p>An instance is simply the object created from a class</p> <p>In this example, <code>person_obj1</code> is a unique instance of the person class. </p> <pre><code># create a class instance\nperson_obj = Person(\n    name='Betty Kawala', \n    height=1.57, \n    feet=4\n    )\n</code></pre> <p>After that, we can now access the properties of the instance (object)</p> <pre><code># accessing the properties\nprint('Name:', person_obj.name)\nprint('Height:', person_obj.height)\nprint('Feet:', person_obj.feet)\n</code></pre> <pre><code>Name: Betty Kawala\nHeight: 1.57\nFeet: 4\n</code></pre> <p>The <code>self</code> parameter allows us to access the attributes and methods of a class</p> <p>The <code>__init__()</code> function allows us to provide values for the attributes of a class</p>"},{"location":"Python/Basics/07_advanced/#instances-are-unique","title":"Instances are unique","text":"<p>Let's say you have 500 people and you need to manage their data.</p> <p>It is inefficient to create a variable for each of them, instead, you can create unique instances of a class.</p> <p>In this example, the student1 and student2 instances are different from each other</p> <pre><code>class Student:\n  def __init__(self, id_number, name, age):\n    self.id_number = id_number\n    self.name = name\n    self.age = age\n\nstudent1 = Student(5243, \"Mary Doe\", 18)\nstudent2 = Student(3221, \"John Doe\", 18)\n\nprint(\"Student 1 ID:\", student1.id_number)\nprint(\"Student 1 Name:\", student1.name)\nprint(\"Student 1 Age:\", student1.age)\n\nprint(\"---------------------\")\n\nprint(\"Student 2 ID:\", student2.id_number)\nprint(\"Student 2 Name:\", student2.name)\nprint(\"Student 2 Age:\", student2.age)\n</code></pre> <pre><code>Student 1 ID: 5243\nStudent 1 Name: Mary Doe\nStudent 1 Age: 18\n---------------------\nStudent 2 ID: 3221\nStudent 2 Name: John Doe\nStudent 2 Age: 18\n</code></pre>"},{"location":"Python/Basics/07_advanced/#methods","title":"Methods","text":"<p>Methods are functions that can access the class attributes. </p> <p>These methods should be defined (created) inside the class</p> <pre><code>class Person:\n    def __init__(self, name, height, feet):\n        self.name = name\n        self.height = height\n        self.feet = feet\n\n    def jump(self):\n        return \"I'm jumping \" + str(self.feet) + \" Feet\"\n</code></pre> <pre><code>person_obj1 = Person(name='Juma', height=1.59, feet=5)\n\nprint(person_obj1.jump())\n</code></pre> <pre><code>I'm jumping 5 Feet\n</code></pre> <p>As you may notice, we used the <code>self</code> parameter to access the <code>feet</code> attribute</p> <p>You can also pass an argument to a method.</p> <pre><code>class Student:\n    def __init__(self, id_number, name, age):\n        self.id_number = id_number\n        self.name = name\n        self.age = age\n\n    def greet_student(self, greetings):\n        print(\"Hello\" + self.name + \", \" + greetings)\n\nstudent1 = Student(43221, \"Agaba Calvin\", 18)\n\n# the string below will be passed as \n# the value of the greetings parameter\nstudent1.greet_student(\"Welcome to this Python Tutorial!\")\n</code></pre> <pre><code>HelloAgaba Calvin, Welcome to this Python Tutorial!\n</code></pre>"},{"location":"Python/Basics/07_advanced/#python-inheritance","title":"Python Inheritance","text":"<p>Inheritance is a feature that allows us to create a class that inherits the attributes or properties and methods of another class</p>"},{"location":"Python/Basics/07_advanced/#example","title":"Example","text":"<p>The <code>Animal</code> class below can be used to tell that an animal can eat</p> <pre><code>class Animal:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def eat(self):\n        print(f\"{self.name} is eating.\")\n</code></pre> <p>Let's say we need to create another class called Dog.</p> <p>Since a dog is also an animal, it's more efficient to have access to all the properties and methods of the <code>Animal</code> class than to create another</p> <p>This example creates a class named <code>Dog</code> and inherits from the <code>Animal</code> class</p> <pre><code>class Dog(Animal):\n    def __init__(self, name, age, color):\n        super().__init__(name, age)\n        self.color = color\n\n    def sound(self):\n        print(self.name, \"barks\")\n</code></pre>  Note!<p>As you may notice, to inherit from a parent, we simply pass the name of that class as a parameter of the child class.</p> <p>Now we can use the properties and methods of both the <code>Animal</code> and the <code>Dog</code> classes using just one instance</p> <pre><code>dog1 = Dog(name='Brian', age=8, color='White')\ndog1.eat()\ndog1.sound()\n</code></pre> <pre><code>Brian is eating.\nBrian barks\n</code></pre> <p>The <code>super()</code> and <code>__init__</code> functions found in the <code>Dog</code> class allow us to inherit the properties and methods of the <code>Animal</code> class.</p>"},{"location":"Python/Basics/07_advanced/#parent-and-child-class","title":"Parent and Child Class","text":"<p>The parent class is the class from whick the other class inherits from.</p> <p>The child class is the the class that inherits from another class</p> <p>In our example above, the <code>Animal</code> is the parent class while the <code>Dog</code> class is the child class</p>"},{"location":"Python/Basics/07_advanced/#formatted-strings","title":"Formatted Strings","text":"<p>In Python, we can format a string by adding substring/s within it.</p> <p>The <code>format()</code> function allows us to format strings.</p>"},{"location":"Python/Basics/07_advanced/#placeholders","title":"Placeholders <code>{}</code>","text":"<p>Placeholders help us control which part of the string should be formated.</p> <p>They are defined using  curly braces <code>{}</code>.</p> <p>In this example, we will concatenate (add) a substring to where the curly braces are placed</p> <pre><code>text = \"I love {} very much!\"\nformatted_text = text.format(\"Python\")\n\nprint(formatted_text)\n</code></pre> <pre><code>I love Python very much!\n</code></pre>"},{"location":"Python/Basics/07_advanced/#multiple-placeholders","title":"Multiple placeholders","text":"<p>If you want to format multiple parts of a string, use multiple placeholders.</p> <pre><code>text = '{} loves to code in {}'\nformatted_text = text.format('Juma', 'JavaScript')\n\nprint(formatted_text)\n</code></pre> <pre><code>Juma loves to code in JavaScript\n</code></pre>"},{"location":"Python/Basics/07_advanced/#using-indexes","title":"Using Indexes","text":"<p>We can use index numbers to specify exactly where the values should be placed. </p> <p>The index numbers should be inside the curly braces: <code>{index_numbers}</code></p> <pre><code>text = 'I love {2}, {1} and {0} very much!'\nformatted_text = text.format('Python', 'JavaScript', 'HTML')\n\nprint(formatted_text)\n</code></pre> <pre><code>I love HTML, JavaScript and Python very much!\n</code></pre>  Note!<p>0 represents the first value, 1 represents the second value and so on.</p>"},{"location":"Python/Basics/07_advanced/#using-named-indexes","title":"Using Named Indexes","text":"<p>We can also use named indexes to specify exactly where the values should be placed.</p> <p>The arguments of the <code>format()</code> function should be in <code>key/value</code> pairs ie <code>key=value</code>.</p> <p>The <code>key/value</code> pairs should be separated by commas.</p> <pre><code>text = 'The color of the {fruit} is {color}.'\nformatted_text = text.format(fruit='banana', color='yellow')\n\nprint(formatted_text)\n</code></pre> <pre><code>The color of the banana is yellow.\n</code></pre>"},{"location":"Python/Basics/07_advanced/#literal-string-interpolation","title":"Literal String Interpolation","text":"<p>Literal string interpolation allows you to use expression inside your strings. </p> <p>Simply add <code>f</code> before you opening quote, then surround your expressions with curly braces <code>{}</code>.</p> <pre><code>name = 'Juma'; \nlanguage = 'JavaScript'\n\nstatement = f'{name} loves to code in {language}'\n\nprint(statement)\n</code></pre> <pre><code>Juma loves to code in JavaScript\n</code></pre> <p>Here's another example</p> <pre><code>number1 = 5\nnumber2 = 7\nanswer = f'The summation of 5 and 7 is {number1 + number2}'\n\nprint(answer)\n</code></pre> <pre><code>The summation of 5 and 7 is 12\n</code></pre>"},{"location":"Python/Basics/07_advanced/#errors-in-python","title":"Errors in Python","text":"<p>When coding in Python, you will encounter errors.</p> <p>When errors occur, the program crashes or stops executing.</p> <p>Fortunately, errors can be handled in Python</p>"},{"location":"Python/Basics/07_advanced/#the-tryexcept-statment","title":"The <code>try...except</code> statment","text":"<p>The <code>try...except</code> statement is used to handle exceptions(errors)</p> <p>The <code>try</code> statement takes a block of code to test for errors</p> <p>The <code>except</code> statement handles the exceptions.</p> <pre><code>try:\n    # age = input('Enter your age: ')\n    age = '32'\n\n    if age &gt;= 18:\n        print('Your vote has been cast')\n    else:\n        print('You are not eligible to vote')\nexcept:\n    print('A problem occured while picking your age \\n'\n          'You did not enter a number')\n\ntext = \"\\nHello world!\"\nprint(text)\n</code></pre> <pre><code>A problem occured while picking your age \nYou did not enter a number\n\nHello world!\n</code></pre>  Note!<p>Even when the exception was thrown, the codes after the try...except were still executed</p>"},{"location":"Python/Basics/07_advanced/#the-else-statement","title":"The <code>else</code> statement","text":"<p>The else statement is executed if there are no exceptions thrown.</p> <pre><code>try:\n    text = \"Hello World!\"\n    print(text)\nexcept:\n    print(\"An error occurred.\")\nelse:\n    print(\"No exception was thrown!\")\n</code></pre> <pre><code>Hello World!\nNo exception was thrown!\n</code></pre>"},{"location":"Python/Basics/07_advanced/#the-finally-statement","title":"The <code>finally</code> statement","text":"<p>The <code>finally</code> statement is executed whether or not an exception is thrown.</p> <pre><code>try: \n    text = \"hello world\"\n    print(text)\nexcept:\n    print(\"An error occured.\")\nfinally:\n    print(\"Hello, I am still printed.\")\n\nprint(\"----------------------\")\n\n# an exception will be thrown here\ntry: \n    print(undefined_variable)\nexcept:\n    print(\"An error occured.\")\nfinally:\n    print(\"Hello, I am still printed.\")\n</code></pre> <pre><code>hello world\nHello, I am still printed.\n----------------------\nAn error occured.\nHello, I am still printed.\n</code></pre>"},{"location":"Python/Basics/07_advanced/#throw-exceptions","title":"Throw Exceptions","text":"<p>We can intentionally throw and exception to stop the execution of a program.</p> <p>The <code>raise</code> keyword throws an excrption.</p> <pre><code># Creating your own errors\ntry: \n    age = 14\n    if age &lt; 18:\n        raise Exception('Not an adult')\nexcept Exception as error:\n    print('A problem occurred \\n'\n          f'Error: {error}')\n</code></pre> <pre><code>A problem occurred \nError: Not an adult\n</code></pre>"},{"location":"Python/Basics/07_advanced/#kinds-of-exceptions","title":"Kinds of Exceptions","text":"<p>In Python, there are different kinds of exceptions and we can handle them individually with the <code>try...except</code> statement.</p> <pre><code>try:\n    # statements\nexcept ExceptionKind:\n    #statments\n</code></pre> <p>One of the most common kind of exceptions is the <code>NameError</code>. This is thrown when you use a variable that is not defined</p> <pre><code>try:\n    print(rand_var)\nexcept NameError:\n    print('You used a variable that is not defined!')\n</code></pre> <pre><code>You used a variable that is not defined!\n</code></pre>"},{"location":"Python/Basics/07_advanced/#variable-scope","title":"Variable Scope","text":""},{"location":"Python/Basics/07_advanced/#python-variable-scopes","title":"Python Variable Scopes","text":"<p>The accessibility of variable depends on its scope. In Python, there are two variable scopes:</p> <ul> <li> Global Scope</li> <li> Local Scope</li> </ul>"},{"location":"Python/Basics/07_advanced/#global-scope","title":"Global Scope","text":"<p>A variable that is defined (created) outside a function has a global scope</p> <p>A global variable can be accessed anywhere in a program</p> <pre><code>name = 'Viola'\n# name can be accessed here\nprint(name)\n\ndef greet():\n    # name can be accessed here\n    print('Hello ' + name)\n\ngreet()\n</code></pre> <pre><code>Viola\nHello Viola\n</code></pre>"},{"location":"Python/Basics/07_advanced/#local-scope","title":"Local Scope","text":"<p>A variable that is defined (created) inside a function has a local scope. A local scope variable can only be accessed and used inside the function.</p> <pre><code>def greet():\n    local_name = 'Viola'\n    print('Hello ' + local_name)\n\ngreet()\n\ntry:\n    # local_name cannot be accessed here\n    print(local_name)\nexcept Exception as e:\n    print(e)\n</code></pre> <pre><code>Hello Viola\nname 'local_name' is not defined\n</code></pre>"},{"location":"Python/Basics/07_advanced/#the-global-keyword","title":"The <code>global</code> Keyword","text":"<p>We can force a local variable to be a global variable by using the <code>global</code> keyword.</p> <pre><code># Global Keyword\n\ndef add():\n    global summ\n    number1 = 5\n    number2 = 7\n    summ = number1 + number2\n    return summ\n\nadd()\n\n# summ is accessible even outside the function\nprint(summ)\n</code></pre> <pre><code>12\n</code></pre>"},{"location":"Python/Basics/07_advanced/#exercise","title":"ExerciseWhat's on your mind? Put it in the comments!","text":"<p>Develop a simple calculator to accept two floating point numbers from the keyboard. Then display a menu to the user and let him/her select a mathematical operation to be performed on those two numbers. Then display the answer. A sample run of you program should be similar to the following:</p> <pre><code>Enter number 1: 20\nEnter number 2: 12\nMathematical Operation\n-----------------------------------\n1 - Add\n2 - Subtract\n3 - Multiply\n4 - Divide\n-----------------------------------\nEnter your preference: 2\nAnswer : 8.00\n</code></pre>"},{"location":"Python/Basics/08_modules/","title":"Python Modules","text":""},{"location":"Python/Basics/08_modules/#modules","title":"Modules","text":"<p>A module in Python is a python file that can contain variables, functions and classes</p>"},{"location":"Python/Basics/08_modules/#why-use-modules","title":"Why use Modules?","text":"<p>Modules allow us to split our code into multiple files</p> <p>Instead of writing all our codes inside a sigle Python file, we can use modules</p> Note!<p>That way, our code will be easier to read, understand and maintain</p>"},{"location":"Python/Basics/08_modules/#creating-a-module","title":"Creating a Module","text":"<p>There is nothing so special with creating a module, simply write you Python code and save it with the <code>.py</code> extension.</p> <p>In this example, we have a module saved as my_module.py and it contains the following code</p> <pre><code># this is my_module.py\n\nfirst_name = 'Viola'\nlast_name = 'Akullu'\n\ndef add(number1, number2):\n    return number1 + number2\n\ndef multiply(number1, number2):\n    return number1 * number2\n</code></pre> <p>After that, to use <code>my_module.py</code>, we need to import it.</p> <p>To import, use the <code>import</code> statement and the module name.</p> <p>Then we can use the variables and functions in the module.</p> <p>In this example, the code below is saved as <code>main_code.py</code> and it imports the <code>module.py</code>.</p> <pre><code># this is main_code.py\n\nimport my_module\n\nfull_name = my_module.first_name + my_module.last_name\nprint('Full name:', full_name)\n\nsummation = my_module.add(3, 7)\nprint('Summation:', summation)\n</code></pre> <pre><code>Full name: ViolaAkullu\nSummation: 10\n</code></pre>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Python/Basics/08_modules/#using-aliases","title":"Using Aliases","text":"<p>We can use an alias to refer to the module</p> <p>To use an alias, use the <code>as</code> keyword</p> <pre><code># this is main_code.py\n\nimport my_module as mm\n\nfull_name = mm.first_name + mm.last_name\nprint('Full name:', full_name)\n\nsummation = mm.add(3, 7)\nprint('Summation:', summation)\n</code></pre> <pre><code>Full name: ViolaAkullu\nSummation: 10\n</code></pre>"},{"location":"Python/Basics/08_modules/#importing-parts-of-a-module","title":"Importing Parts of a Module","text":"<p>We can choose to import only some specific parts of a module</p> <p>Note! When we import a part of a module, we will be able to use its variables and functions directly</p> <p>Use the <code>from</code> keyword to import a part of a module.</p> <p>In this example, we will import the <code>first_name</code> variable and access it directly</p> <pre><code>from my_module import first_name\n\n# now we can use it directly as \nprint(first_name)\n</code></pre> <pre><code>Viola\n</code></pre>"},{"location":"Python/Basics/08_modules/#the-dir-function","title":"The dir() Function","text":"<p>The <code>dir()</code> function returns a list of all the variables, functions and classes available in a module</p> <pre><code>import my_module\n\ndir_ = dir(my_module)\n\nprint(dir_)\n</code></pre> <pre><code>['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'add', 'first_name', 'last_name', 'multiply']\n</code></pre>"},{"location":"Python/Basics/08_modules/#built-in-modules","title":"Built in Modules","text":"<p>Python has many useful built-in modules that we can use to make coding easier.</p> <p>Built-in modules can be imported without having to create them</p> <p>In this example, we will import the <code>sysconfig</code> module and use its <code>get_python_version()</code> to return the Python version we're using</p> <pre><code>import sysconfig\n\npython_version = sysconfig.get_python_version()\nprint(python_version)\n</code></pre> <pre><code>3.10\n</code></pre>"},{"location":"Python/Basics/08_modules/#math-module","title":"Math Module","text":"<p>The math module gives us access to mathematical functions</p> <p>To use the math module, import it first, then we can start using it.</p> <p>We can use the math module to find the square root of a number using the <code>math.sqrt()</code> method</p> <pre><code>import math\n\nnumber = 16\nnumber_sqrt = math.sqrt(number)\n\nprint('Number:', number)\nprint('Square root of number:', number_sqrt)\n</code></pre> <pre><code>Number: 16\nSquare root of number: 4.0\n</code></pre> <p>We can use the math module to get the factorial of a number by using the <code>math.factorial()</code> method</p> <pre><code>import math\n\nnumber = 5\nnumber_factorial = math.factorial(number)\n\nprint('Number:', number)\nprint('Factorial:', number_factorial)\n</code></pre> <pre><code>Number: 5\nFactorial: 120\n</code></pre> <p>The math module also contains some constants like <code>pi</code> and <code>e</code></p> <pre><code>import math\n\nprint('e:', math.e)\nprint('pi:', math.pi)\n</code></pre> <pre><code>e: 2.718281828459045\npi: 3.141592653589793\n</code></pre> <p>The math module can do those and so much more</p>"},{"location":"Python/Basics/08_modules/#random-module","title":"Random Module","text":"<p>The <code>random</code> module lets us generate a random number</p> <p>As usual, to use the <code>random</code> module, import it first.</p> <p>We can generate a random number that falls within a specified range by using the <code>random.randint()</code> method</p> <pre><code>import random\n\nrandom_integer = random.randint(1,100)\nprint('Random Integer:', random_integer)\n</code></pre> <pre><code>Random Integer: 4\n</code></pre> <p>We can generate numbers from a gaussian distribution with mean (<code>mu</code>) as 0 and standard deviation (<code>sigma</code>) as 1</p> <pre><code>numbers = []\n\ncounter = 0\nwhile counter &lt; 100:\n    numbers.append(random.gauss(mu=0, sigma=1))\n    counter += 1\n\nprint(numbers)\n</code></pre> <pre><code>[0.8310128735410047, 2.402375340413018, -1.2769617295659348, 0.7569506717477539, 1.6026026122392498, 1.4142936594217554, -0.3169917649104485, -0.07305941097531603, -0.7885301448554015, -0.0674611332298377, 0.28288857512573684, 0.08844216926370602, -1.249987094506388, 0.870793290313952, -0.6607737394803138, 0.3780605189691181, 0.20288623881856632, 0.8439702923769746, 1.6500270929422152, -0.5579247768953991, -0.3076290349937902, 0.8927675985413197, -2.3716599434459114, 0.23253728473684382, 0.01698634011714592, -1.506684284668113, -1.516156046117149, -0.7549199652372819, 0.4855840249497611, -1.9426218553454226, -0.5672748318805165, 1.7849639815888045, -0.4223703532919884, -1.4182523392919628, 0.3817982448773813, -1.2151583559744263, 0.21736913499460964, 0.0743448686041854, -0.6217874541247053, -0.05369712902089164, 0.06560332100098984, 0.5791279113149166, 1.5329264216964942, -1.5523813284095307, 0.256018716284597, 1.498941708596562, 0.6484203278916434, 0.956658998431066, -0.7469607705965761, 0.9093585267915438, -0.3301676177291813, -2.1020486475752564, -0.6324768823835674, -0.2621489739923403, 0.36805271395009337, -0.1987104858441708, -0.20226660046300027, -1.0227302328088852, 0.9440428943259802, 1.3499647213634605, 0.28655811659281705, -0.48212404896946465, 1.5732404576352244, 1.7024230857294205, -0.32802550098029193, 2.0808443667109597, 2.2783854541239874, -0.265626754707208, -0.04641950638081212, 0.7941371582079103, -0.36860553191079254, -0.9098450679735101, 1.234946260813307, -2.835066105841072, 1.3883254119625694, 1.2853299658795028, 1.178005875662903, 0.3186472037221876, -1.0006920744966419, -2.3745959188263885, 1.8440465299894964, -0.35610549619690796, 0.5857012223823791, 0.7400382246661824, 0.07225122970263118, -0.5508995490344698, -0.038356750477046286, -0.040997463659922434, 0.6802546773316889, -1.3861271290488735, 0.7275261286416534, 0.3729374034245036, -0.013616473457934613, -0.7620103036607296, 0.15556952852877587, -1.7898533901375224, -1.137248630020012, -1.71518120153122, -0.5817297506694047, -0.4035542913039588]\n</code></pre>"},{"location":"Python/Basics/08_modules/#date-and-time","title":"Date and Time","text":"<p>The <code>datetime</code> module allows us to work with dates</p> <p>As usual we have to import the <code>datetime</code> module to be able to use it.</p>"},{"location":"Python/Basics/08_modules/#current-date-and-time","title":"Current Date and Time","text":"<p>The <code>datetime.datetime.now()</code> method returns the current date and time</p> <pre><code>import datetime\n\ntime_now = datetime.datetime.now()\nprint(time_now)\n</code></pre> <pre><code>2024-05-01 08:18:09.070054\n</code></pre>"},{"location":"Python/Basics/08_modules/#the-date-object","title":"The <code>date</code> Object","text":"<p>The <code>date</code> object represents a date (year, month and day)</p> <p>To create a <code>date</code> object, import it from the <code>datetime</code> module first.</p> <pre><code>from datetime import date\n\ntoday = date.today()\nprint('Current date:', today)\n</code></pre> <pre><code>Current date: 2024-05-01\n</code></pre>"},{"location":"Python/Basics/08_modules/#json","title":"JSON","text":"<p>JSON stands for JavaScript Object Notation.</p> <p>JSON contains data that are sent or received to and from a server</p> <p>JSON is simply a string, if follows a format similar to a Python dictionary</p> <p>Example:</p> <pre><code>data = \"{'first_name': 'Juma','last_name': 'Shafara', 'age': 39}\"\n\nprint(data)\n</code></pre> <pre><code>{'first_name': 'Juma','last_name': 'Shafara', 'age': 39}\n</code></pre>"},{"location":"Python/Basics/08_modules/#json-to-dictionary","title":"JSON to Dictionary","text":"<p>Before we can individually access the data of a JSON, we need to convert it to a Python dictionary first.</p> <p>To do that, we need to import the <code>json</code> module</p> <pre><code>import json \n\ndata = '{\"first_name\": \"Juma\",\"last_name\": \"Shafara\", \"age\": 39}'\n\n# convert to dictionary\ndata_dict = json.loads(data)\n\nprint('Fist name:',data_dict['first_name'])\nprint('Last name:', data_dict['last_name'])\nprint('Age:', data_dict['age'])\n</code></pre> <pre><code>Fist name: Juma\nLast name: Shafara\nAge: 39\n</code></pre>"},{"location":"Python/Basics/08_modules/#dictionary-to-json","title":"Dictionary to JSON","text":"<p>To convert a dictionay to JSON, use the <code>json.dumps()</code> method.</p> <pre><code>import json\n\ndata_dict = {\n    \"first_name\": \"Juma\",\n    \"last_name\": \"Shafara\", \n    \"age\": 39\n    }\n\ndata_json = json.dumps(data_dict)\n</code></pre>"},{"location":"Python/Basics/08_modules/#exercise","title":"ExerciseWhat's on your mind? Put it in the comments!","text":"<p>Write a program to calculate the factorial of any given positive integer.</p>"},{"location":"Python/Basics/09_file_handling/","title":"Python File Handling","text":"<p>Python allows us to read, write, create and delete files. This process is called file handling. This is what we will discuss in this lesson</p>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Python/Basics/09_file_handling/#the-open-function","title":"The <code>open()</code> function","text":"<p>The <code>open()</code> function allows us to read, create and update files</p> <p>It takes 2 parameters:</p> <ul> <li><code>file</code> - the file or file path to be opened</li> <li><code>mode</code> - the mode in which a file is opened for</li> </ul> <p>The mode is a string that can either be any of the following:</p> Mode Meaning <code>'r'</code> Open a file for reading <code>'w'</code> Open a file for writing, creates the file if it does not exist <code>'a'</code> Open a file for writing, appends to the end of the file <code>'x'</code> Open a file for creating, fails if file already exists"},{"location":"Python/Basics/09_file_handling/#python-file-reading","title":"Python File reading","text":"<p>To better explain this, let us say we have a folder named <code>my_folder</code>.</p> <p>Inside <code>my_folder</code> we have the following files:</p> <ul> <li><code>demo.txt</code></li> <li><code>main_code.py</code></li> </ul> <p>The content of the <code>demo.txt</code> file is the following</p> <p><pre><code>Hello World!\nI love Python\n</code></pre> Now our goal is to read the content of the <code>demo.txt</code> file and then print it using the <code>main_code.py</code> file</p> <p>To achieve this, we will use the <code>open()</code> function with <code>'r'</code> mode.</p> <pre><code># this is main code\n\nfile = open(\n    file='demo.txt',\n    mode='r'\n)\ncontent = file.read()\nprint(content)\n</code></pre> <pre><code>Hello World!\nI love Python\n</code></pre>"},{"location":"Python/Basics/09_file_handling/#reading-lines","title":"Reading Lines","text":"<p>We can also read each line using the <code>readline()</code> method.</p> <pre><code># this is main_code.py\n\nfile = open(\n    file='demo.txt',\n    mode='r'\n)\n\nfirst_line = file.readline()\nsecond_line = file.readline()\n\nprint('First line:', first_line)\nprint('Second line:', second_line)\n</code></pre> <pre><code>First line: Hello World!\n\nSecond line: I love Python\n</code></pre>"},{"location":"Python/Basics/09_file_handling/#writing-a-file","title":"Writing a File","text":"<p>In simplest terms, writing a file means modifying the content of a file or creating it if it doesnot exist yet.</p> <p>In Python, there are 2 modes to write to file.</p> <ul> <li><code>'w'</code> - overwrites content of a file, creates file if it does not exist</li> <li><code>'a'</code> - appends content to the end of a file, creates the file if it does not exist</li> </ul> <p>Example To better explain this, lets say we have a folder named <code>my_folder</code>. Inside <code>my_folder</code> we have the following files</p> <ul> <li><code>demo.txt</code></li> <li><code>main_code.py</code></li> </ul> <p>The content of the <code>demo.txt</code> file is the following</p> <p><pre><code>I love Python\n</code></pre> In this example, we will use the <code>'w'</code> mode which will overwrite(replace) the content of the file</p> <pre><code># this is main_code.py\n\nfile = open(\n    file='demo.txt',\n    mode='w'\n)\nfile.write('I love JavaScript')\nfile.close()\n</code></pre> <p>When the above code is run, the content of the file <code>demo.txt</code> will be this: <pre><code>I love JavaScript\n</code></pre></p> <p>Another example, this time we will use the <code>a</code> mode which will append or add content to the end of the file</p> <pre><code># this is main_code.py\n\nfile = open(\n    file='demo.txt',\n    mode='a'\n)\nfile.write(' and JavaScript')\nfile.close()\n</code></pre> <p>When the above script is run, the content of the <code>demo.txt</code> file will be this: <pre><code>I love Python and JavaScript\n</code></pre></p>"},{"location":"Python/Basics/09_file_handling/#deleting-a-file","title":"Deleting a file","text":"<p>To delete a file, use the <code>os</code> module. The <code>os</code> modules contains the <code>remove()</code> method which we can use to delete files.</p> <pre><code># this is main_code.py\n\n# import os\n\n# os.remove('demo.txt')\n</code></pre>"},{"location":"Python/Basics/09_file_handling/#exercise","title":"ExerciseWhat's on your mind? Put it in the comments!","text":"<ol> <li>Develop a simple telephone directory which saves your friends contact information in a file named directory.txt. The program should have a menu similar to the following: <pre><code>----------------Menu-------------------------\n1. Add new friend.\n2. Display contact info.\n3. Exit\n------------------------------------------------\nEnter menu number:\n</code></pre> When you press \u201c1\u201d it should request you to enter following data: <pre><code>---------New friend info--------\nName : Saman\nPhone-No: 011-2123456\ne-Mail : saman@cse.mrt.ac.lk\n</code></pre> After adding new contact information it should again display the menu. When you press \u201c2\u201d it should display all the contact information stored in the directory.txt file as follows:</li> </ol> <pre><code>--------------Contact info---------------\nName            Tel-No          e-Mail\nKamala          077-7123123     kamala@yahoo.com\nKalani          033-4100101     kalani@gmail.com\nSaman           011-2123456     saman@cse.mrt.ac.lk\n-----------------------------------------\n</code></pre>"},{"location":"Python/Basics/13_end_of_course_exercise/","title":"End of Course Exam","text":""},{"location":"Python/Basics/13_end_of_course_exercise/#objectives","title":"Objectives","text":"<ul> <li>Write and run Python code</li> <li>Practice problem solving using Python code.</li> <li>Learn how to detect programming errors through program testing.</li> <li>Correct/fix syntax errors and bugs.</li> </ul>"},{"location":"Python/Basics/13_end_of_course_exercise/#prerequisites","title":"Prerequisites","text":"<ul> <li>Students are expected to be familiar with a text editor.</li> <li>A basic understanding of running and linking in concept.</li> <li>Knowledge in area such as Python language syntax, input output function is recommended.</li> </ul>"},{"location":"Python/Basics/13_end_of_course_exercise/#questions","title":"Questions","text":"<ol> <li>Write a Python program to display the following text on the screen.</li> </ol> <pre><code>DATAIDEA\nSir Apollo Kagwa Road,\nKampala,\nUganda\n-------------------\nwww.dataidea.org\n</code></pre> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to assign the number 34.5678 to a variable named \u201cnumber\u201d then display the number rounded to the nearest integer value and next the number rounded to two decimal places.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to identify whether a number input from the keyboard is even or odd. If it is even, the program should display the message \u201cNumber is even\u201d, else it should display \u201cNumber is odd\u201d.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to display the student\u2019s grade based on the following table:</li> </ol> Marks Grade &gt;= 75 A &gt; 50 and &lt;=75 B &gt; 25 and &lt;=50 C &lt;= 25 D <pre><code># your solution\n</code></pre> <ol> <li>Write a program to convert a given character from uppercase to lowercase and vice versa.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program which accepts a number (an amount of money to be paid by a customer in rupees) entered from the keyboard. If the amount is greater than or equal to 1000 rupees, a 5% discount is given to the customer. Then display the final amount that the customer has to pay.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>A car increases it velocity from u ms-1 to v ms-1 within t seconds. Write a program to calculate the acceleration.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to input a temperature reading in either Celsius(c) or Fahrenheit(f) scale and convert it to the other scale. The temperature reading consists of a decimal number followed by letter \u201dF\u201d or \u201df\u201d if the temperature is in Fahrenheit scale or letter \u201dC\u201d or \u201dc\u201d if it is in Celsius scale. You may use a similar format for the output of the program. Note: <code>c = 5( f \u2212 32) / 9</code></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Develop a simple calculator to accept two floating point numbers from the keyboard. Then display a menu to the user and let him/her select a mathematical operation to be performed on those two numbers. Then display the answer. A sample run of you program should be similar to the following:</li> </ol> <pre><code>Enter number 1: 20\nEnter number 2: 12\nMathematical Operation\n-----------------------------------\n1 - Add\n2 - Subtract\n3 - Multiply\n4 - Divide\n-----------------------------------\nEnter your preference: 2\nAnswer : 8.00\n</code></pre> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to display all the integers from 100 to 200.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to calculate the sum of all the even numbers up to 100.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to compute the sum of all integers form 1 to 100.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to calculate the factorial of any given positive integer.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to compute the sum of all integers between any given two numbers. In this program both inputs should be given from the keyboard.</li> </ol> <pre><code># your solution\n</code></pre>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <ol> <li>Modify the above so that it works only for positive integers.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to display the following symbol pattern: <pre><code>*\n**\n***\n****\n*****\n******\n</code></pre></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to display the message \u201cHello World!\u201d 10000 times. The program should allow users to terminate the program at any time by pressing any key before it displays all the 10000 messages.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to display a sine table. The program should display all the sine values from 0 to 360 degrees (at 5 degrees increments) and it should display only 20 rows at a time</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to store marks of 5 students for 5 subjects given through the keyboard. Calculate the average of each students marks and the average of marks taken by all the students</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to calculate the circumference and area of a circle given its radius. Implement calculation of circumference and areas as separate functions.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to read the file \u201cmy file.txt\u201d which has the message: <pre><code>Hello World!\nThis is my first file\n</code></pre></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a Python program to store the message \u201cIntroduction Python Programming\u201d in a file named \u201cmessage.txt\u201d.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to display the following menu on the screen and let the user select a menu item. Based on the user\u2019s selection display the category of software that the user selected program belongs to. <pre><code>Menu\n-----------------------------------\n1 \u2013 Microsoft Word\n2 \u2013 Yahoo messenger\n3 \u2013 AutoCAD\n4 \u2013 Java Games\n-----------------------------------\nEnter number of your preference:\n</code></pre></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Develop a simple telephone directory which saves your friends contact information in a file named directory.txt. The program should have a menu similar to the following: <pre><code>----------------Menu-------------------------\n1. Add new friend.\n2. Display contact info.\n3. Exit\n------------------------------------------------\nEnter menu number:\n</code></pre> When you press \u201c1\u201d it should request you to enter following data: <pre><code>---------New friend info--------\nName : Saman\nPhone-No: 011-2123456\ne-Mail : saman@cse.mrt.ac.lk\n</code></pre> After adding new contact information it should again display the menu. When you press \u201c2\u201d it should display all the contact information stored in the directory.txt file as follows:</li> </ol> <pre><code>--------------Contact info---------------\nName            Tel-No          e-Mail\nKamala          077-7123123     kamala@yahoo.com\nKalani          033-4100101     kalani@gmail.com\nSaman           011-2123456     saman@cse.mrt.ac.lk\n-----------------------------------------\n</code></pre> <pre><code># your solution\n</code></pre> <ol> <li>Given a date as a triplet of numbers (y, m, d), with y indicating the year, m the month (m = 1 for January, m = 2 for February, etc.), and d the day of the month, the corresponding day of the week f (f = 0 for Sunday, f = 1 for Monday, etc.) can be found as follows: <pre><code>(a) if m &lt; 3\n(b) let m = m + 12 and let y = y - 1\n(c) let a = 2m + 6 (m + 1) / 10\n(d) let b = y + y/4 \u2013 y/100 + y/400\n(e) let f 1 = d + a + b + 1\n(t) let f = f 1 mod 7\n(g) stop.\n</code></pre> Write a program that will read a date and print the corresponding day of the week. All divisions indicated above are integer divisions.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to input a series of positive integers and determine whether they are prime. The program should terminate if a negative integer is given as the input. A prime number is a number that is divisible by only one and itself. However one is not considered a prime number.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to find out whether a given number is a perfect number. The program should terminate if a negative integer is given as the input. A perfect number is a number whose factors other than itself add up to itself.</li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to find and display the minimum and the maximum among 10 numbers entered from the keyboard. Use a single-dimensional array to store the numbers entered. The numbers can be non-integers. An example would be as follows: <pre><code>Enter 10 numbers: 5 7.8 9.6 54 3.4 1.2 3 7 8.8 5\nMinimum = 1.2\nMaximum = 54\n</code></pre></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Suppose there are 4 students each having marks of 3 subjects. Write a program to read the marks from the keyboard and calculate and display the total marks of each student. Use a 2D (two-dimensional) array to store the marks. An example would be as follows: <pre><code>Enter the marks of four students, on four rows:\n50 60 80\n60 75 90\n30 49 99\n66 58 67\n\nTotal marks of four students:\n190\n225\n178\n191\n</code></pre></li> </ol> <pre><code># your solution\n</code></pre> <ol> <li>Write a program to read in two matrices A and B of dimensions 3\u00d74 and 4\u00d73 respectively, and compute and display their product AB (of dimensions 3\u00d73). Assume that the elements of the matrices are integers. Use functions to while implementing this program.</li> </ol> <pre><code># your solution\n</code></pre>"},{"location":"Python/Basics/13_end_of_course_exercise/#congratulation","title":"Congratulation!What's on your mind? Put it in the comments!","text":"<p>Congratulations on completing this Python Fundamentals Course</p>"},{"location":"Python/Libraries/12_numpy/","title":"Numpy Crash Course","text":""},{"location":"Python/Libraries/12_numpy/#objective","title":"Objective","text":"<p>In this lesson, you will learn all you need to know to get moving with numpy. ie:</p> <ul> <li> What is Numpy</li> <li> Inspecting Numpy arrays</li> <li> Performing array mathematics</li> <li> Subsetting, Slicing and Indexing arrays</li> <li> Array manipulation</li> </ul>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Python/Libraries/12_numpy/#what-is-numpy","title":"What is Numpy","text":"<p> Numpy is a python package used for scientific computing</p> <p> Numpy provides arrays which are greater and faster alternatives to traditional python lists. An array is a group of elements of the same data type</p> <p> A standard numpy array is required to have elements of the same data type.</p> <pre><code># Uncomment and run this cell to install numpy\n# !pip install numpy\n</code></pre>"},{"location":"Python/Libraries/12_numpy/#inspecting-our-arrays","title":"Inspecting our arrays","text":"<p>To use numpy, we'll first import it (you must have it installed for this to work)</p> <pre><code># import numpy module\nimport numpy as np\n</code></pre> <p>We can check the version we'll be using by using the <code>__version__</code> method</p> <pre><code># checking the numpy version\nnp.__version__\n</code></pre> <pre><code>'1.26.3'\n</code></pre> <p>Numpy gives us a more powerful Python List alternative data structure called a Numpy ndarray, we creat it using the <code>array()</code> from numpy</p> <pre><code># creating a numpy array\nnum_arr = np.array([1, 2, 3, 4])\n</code></pre> <p>The object that's created by <code>array()</code> is called <code>ndarray</code>. This can be shown by checking the type of the object using <code>type()</code></p> <pre><code># Checking type of object\ntype(num_arr)\n</code></pre> <pre><code>numpy.ndarray\n</code></pre>"},{"location":"Python/Libraries/12_numpy/#data-types","title":"Data Types","text":"<p>The table below describes some of the most common data types we use in numpy</p> Data Type Description <code>np.int64</code> Signed 64-bit integer types <code>np.float32</code> Standard double-precision floating point <code>np.complex</code> Complex numbers represented by 128 floats <code>np.bool</code> Boolean type storing <code>True</code> and <code>False</code> values <code>np.object</code> Python object type <code>np.string_</code> Fixed-length string type <code>np.unicode_</code> Fixed-length unicode type <p>We can check the shape of a numpy array by using the <code>shape</code> attribute as demonstrated below</p> <pre><code># shape of array\nnum_arr.shape\n</code></pre> <pre><code>(4,)\n</code></pre> <p>Dimensions</p> <p>Similarly, we find the the number of dimensions in our array using the <code>ndim</code> attribute. A dimension in NumPy refers to the number of axes or levels of depth in an array, determining its shape (e.g., 2D for a matrix, 3D for a tensor).</p> <pre><code># finding the number of dimensions\nnum_arr.ndim\n</code></pre> <pre><code>1\n</code></pre> <p>Length</p> <p>In NumPy, the length refers to the size of the first axis (dimension) of an array, which is the number of elements along that axis. We can use the <code>len()</code> method to find the length.</p> <pre><code># number of elements in array\nlen(num_arr)\n</code></pre> <pre><code>4\n</code></pre> <p>Size</p> <p>Size in NumPy refers to the total number of elements in an array across all dimensions. We can use the size of a numpy array using the <code>size</code> attribute</p> <pre><code># another way to get the number of elements\nnum_arr.size\n</code></pre> <pre><code>4\n</code></pre> <p>Data Type(<code>dtype</code>)</p> <p><code>dtype</code> in NumPy refers to the data type of the elements stored in an array, such as <code>int</code>, <code>float</code>, <code>bool</code>, etc.</p> <pre><code># finding data type of array elements\nprint(num_arr.dtype.name)\n</code></pre> <pre><code>int64\n</code></pre> <p>Converting Array Data Types</p> <p>We cas use <code>astype()</code> method to convert an array from one type to another.</p> <pre><code># converting an array\nfloat_arr = np.array([1.2, 3.5, 7.0])\n\n# use astype() to convert to a specific\nint_arr = float_arr.astype(int)\n\nprint(f'Array: {float_arr}, Data Type: {float_arr.dtype}')\nprint(f'Array: {int_arr}, Data Type: {int_arr.dtype}')\n</code></pre> <pre><code>Array: [1.2 3.5 7. ], Data Type: float64\nArray: [1 3 7], Data Type: int64\n</code></pre>"},{"location":"Python/Libraries/12_numpy/#ask-for-help","title":"Ask for help","text":"<pre><code>np.info(np.ndarray.shape)\n</code></pre> <pre><code>?np.ndarray.shape\n</code></pre>"},{"location":"Python/Libraries/12_numpy/#array-mathematics","title":"Array mathematics","text":"<p>Numpy has out of the box tools to help us perform some import mathematical operations</p>"},{"location":"Python/Libraries/12_numpy/#arithmetic-operations","title":"Arithmetic Operations","text":"<p>Arithmetic operations in NumPy are element-wise operations like addition, subtraction, multiplication, and division that can be performed directly between arrays or between an array and a scalar.</p> <pre><code># creating arrays\narray1 = np.array([1, 4, 6, 7])\narray2 = np.array([3, 5, 3, 1])\n</code></pre> <pre><code># subtract\ndifference1 = array2 - array1\nprint('difference1 =', difference1)\n\n# another way\ndifference2 = np.subtract(array2, array1)\nprint('difference2 =', difference2)\n</code></pre> <p>As we may notice, numpy does element-wise operations for ordinary arithmetic operations</p> <pre><code># sum\nsummation1 = array1 + array2\nprint('summation1 =', summation1)\n\n# another way\nsummation2 = np.add(array1, array2)\nprint('summation2 =', summation2)\n</code></pre>"},{"location":"Python/Libraries/12_numpy/#trigonometric-operations","title":"Trigonometric operations","text":"<p>Trigonometric operations in NumPy are functions like <code>np.sin()</code>, <code>np.cos()</code>, and <code>np.tan()</code> that perform element-wise trigonometric calculations on arrays.</p> <pre><code># sin\nprint('sin(array1) =', np.sin(array1))\n# cos\nprint('cos(array1) =', np.cos(array1))\n# log\nprint('log(array1) =', np.log(array1))\n</code></pre> <pre><code># dot product\narray1.dot(array2)\n</code></pre> <p>Given matrices A and B, the <code>dot</code> operation mulitiplies A with the transpose of B</p> <p>Research:</p> <p> another way to dot matrices (arrays)</p>"},{"location":"Python/Libraries/12_numpy/#comparison","title":"Comparison","text":"<p>In NumPy, comparison operators perform element-wise comparisons on arrays and return boolean arrays of the same shape, where each element indicates True or False based on the corresponding element-wise comparison.</p> <pre><code>array1 == array2\n</code></pre> <pre><code>array1 &gt; 3\n</code></pre>"},{"location":"Python/Libraries/12_numpy/#aggregate-functions","title":"Aggregate functions","text":"<p>NumPy provides several aggregate functions that perform operations across the elements of an array and return a single scalar value.</p> <pre><code># average\nmean = array1.mean()\nprint('Mean: ', mean)\n\n# min\nminimum = array1.min()\nprint('Minimum: ', minimum)\n\n# max\nmaximum = array1.max()\nprint('Maximum: ', maximum)\n\n# corrcoef\ncorrelation_coefficient = np.corrcoef(array1, array2)\nprint('Correlation Coefficient: ', correlation_coefficient)\n\nstandard_deviation = np.std(array1)\nprint('Standard Deviation: ', standard_deviation)\n</code></pre> <p>Research:</p> <p> copying arrays (you might meet <code>view()</code>, <code>copy()</code>) </p>"},{"location":"Python/Libraries/12_numpy/#subsetting-slicing-and-indexing","title":"Subsetting, Slicing and Indexing","text":"<p> Indexing is the technique we use to access individual elements in an array. 0 represents the first element, 1 the represents second element and so on.</p> <p> Slicing is used to access elements of an array using a range of two indexes. The first index is the start of the range while the second index is the end of the range. The indexes are separated by a colon ie <code>[start:end]</code></p> <pre><code># Creating numpy arrays of different dimension\n# 1D array\narr1 = np.array([1, 4, 6, 7])\nprint('Array1 (1D): \\n', arr1)\n\n# 2D array\narr2 = np.array([[1.5, 2, 3], [4, 5, 6]]) \nprint('Array2 (2D): \\n', arr2)\n\n#3D array\narr3 = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]], \n                 [[10, 11, 12], [13, 14, 15], [16, 17, 18]]]) \nprint('Array3 (3D): \\n', arr3)\n</code></pre> <pre><code>Array1 (1D): \n [1 4 6 7]\nArray2 (2D): \n [[1.5 2.  3. ]\n [4.  5.  6. ]]\nArray3 (3D): \n [[[ 1  2  3]\n  [ 4  5  6]\n  [ 7  8  9]]\n\n [[10 11 12]\n  [13 14 15]\n  [16 17 18]]]\n</code></pre> <pre><code># find the dimensions of an array\nprint('Array1 (1D):', arr1.shape)\nprint('Array2 (2D):', arr2.shape)\nprint('Array3 (3D):', arr3.shape)\n</code></pre> <pre><code>Array1 (1D): (4,)\nArray2 (2D): (2, 3)\nArray3 (3D): (2, 3, 3)\n</code></pre>"},{"location":"Python/Libraries/12_numpy/#indexing","title":"Indexing","text":"<pre><code># accessing items in a 1D array\narr1[2]\n</code></pre> <pre><code>6\n</code></pre> <pre><code># accessing items in 2D array\narr2[1, 2]\n</code></pre> <pre><code>6.0\n</code></pre> <pre><code># accessing in a 3D array\narr3[0, 1, 2]\n</code></pre> <pre><code>6\n</code></pre>"},{"location":"Python/Libraries/12_numpy/#slicing","title":"slicing","text":"<pre><code># slicing 1D array\narr1[0:3]\n</code></pre> <pre><code>array([1, 4, 6])\n</code></pre> <pre><code># slicing a 2D array\narr2[1, 1:]\n</code></pre> <pre><code>array([5., 6.])\n</code></pre> <pre><code># slicing a 3D array\nfirst = arr3[0, 2]\nsecond = arr3[1, 0]\n\nnp.concatenate((first, second))\n</code></pre> <pre><code>array([ 7,  8,  9, 10, 11, 12])\n</code></pre>"},{"location":"Python/Libraries/12_numpy/#boolean-indexing","title":"Boolean Indexing","text":"<p>Boolean indexing in NumPy allows you to select elements from an array based on a boolean condition or a boolean array of the same shape. The elements corresponding to True values in the boolean array/condition are selected, while those corresponding to False are discarded. </p> <pre><code># boolean indexing\narr1[arr1 &lt; 5]\n</code></pre> <p>Research:</p> <p> Fancy Indexing</p>"},{"location":"Python/Libraries/12_numpy/#array-manipulation","title":"Array manipulation","text":"<p>NumPy provides a wide range of functions that allow you to change the shape, dimensions, and structure of arrays to suit your needs</p> <pre><code>print(arr2)\n</code></pre> <pre><code>[[1.5 2.  3. ]\n [4.  5.  6. ]]\n</code></pre> <pre><code># transpose\narr2_transpose1 = np.transpose(arr2) \nprint('Transpose1: \\n', arr2_transpose1)\n\n# another way\narr2_transpose2 = arr2.T\nprint('Transpose2: \\n', arr2_transpose2)\n</code></pre> <pre><code>Transpose1: \n [[1.5 4. ]\n [2.  5. ]\n [3.  6. ]]\nTranspose2: \n [[1.5 4. ]\n [2.  5. ]\n [3.  6. ]]\n</code></pre> <pre><code># combining arrays\nfirst = arr3[0, 2]\nsecond = arr3[1, 0]\n\nnp.concatenate((first, second))\n</code></pre> <pre><code>array([ 7,  8,  9, 10, 11, 12])\n</code></pre> <pre><code>test_arr1 = np.array([[7, 8, 9], [10, 11, 12]])\ntest_arr2 = np.array([[1, 2, 3], [4, 5, 6]])\n\nnp.concatenate((test_arr1, test_arr2), axis=1)\n</code></pre> <pre><code>array([[ 7,  8,  9,  1,  2,  3],\n       [10, 11, 12,  4,  5,  6]])\n</code></pre>"},{"location":"Python/Libraries/12_numpy/#some-homework","title":"Some homework","text":"<p>Research:</p> <p>Adding/Removing Elements  <code>resize()</code></p> <p> <code>append()</code></p> <p> <code>insert()</code></p> <p> <code>delete()</code></p> <p>Changing array shape  <code>ravel()</code></p> <p> <code>reshape()</code></p> <pre><code># stacking \n# np.vstack((a,b))\n# np.hstack((a,b))\n# np.column_stack((a,b))\n# np.c_[a, b]\n</code></pre> <pre><code># splitting arrays\n# np.hsplit()\n# np.vsplit()\n</code></pre> What's on your mind? Put it in the comments!"},{"location":"Python/Libraries/13_pandas/","title":"Pandas Crash Course","text":"<p>Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool. </p> <p>This tutoral will show you the basic and intermediate concepts in Pandas</p>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code># Uncomment and run this cell to install pandas\n# !pip install pandas\n# !pip install openpyxl\n</code></pre> <pre><code># import pandas\nimport pandas as pd\n</code></pre> <pre><code># to check python version\nprint(pd.__version__)\n</code></pre> <pre><code>2.2.2\n</code></pre>"},{"location":"Python/Libraries/13_pandas/#creating-dataframes","title":"Creating Dataframes","text":"<p>The reason why data analysts like pandas is because pandas provides them with a very powerful data structure called a dataframe. A dataframe is a 2D structure that offers us rows and columns similar to tables in excel, sql etc</p>"},{"location":"Python/Libraries/13_pandas/#creating-dataframes-from-existing-files","title":"Creating dataframes from existing files","text":"<p>If you already have some data in maybe an excel, csv, or stata file, you can be able to load it into a dataframe and then perform manipulation.</p> <pre><code># loading an excel file into a dataframe\ndata = pd.read_excel(io='../assets/demo.xlsx')\n</code></pre>  Note!<p>You must have `openpyxl` installed to be able to read excel files using pandas.</p> <p>The data structure that is returned by the statement is called a <code>DataFrame</code></p> <pre><code># checking the datatype of the data object\nprint(type(data))\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\n</code></pre> <pre><code># randomly sample some values\ndata.sample(n=5)\n</code></pre> Age Gender Marital Status Address Income Income Category Job Category 137 32 m 0 1 26 2 2 92 61 m 1 18 23 1 3 39 21 f 0 0 13 1 1 41 56 f 0 7 213 4 3 48 51 f 0 0 47 2 1"},{"location":"Python/Libraries/13_pandas/#creating-a-dataframe-from-a-dictionary","title":"Creating a DataFrame from a Dictionary","text":"<p>For the previous case, we may be having some data already, but sometimes we may want to create a dataframe from scratch. We can create pandas dataframes using two major ways:</p> <ul> <li> Using a dictionary</li> <li> Using a 2D list</li> </ul> <p>We've met dictionaries and lists in the Containers/Collections module of the Python 3 Beginner Course.</p> <p>To begin with, we're gonna create a dataframe from a dictionary. The way we create the dictionary is important, keys will be used as column names, and the values will be used as rows. So, you typically want your values to be lists or tuples.</p> <p>Now you might observe that the values (lists) are of equal length</p> <pre><code># create a pandas dataframe using a dictionary\ndata_dictionary = {\n    'age': [65, 51, 45, 38, 40],\n    'gender': ['m', 'm', 'm', 'f', 'm'],\n    'income': [42, 148, 147, 43, 89]\n}\n\ndataframe_from_dict = pd.DataFrame(data=data_dictionary)\n</code></pre> <pre><code># display the dataframe\ndataframe_from_dict\n</code></pre> age gender income 0 65 m 42 1 51 m 148 2 45 m 147 3 38 f 43 4 40 m 89 <p>Next up, we are gonna create a dataframe from a list. For this case, the list be of 2D shape. Again, the way we organize data in our list is important. We should organize that in a format close to rows and columns as showed below. </p> <p>It turns out that when creating a dataframe from a list, we need to explicitly define the column names as demonstrated below</p> <pre><code># creating a dataframe from a 2D list\ndata_list = [\n    [28, 'm', 24],\n    [59, 'm', 841],\n    [54, 'm', 741],\n    [83, 'f', 34],\n    [34, 'm', 98]\n]\n# let's specify the column names\nnames = ['age', 'gender', 'income']\n\ndataframe_from_list = pd.DataFrame(data=data_list, \n                                   columns=names)\n</code></pre> <pre><code># display the dataframe\ndataframe_from_list\n</code></pre> age gender income 0 28 m 24 1 59 m 841 2 54 m 741 3 83 f 34 4 34 m 98 <p>Before we continue, I would like to share some ways you would look for help or more information about pandas methods.</p> <ul> <li> One way is by using the `help()` method.</li> <li> Another is by using the query operator `?`</li> </ul> <pre><code># Finding more information\n# help(pd.DataFrame)\n</code></pre> <pre><code>## Another way to find more information\n# ?pd.DataFrame\n</code></pre> <p>The latter is my favorite because it works in all situations.</p>"},{"location":"Python/Libraries/13_pandas/#concatenating-dataframes","title":"Concatenating DataFrames","text":"<p>Sometimes there's a need to add two or more dataframes. To perform this, for the start, we can use the <code>pd.concat()</code>. The <code>concat()</code> method takes in a list of dataframes we would like to combine</p> <p>Remember we created 2 dataframes earlier, one from a dictionary and another from a list, now let's combine them to make one dataframe</p> <pre><code>concatenated_dataframe = pd.concat(\n    objs=[dataframe_from_dict, dataframe_from_list], \n    ignore_index=True\n)\n</code></pre> <pre><code>concatenated_dataframe\n</code></pre> age gender income 0 65 m 42 1 51 m 148 2 45 m 147 3 38 f 43 4 40 m 89 5 28 m 24 6 59 m 841 7 54 m 741 8 83 f 34 9 34 m 98 <p>We set <code>ignore_index=True</code> to correct the indexing so that we can have unique indexes and hence be able to able to uniquely identify rows by index</p> <p>Exercise: Demonstrate how to concatenate two or more dataframes by column ie if dataframe A has columns a, b, c and dataframe B has columns x, y, z, the resulting dataframe should have columns a, b, c, x, y, z</p>"},{"location":"Python/Libraries/13_pandas/#sampling-values-in-the-dataframe","title":"Sampling values in the DataFrame","text":"<p>In this section, we are gonna look at how to pick out some sections or parts of the data. We'll look at <code>head()</code>, <code>tail()</code> and <code>sample()</code>.</p> <p>To demonstrate these, we'll continue with our concatenated dataframe from the previous section</p> <p>We can use <code>head()</code> to look at the top part of the data. Out of the box, it returns the top 5 rows, however modifying the value for <code>n</code> can help us pick a specific number of rows from the top.</p> <pre><code># We can have look at the top part \nconcatenated_dataframe.head(n=3)\n</code></pre> age gender income 0 65 m 42 1 51 m 148 2 45 m 147 <p>We can use <code>tail()</code> to look at the bottom part of the data. Out of the box, it returns the bottom 5 rows, however modifying the value for <code>n</code> can help us pick a specific number of rows from the bottom.</p> <pre><code># We can look at the bottom part\nconcatenated_dataframe.tail(n=3)\n</code></pre> age gender income 7 54 m 741 8 83 f 34 9 34 m 98 <p>We can use <code>sample()</code> to look random rows data rows. Out of the box, it returns only 1 row, however modifying the value for <code>n</code> can help us pick a specific number of rows at random.</p> <pre><code># We can also randomly sample out some values in a DataFrame\nconcatenated_dataframe.sample(n=3)\n</code></pre> age gender income 0 65 m 42 2 45 m 147 8 83 f 34"},{"location":"Python/Libraries/13_pandas/#selection","title":"Selection","text":"<p>In this section we are gonna look at some tricks and techniques we can use to pick some really specific values from the dataframe</p>"},{"location":"Python/Libraries/13_pandas/#selecting-boolean-indexing-and-setting","title":"Selecting, Boolean Indexing and Setting","text":"<p>To demonstrate these, we're creating a little countries dataframe. As you may observe we use <code>pd.DataFrame()</code> to create our dataframe and notice we're passing in a dictionary for the value of data.</p> <pre><code>country_data = pd.DataFrame(data={\n    'Country': ['Uganda', 'Kenya', 'Tanzania'],\n    'Capital': ['Kampala', 'Nairobi', 'Dodoma'],\n    'Population': [11190846, 1303171035, 207847528]\n    })\ncountry_data\n</code></pre> Country Capital Population 0 Uganda Kampala 11190846 1 Kenya Nairobi 1303171035 2 Tanzania Dodoma 207847528 <p>We can pick a specific value (or values) from a dataframe by indexing usin the <code>iloc</code> and <code>iat</code> methods. We insert the row number and the column number of an item that we want to pick from the dataframe in the square brackets. </p> <p>We can also use these techniques to replace values in a dataframe.</p> <pre><code># position 1\nprint(country_data.iloc[0, 0])\nprint(country_data.iloc[2, 1])\n</code></pre> <pre><code>Uganda\nDodoma\n</code></pre> <pre><code># position 2\nprint(country_data.iat[0, 0])\nprint(country_data.iat[2, 1])\n</code></pre> <pre><code>Uganda\nDodoma\n</code></pre> <p>Ponder:</p> <p> How can you use the <code>pd.DataFrame.iat</code> method to replace (or modify) a specific value in a dataframe</p> <p>We can access any value(s) by their row index and column name with the help of the <code>loc[]</code> and <code>at[]</code> methods.</p> <p>As you may observe the difference now is that we are using row index and column name instead of row index and column index for <code>iloc[]</code> and <code>iat[]</code></p> <pre><code># using label\nprint(country_data.loc[0, 'Capital'])\nprint(country_data.loc[1, 'Population'])\n</code></pre> <pre><code>Kampala\n1303171035\n</code></pre> <pre><code># using label\nprint(country_data.at[2, 'Population'])\nprint(country_data.at[1, 'Capital'])\n</code></pre> <pre><code>207847528\nNairobi\n</code></pre> <p>We can be able to pick out an entire column by either using the <code>.</code> operator or <code>[]</code>.</p> <ul> <li> We use the `.` operator when a column name is one single word</li> <li> We can use the `[]` when the column name is containing more that one word</li> <li> We can also use the `[]` when creating or assigning values to columns in a dataframe</li> </ul> <pre><code># picking out data from a specific column\ncountry_data.Country\n</code></pre> <pre><code>0      Uganda\n1       Kenya\n2    Tanzania\nName: Country, dtype: object\n</code></pre> <pre><code># another way to pick data from a specific column\ncountry_data['Country']\n</code></pre> <pre><code>0      Uganda\n1       Kenya\n2    Tanzania\nName: Country, dtype: object\n</code></pre> <p>The data structure that is returned by the statement is called a <code>Series</code></p> <pre><code>lakes = ['Albert', 'Turkana', 'Tanganyika']\ncountry_data['Lake'] = lakes\n\n# lets display the updated data\ncountry_data\n</code></pre> Country Capital Population Lake 0 Uganda Kampala 11190846 Albert 1 Kenya Nairobi 1303171035 Turkana 2 Tanzania Dodoma 207847528 Tanganyika <pre><code># lets check it\ntype(country_data['Capital'])\n</code></pre> <pre><code>pandas.core.series.Series\n</code></pre> <pre><code># Get specific row data (using indexing)\ncountry_data.iloc[0]\n</code></pre> <pre><code>Country         Uganda\nCapital        Kampala\nPopulation    11190846\nLake            Albert\nName: 0, dtype: object\n</code></pre> <p>We can get a range of rows by passing into <code>iloc[]</code> a range of indexes. The demonstration below returns rows of indexes 0 until but not including 2 ie 0 for Uganda and 1 for Kenya</p> <pre><code># Get specific rows (using subsetting)\ncountry_data.iloc[0:2]\n</code></pre> Country Capital Population Lake 0 Uganda Kampala 11190846 Albert 1 Kenya Nairobi 1303171035 Turkana <p>We can be able to pickout only rows whose values satisfy a specific condition, this trick is called boolean indexing. In the example below, we find all rows whose contry is Uganda</p> <pre><code># get all rows that have a column-value matching a specific value\n# eg where country is Belgium\ncountry_data[country_data['Country'] == 'Uganda']\n</code></pre> Country Capital Population Lake 0 Uganda Kampala 11190846 Albert <pre><code># Think about this\ncountry_data['Country'] == 'Tanzania'\n</code></pre> <pre><code>0    False\n1    False\n2     True\nName: Country, dtype: bool\n</code></pre> <p>You donot have to submit that </p>"},{"location":"Python/Libraries/13_pandas/#dropping","title":"Dropping","text":"<p>In this part we will learn some tricks and techniques to drop or remove some data from a dataframe.</p> <pre><code>country_data\n</code></pre> Country Capital Population Lake 0 Uganda Kampala 11190846 Albert 1 Kenya Nairobi 1303171035 Turkana 2 Tanzania Dodoma 207847528 Tanganyika <p>We may realize that all our population values are terribly wrong and may choose the entire column. we can do this by using the <code>drop()</code> method. We specify the column name and <code>axis=1</code> to drop a column. </p> <pre><code># drop a column from a dataframe\ncountry_data.drop(\n    labels='Population', \n    axis=1\n)\n</code></pre> Country Capital Lake 0 Uganda Kampala Albert 1 Kenya Nairobi Turkana 2 Tanzania Dodoma Tanganyika <p>To drop many columns, we can pass all the columns to the <code>drop()</code> method as a list or tuple, and again specify the <code>axis=1</code>.</p> <pre><code>country_data.drop(\n    labels=['Lake', 'Population'], \n    axis=1\n)\n</code></pre> Country Capital 0 Uganda Kampala 1 Kenya Nairobi 2 Tanzania Dodoma <p>Another way to drop many columns is by passing them to the <code>drop()</code> method as a list value to the <code>columns</code> parameter. In this case we don't need to specify the <code>axis</code>.</p> <pre><code># You can drop many columns by passing in a columns list\ncountry_data.drop(columns=['Country', 'Population'])\n</code></pre> Capital Lake 0 Kampala Albert 1 Nairobi Turkana 2 Dodoma Tanganyika <p>To drop a row or many rows, we shall pass the index(es) as labels to the <code>drop</code> method method and optionally set <code>axis=0</code>. It turns out that the default value for <code>axis</code> is actually 0. Below, we have some two examples.</p> <pre><code># how to drop 1 row\ncountry_data.drop(labels=0) # drops out Uganda\n</code></pre> Country Capital Population Lake 1 Kenya Nairobi 1303171035 Turkana 2 Tanzania Dodoma 207847528 Tanganyika <pre><code># how to drop row data\ncountry_data.drop(labels=[0, 2], axis=0)\n</code></pre> Country Capital Population Lake 1 Kenya Nairobi 1303171035 Turkana <p>This drops rows in indexes 0 and 2, ie Uganda and Tanzania</p>"},{"location":"Python/Libraries/13_pandas/#research-on","title":"Research on:","text":"<p> sort and rank data</p>"},{"location":"Python/Libraries/13_pandas/#retrieving-information-about-dataframe","title":"Retrieving information about DataFrame","text":"<p>Pandas offers us some quick way with which we can find some quick information about our dataset (or dataframe)</p>"},{"location":"Python/Libraries/13_pandas/#basic-information","title":"Basic Information","text":"<pre><code>country_data = pd.DataFrame({\n    'Country': ['Uganda', 'Kenya', 'Tanzania'],\n    'Capital': ['Kampala', None, None],\n    'Population': [11190846, 1303171035, 207847528]\n    })\n\ncountry_data\n</code></pre> Country Capital Population 0 Uganda Kampala 11190846 1 Kenya None 1303171035 2 Tanzania None 207847528 <p>We can use the <code>shape</code> attribute to obtain the number of rows and columns available in our dataset as illustrated.</p> <pre><code># shape of a dataframe (ie rows, columns)\ncountry_data.shape\n</code></pre> <pre><code>(3, 3)\n</code></pre> <p>If you're only interested in the number of rows you can use the <code>len()</code> method to find that eg</p> <pre><code># len of a dataframe (ie no of rows)\nlen(country_data)\n</code></pre> <pre><code>3\n</code></pre> <p>If you are interested in looking at the columns (column names), you can use the <code>columns</code> attribute to obtain the <code>Index</code> object of column names</p> <pre><code># Get all columns in a dataframe\ncountry_data.columns\n</code></pre> <pre><code>Index(['Country', 'Capital', 'Population'], dtype='object')\n</code></pre> <p>We can also use the <code>len()</code> method on this <code>Index</code> object to obtain the number of columns</p> <pre><code>len(country_data.columns)\n</code></pre> <pre><code>3\n</code></pre> <p>We can use the <code>info()</code> method to find some information on our dataframe ie:</p> <ul> <li> Columns (All columns in the dataframe)</li> <li> Non-Null Count (Number of non null values per column)</li> <li> Dtype (Data type each column)</li> <li> Total number of entries (rows)</li> </ul> <pre><code># get some basic info about the dataframe\ncountry_data.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   Country     3 non-null      object\n 1   Capital     1 non-null      object\n 2   Population  3 non-null      int64 \ndtypes: int64(1), object(2)\nmemory usage: 204.0+ bytes\n</code></pre> <p>By using the <code>count()</code> method on the dataframe, we can obtain the number of non-null values per a column</p> <pre><code># Count non-null values in each column\ncountry_data.count()\n</code></pre> <pre><code>Country       3\nCapital       1\nPopulation    3\ndtype: int64\n</code></pre>"},{"location":"Python/Libraries/13_pandas/#summary","title":"Summary","text":"<p>Finally, we can use the <code>describe()</code> to obtain some quick summary (descriptive) statistics about our data eg count, mean, standard deviation, minimum and maximum values, percentile</p> <pre><code># summary statistics\ncountry_data.describe()\n</code></pre> Population count 3.000000e+00 mean 5.074031e+08 std 6.961346e+08 min 1.119085e+07 25% 1.095192e+08 50% 2.078475e+08 75% 7.555093e+08 max 1.303171e+09"},{"location":"Python/Libraries/13_pandas/#research","title":"Research","text":"<p>Find out how to get for specific columns:</p> <ul> <li> mean</li> <li> median</li> <li> cummulative sum</li> <li> minimum</li> <li> maximum</li> </ul> What's on your mind? Put it in the comments!"},{"location":"Python/Libraries/31_matplotlib_refined/","title":"Matplotlib Crash Course","text":""},{"location":"Python/Libraries/31_matplotlib_refined/#what-is-matploblib","title":"What is Matploblib","text":"<p>Matplotlib is a powerful plotting library in Python commonly used for data visualization. </p> <p>When working with datasets, you can use Matplotlib to create various plots to explore and visualize the data. </p> <p>Here are some major plots you can create using Matplotlib with the Titanic dataset:</p>  Don't Miss Any Updates! <p> Before we continue, we have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p> <pre><code># # Uncomment and run this cell to install the libraries\n# !pip install pandas matplotlib dataidea\n</code></pre> <pre><code># import the libraries, packages and modules\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom dataidea.datasets import loadDataset\n</code></pre> <p>Let's demonstrate each of the plots using the Titanic dataset.  We'll first load the dataset and then create each plot using Matplotlib.</p> <pre><code># Load the Titanic dataset\ntitanic_df = loadDataset('titanic')\n</code></pre> <p>We can load this dataset like this because it is inbuilt in the dataidea package</p> <pre><code>titanic_df.head(n=5)\n</code></pre> pclass survived name sex age sibsp parch ticket fare cabin embarked boat body home.dest 0 1.0 1.0 Allen, Miss. Elisabeth Walton female 29.0000 0.0 0.0 24160 211.3375 B5 S 2 NaN St Louis, MO 1 1.0 1.0 Allison, Master. Hudson Trevor male 0.9167 1.0 2.0 113781 151.5500 C22 C26 S 11 NaN Montreal, PQ / Chesterville, ON 2 1.0 0.0 Allison, Miss. Helen Loraine female 2.0000 1.0 2.0 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON 3 1.0 0.0 Allison, Mr. Hudson Joshua Creighton male 30.0000 1.0 2.0 113781 151.5500 C22 C26 S NaN 135.0 Montreal, PQ / Chesterville, ON 4 1.0 0.0 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) female 25.0000 1.0 2.0 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON <ol> <li>Bar Plot: You can create a bar plot to visualize categorical data such as the number of passengers in each class (first class, second class, third class), the number of survivors vs. non-survivors, or the number of passengers embarked from each port (Cherbourg, Queenstown, Southampton).</li> </ol> <pre><code># 1. Bar Plot - Number of passengers in each class\nclass_counts = titanic_df.pclass.value_counts()\nclasses = class_counts.index\ncounts = class_counts.values\n\nplt.bar(x=classes, height=counts, color='#dd8604')\nplt.title('Number of Passengers Per Passenger Class')\nplt.xlabel('Passenger Class')\nplt.ylabel('Number of Passengers')\n\nplt.show()\n</code></pre> <p></p> <p>It's easy to see from the graph that the 3rd class had the largest number of passengers, followed by the 1st class and 2nd class comes last</p> <ol> <li>Histogram: Histograms are useful for visualizing the distribution of continuous variables such as age or fare. You can create histograms to see the age distribution of passengers or the fare distribution.</li> </ol> <pre><code># 2. Histogram - Age distribution of passengers\nages = titanic_df.age\nplt.hist(x=ages, bins=20, color='#dd8604', \n         edgecolor='#66FDEE')\nplt.title('Age Distribution of Passengers')\nplt.ylabel('Frequency')\nplt.xlabel('Age')\nplt.show()\n</code></pre> <p></p> <p>From the histogram we can observe that:</p> <ul> <li> The majority of the people we of ages between 15 and 35</li> <li> Fewer older people(above 60 years) boarded the titanic (below 20)t</li> </ul> <ol> <li>Box Plot: A box plot can be used to show the distribution of a continuous variable across different categories. For example, you can create a box plot to visualize the distribution of age or fare across different passenger classes.</li> </ol> <p>3.1. Age distribution boxplot</p> <pre><code># 3.1 Age distribution boxplot\nages = titanic_df.age.dropna()\nplt.boxplot(x=ages, vert=False,)\nplt.title('Age Distribution of Passengers')\nplt.xlabel('Age')\nplt.show()\n</code></pre> <p></p> <p>Features of a box plot:</p> <p> Box: The box in a boxplot represents the interquartile range (IQR), which contains the middle 50% of the data. The top and bottom edges of the box are the third quartile (Q3) and the first quartile (Q1), respectively.</p> <p> Median Line: A line inside the box indicates the median (Q2) of the data, which is the middle value of the dataset.</p> <p> Whiskers: The whiskers extend from the edges of the box to the smallest and largest values within 1.5 times the IQR from Q1 and Q3. They represent the range of the bulk of the data.</p> <p> Outliers: Data points that fall outside the whiskers are considered outliers. They are typically plotted as individual points. Outliers can be indicative of variability or errors in the data.</p> <p> Minimum and Maximum: The ends of the whiskers show the minimum and maximum values within the range of 1.5 times the IQR from the first and third quartiles.</p> <p>Meaning: A boxplot provides a visual summary of several important aspects of a dataset:</p> <ul> <li>Central Tendency: The median line shows the central point of the data.</li> <li>Spread: The IQR (the length of the box) shows the spread of the middle 50% of the data.</li> <li>Symmetry and Skewness: The relative position of the median within the box and the length of the whiskers can indicate whether the data is symmetric or skewed.</li> <li>Outliers: Individual points outside the whiskers highlight potential outliers.</li> </ul> <p>Boxplots are particularly useful for comparing distributions between several groups or datasets and identifying outliers and potential anomalies.</p> <p>3.2 Age Distribution Across Passenger Classes</p> <pre><code># 3. Box Plot - Distribution of age across passenger classes\nplt.boxplot([titanic_df[titanic_df['pclass'] == 1]['age'].dropna(),\n             titanic_df[titanic_df['pclass'] == 2]['age'].dropna(),\n             titanic_df[titanic_df['pclass'] == 3]['age'].dropna()],\n            labels=['1st Class', '2nd Class', '3rd Class'])\nplt.xlabel('Passenger Class')\nplt.ylabel('Age')\nplt.title('Distribution of Age Across Passenger Classes')\nplt.show()\n</code></pre> <pre><code>/tmp/ipykernel_16695/4289029800.py:2: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n  plt.boxplot([titanic_df[titanic_df['pclass'] == 1]['age'].dropna(),\n</code></pre> <p></p> <ol> <li>Scatter Plot: Scatter plots are helpful for visualizing the relationship between two continuous variables. You can create scatter plots to explore relationships such as age vs. fare. Read more about the scatter plot from the Matplotlib documentation</li> </ol> <pre><code># 4. Scatter Plot - Age vs. Fare\nplt.scatter(\n    x=titanic_df['age'], \n    y=titanic_df['fare'], \n    alpha=.5, \n    c=titanic_df['survived'], \n    cmap=ListedColormap(['#dd8604', '#000000'])\n)\nplt.xlabel('Age')\nplt.ylabel('Fare')\nplt.title('Age vs. Fare')\nplt.colorbar(label='Survived')  \nplt.show()\n</code></pre> <p></p> <p>I don't about you but for me I don't see a linear relationship between the age and fare of the titanic passengers</p> <ol> <li>Pie Chart: Pie charts can be used to visualize the proportion of different categories within a dataset. For example, you can create a pie chart to show the proportion of male vs. female passengers or the proportion of survivors vs. non-survivors.</li> </ol> <pre><code># 5. Pie Chart - Proportion of male vs. female passengers\ngender_counts = titanic_df['sex'].value_counts()\nplt.pie(x=gender_counts, labels=gender_counts.index, \n        autopct='%1.1f%%', startangle=90, \n        colors=['#dd8604', '#66FDEE'])\nplt.title('Proportion of Male vs. Female Passengers')\nplt.legend(loc='lower right')\nplt.show()\n</code></pre> <p></p> <ol> <li>Stacked Bar Plot: Stacked bar plots can be used to compare the composition of different categories across groups. For example, you can create a stacked bar plot to compare the proportion of survivors and non-survivors within each passenger class.</li> </ol> <pre><code># 6. Stacked Bar Plot - Survival status within each passenger class\nsurvival_counts = titanic_df.groupby(['pclass', 'survived']).size().unstack()\nsurvival_counts.plot(kind='bar', stacked=True,  \n                     color=['#dd8604', '#66FDEE'])\nplt.xlabel('Passenger Class')\nplt.ylabel('Number of Passengers')\nplt.title('Survival Status Within Each Passenger Class')\nplt.legend(['Did not survive', 'Survived'])\nplt.show()\n</code></pre> <p></p> <pre><code>titanic_df.groupby(['pclass', 'survived']).size().unstack()\n</code></pre> survived 0.0 1.0 pclass 1.0 123 200 2.0 158 119 3.0 528 181 <p>We observe that:</p> <ul> <li> More passengers in class 1 survived than those that did not survive (200 vs 123)</li> <li> Most of the passengers in class 3 did not survive (528 vs 181)</li> <li> Slightly more passengers did not survive as compared to those that survived in class 2 (152 vs 119)</li> </ul> <ol> <li>Line Plot: Line plots can be useful for visualizing trends over time or continuous variables. While the Titanic dataset may not have explicit time data, you can still use line plots to visualize trends such as the change in survival rate with increasing age or fare.</li> </ol> <pre><code># 7. Line Plot - Mean age of passengers by passenger class\nmean_age_by_class = titanic_df.groupby('pclass')['age'].mean()\nplt.plot(mean_age_by_class.index, mean_age_by_class.values, \n         marker='*', color='#dd8604')\nplt.xlabel('Passenger Class')\nplt.ylabel('Mean Age')\nplt.title('Mean Age of Passengers by Passenger Class')\nplt.show()\n</code></pre> <p></p> <p>We can quickly see the average ages for each passenger class, ie:</p> <ul> <li> Around 39 for first class</li> <li> Around 30 for second class</li> <li> Around 25 for third class</li> </ul> <p>These are some of the major plots you can create using Matplotlib. Each plot serves a different purpose and can help you gain insights into the data and explore relationships between variables.</p> <pre><code>air_passengers_data = loadDataset('air_passengers')\nair_passengers_data.head()\n</code></pre> Month Passengers 0 1949-01 112 1 1949-02 118 2 1949-03 132 3 1949-04 129 4 1949-05 121 <pre><code>air_passengers_data['Month'] = pd.to_datetime(air_passengers_data.Month)\nplt.plot('Month', 'Passengers', data=air_passengers_data, color='#008374')\nplt.xlabel('Years')\nplt.ylabel('Number of Passengers')\nplt.show()\n</code></pre> <p></p> <p>We can observe that the number of passengers seems to increase with time</p>"},{"location":"Python/Libraries/31_matplotlib_refined/#review","title":"ReviewWhat's on your mind? Put it in the comments!","text":"<p>Congratulations on reaching the end of this tutorial. In this tutorial, we have learned the basic graphs and how to interprete them. ie</p> <ul> <li> Bar chart</li> <li> Histogram</li> <li> Scatter plot</li> <li> Line plot</li> <li> Box plot</li> <li> Pie chart</li> <li> Stacked bar chart</li> </ul>"},{"location":"Python/Python%20Builtin/filter_function/","title":"The <code>filter()</code> function in Python","text":"<p>The filter() function is used to filter elements from an iterable based on a condition.</p>"},{"location":"Python/Python%20Builtin/filter_function/#syntax","title":"Syntax","text":"<pre><code>filter(function, iterable)\n</code></pre> <p><code>function</code>: A function that returns True or False.</p> <p><code>iterable</code>: The list (or tuple, etc.) to filter.</p> <p>It returns a filter object, which you can convert to a list, set, or tuple.</p>"},{"location":"Python/Python%20Builtin/filter_function/#example","title":"Example","text":"<p>Let\u2019s say we want to filter out only even numbers from a list.</p>"},{"location":"Python/Python%20Builtin/filter_function/#without-filter","title":"Without <code>filter()</code>:","text":"<pre><code>numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]\neven_numbers = []\n\nfor num in numbers:\n  if num % 2 == 0:\n    even_numbers.append(num)\n\nprint(even_numbers)\n</code></pre> <pre><code>[2, 4, 6, 8]\n</code></pre>"},{"location":"Python/Python%20Builtin/filter_function/#with-filter","title":"With <code>filter()</code>:","text":"<pre><code>even_numbers = filter(lambda num: num % 2 == 0, numbers)\nprint(list(even_numbers))\n</code></pre> <pre><code>[2, 4, 6, 8]\n</code></pre>"},{"location":"Python/Python%20Builtin/filter_function/#using-named-functions","title":"Using named functions:","text":"<p>We can also use named functions instead of lambda functions, as of the example below</p> <pre><code>def is_even(num):\n  return num % 2 == 0\n\neven_numbers = filter(is_even, numbers)\n\nprint(list(even_numbers))\n</code></pre> <pre><code>[2, 4, 6, 8]\n</code></pre>"},{"location":"Python/Python%20Builtin/filter_function/#using-filter-with-strings","title":"Using <code>filter()</code> with strings:","text":"<p>In the example below, we filter out the \"short\" words from the list of words</p> <pre><code>words = ['data', 'ai', 'machine', 'learning']\n\nlong_words = filter(lambda word: len(word) &gt; 4, words)\n\nprint(list(long_words))\n</code></pre> <pre><code>['machine', 'learning']\n</code></pre>"},{"location":"Python/Python%20Builtin/filter_function/#using-filter-to-clean-data","title":"Using <code>filter()</code> to clean data:","text":"<p>In the example below, we remove all the None values from the list</p> <pre><code>data = [10, None, 25, None, 40, 0]\n\ncleaned = filter(lambda value: value is not None, data)\nprint(list(cleaned))\n</code></pre> <pre><code>[10, 25, 40, 0]\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Python/Python%20Builtin/map_function/","title":"The <code>map()</code> function:","text":"<p>The <code>map()</code> function in Python is used to apply a function to every item in an iterable\u2014like a list or a tuple\u2014and returns a new map object with the results.</p>"},{"location":"Python/Python%20Builtin/map_function/#syntax","title":"Syntax:","text":"<p><code>map(function, iterable)</code></p> <ul> <li><code>function</code>: This is the function you want to apply to each item.</li> <li><code>iterable</code>: This is the list, tuple, or any other iterable whose items you want to process.</li> </ul>"},{"location":"Python/Python%20Builtin/map_function/#basic-example","title":"Basic Example:","text":"<p>Imagine we want to square every number in a list.</p>"},{"location":"Python/Python%20Builtin/map_function/#without-map","title":"Without <code>map()</code>:","text":"<pre><code>numbers = [1, 2, 3]\nsquared = []\n\nfor num in numbers:\n  square = num ** 2\n  squared.append(square)\n\nprint(squared)\n</code></pre> <pre><code>[1, 4, 9]\n</code></pre>"},{"location":"Python/Python%20Builtin/map_function/#without-map_1","title":"Without <code>map()</code>:","text":"<pre><code>squared = map(lambda num: num ** 2, numbers)\nprint(list(squared))\n</code></pre> <pre><code>[1, 4, 9]\n</code></pre> <p>The <code>map</code></p> <pre><code>a = [1, 2, 3]\nb = [4, 5, 6]\n\nsumms = []\n\nfor count in range(len(a)):\n  summs.append(a[count] + b[count])\n\nprint(summs)\n</code></pre> <pre><code>[5, 7, 9]\n</code></pre> <pre><code>summs = map(lambda x, y: x + y, a, b)\nprint(list(summs))\n</code></pre> <pre><code>[5, 7, 9]\n</code></pre> <pre><code>def add(x, y):\n  return x + y\n\nsumms = map(add, a, b)\nprint(list(summs))\n</code></pre> <pre><code>[5, 7, 9]\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Python/Python%20Builtin/reduce_function/","title":"Python <code>reduce()</code> Function Explained","text":""},{"location":"Python/Python%20Builtin/reduce_function/#syntax","title":"Syntax","text":"<pre><code>from functools import reduce\n\nreduce(function, iterable, initializer)\n</code></pre> <pre><code>\n</code></pre> <pre><code>numbers = [1, 2, 3, 4]\ntotal = 0\n\nfor num in numbers:\n  total += num\n\nprint(total)\n</code></pre> <pre><code>10\n</code></pre> <pre><code>from functools import reduce\n\nnumbers = [1, 2, 3, 4]\nresult = reduce(lambda x, y: x + y, numbers)\nprint(result)\n</code></pre> <pre><code>10\n</code></pre> <pre><code>numbers = [1, 2, 3, 4]\n\ndef add(a, b):\n  return a * b\n\nresult = reduce(add, numbers)\nprint(result)\n</code></pre> <pre><code>24\n</code></pre> <pre><code>numbers = [1, 2, 3, 4]\nresult = reduce(lambda x, y: x + y, numbers, 10)\nprint(result)\n</code></pre> <pre><code>20\n</code></pre> <pre><code>numbers = [4, 8, 1, 6, 10, 3]\n\nresult = reduce(lambda x, y: x if x &gt; y else y, numbers)\nprint(result)\n</code></pre> <pre><code>10\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Time%20Series/90_introduction/","title":"What is Time Series","text":"<p>Any data recorded with some fixed interval of time is called as time series data. This fixed interval can be hourly, daily, monthly or yearly. e.g. hourly temperature reading, daily changing fuel prices, monthly electricity bill, annul company profit report etc. In time series data, time will always be independent variable and there can be one or many dependent variable. </p> <p>Sales forecasting time series with shampoo sales for every month will look like this, </p> <p></p> <p>In above example since there is only one variable dependent on time so its called as univariate time series. If there are multiple dependent variables, then its called as multivariate time series.</p> <p>Objective of time series analysis is to understand how change in time affect the dependent variables and accordingly predict values for future time intervals.</p>"},{"location":"Time%20Series/90_introduction/#time-series-characteristics","title":"Time Series Characteristics","text":"<p>Mean, standard deviation and seasonality defines different characteristics of the time series. </p> <p></p> <p>Important characteristics of the time series are as below</p>"},{"location":"Time%20Series/90_introduction/#trend","title":"Trend","text":"<p>Trend represent the change in dependent variables with respect to time from start to end. In case of increasing trend dependent variable will increase with time and vice versa. It's not necessary to have definite trend in time series, we can have a single time series with increasing and decreasing trend. In short trend represent the varying mean of time series data.</p> <p></p>"},{"location":"Time%20Series/90_introduction/#seasonality","title":"Seasonality","text":"<p>If observations repeats after fixed time interval then they are referred as seasonal observations. These seasonal changes in data can occur because of natural events or man-made events. For example every year warm cloths sales increases just before winter season. So seasonality represent the data variations at fixed intervals.</p> <p></p>"},{"location":"Time%20Series/90_introduction/#irregularities","title":"Irregularities","text":"<p>This is also called as noise. Strange dips and jump in the data are called as irregularities. These fluctuations are caused by uncontrollable events like earthquakes, wars, flood, pandemic etc. For example because of COVID-19 pandemic there is huge demand for hand sanitizers and masks.</p> <p></p>"},{"location":"Time%20Series/90_introduction/#cyclicity","title":"Cyclicity","text":"<p>Cyclicity occurs when observations in the series repeats in random pattern. Note that if there is any fixed pattern then it becomes seasonality, in case of cyclicity observations may repeat after a week, months or may be after a year. These kinds of patterns are much harder to predict.</p> <p></p> <p>Time series data which has above characteristics is called as 'Non-Stationary Data'. For any analysis on time series data we must convert it to 'Stationary Data'</p> <p>The general guideline is to estimate the trend and seasonality in the time series, and then make the time series stationary for data modeling. In data modeling step statistical techniques are used for time series analysis and forecasting. Once we have the predictions, in the final step forecasted values converted into the original scale by applying trend and seasonality constraints back.</p> What's on your mind? Put it in the comments!"},{"location":"Time%20Series/91_analysis/","title":"Time Series Analysis","text":"<p>As name suggest its analysis of the time series data to identify the patterns in it. I will briefly explain the different techniques and test for time series data analysis.</p>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Time%20Series/91_analysis/#air-passengers-data-set","title":"Air Passengers Data Set","text":"<p>We have a monthly time series data of the air passengers from 1 Jan 1949 to 1 Dec 1960. Each row contains the air passenger number for a month of that particular year. Objective is to build a model to forecast the air passenger traffic for future months.</p>"},{"location":"Time%20Series/91_analysis/#decomposition-of-time-series","title":"Decomposition of Time Series","text":"<p>Time series decomposition helps to deconstruct the time series into several component like trend and seasonality for better visualization of its characteristics. Using time-series decomposition makes it easier to quickly identify a changing mean or variation in the data</p> <p></p>"},{"location":"Time%20Series/91_analysis/#stationary-data","title":"Stationary Data","text":"<p>For accurate analysis and forecasting trend and seasonality is removed from the time series and converted it into stationary series. Time series data is said to be stationary when statistical properties like mean, standard deviation are constant and there is no seasonality. In other words statistical properties of the time series data should not be a function of time.</p> <p></p>"},{"location":"Time%20Series/91_analysis/#test-for-stationarity","title":"Test for Stationarity","text":"<p>Easy way is to look at the plot and look for any obvious trend or seasonality. While working on real world data we can also use more sophisticated methods like rolling statistic and Augmented Dickey Fuller test to check stationarity of the data.</p>"},{"location":"Time%20Series/91_analysis/#rolling-statistics","title":"Rolling Statistics","text":"<p>In rolling statistics technique we define a size of window to calculate the mean and standard deviation throughout the series. For stationary series mean and standard deviation shouldn't change with time.</p>"},{"location":"Time%20Series/91_analysis/#augmented-dickey-fuller-adf-test","title":"Augmented Dickey Fuller (ADF) Test","text":"<p>I won't go into the details of how this test works. I will concentrate more on how to interpret the result of this test to determine the stationarity of the series. ADF test will return 'p-value' and 'Test Statistics' output values. * p-value &gt; 0.05: non-stationary. * p-value &lt;= 0.05: stationary. * Test statistics: More negative this value more likely we have stationary series. Also, this value should be smaller than critical values(1%, 5%, 10%). For e.g. If test statistic is smaller than the 5% critical values, then we can say with 95% confidence that this is a stationary series</p>"},{"location":"Time%20Series/91_analysis/#convert-non-stationary-data-to-stationary-data","title":"Convert Non-Stationary Data to Stationary Data","text":"<p>Accounting for the time series data characteristics like trend and seasonality is called as making data stationary. So by making the mean and variance of the time series constant, we will get the stationary data. Below are the few technique used for the same\u2026</p>"},{"location":"Time%20Series/91_analysis/#differencing","title":"Differencing","text":"<p>Differencing technique helps to remove the trend and seasonality from time series data. Differencing is performed by subtracting the previous observation from the current observation. The differenced data will contain one less data point than original data. So differencing actually reduces the number of observations and stabilize the mean of a time series.</p> \\[d = t - t0\\] <p>After performing the differencing it's recommended to plot the data and  visualize the change. In case there is not sufficient improvement you can perform second order or even third order differencing.</p>"},{"location":"Time%20Series/91_analysis/#transformation","title":"Transformation","text":"<p>A simple but often effective way to stabilize the variance across time is to apply a power transformation to the time series. Log, square root, cube root are most commonly used transformation techniques. Most of the time you can pick the type of growth of the time series and accordingly choose the transformation method. For. e.g. A time series that has a quadratic growth trend can be made linear by taking the square root. In case differencing don't work, you may first want to use one of above transformation technique to remove the variation from the series. </p> <p></p>"},{"location":"Time%20Series/91_analysis/#moving-average","title":"Moving Average","text":"<p>In moving averages technique, a new series is created by taking the averages of data points from original series. In this technique we can use two or more raw data points to calculate the average. This is also called as 'window width (w)'. Once window width is decided, averages are calculated from start to the end for each set of w consecutive values, hence the name moving averages. It can also be used for time series forecasting.</p> <p></p>"},{"location":"Time%20Series/91_analysis/#weighted-moving-averageswma","title":"Weighted Moving Averages(WMA)","text":"<p>WMA is a technical indicator that assigns a greater weighting to the most recent data points, and less weighting to data points in the distant past. The WMA is obtained by multiplying each number in the data set by a predetermined weight and summing up the resulting values. There can be many techniques for assigning weights. A popular one is exponentially weighted moving average where weights are assigned to all the previous values with a decay factor.</p>"},{"location":"Time%20Series/91_analysis/#centered-moving-averagescms","title":"Centered Moving Averages(CMS)","text":"<p>In a centered moving average, the value of the moving average at time t is computed by centering the window around time t and averaging across the w values within the window. For example, a center moving average with a window of 3 would be calculated as</p> <p>\\(\\(CMA(t) = mean(t-1, t, t+1)\\)\\)</p> <p>CMA is very useful for visualizing the time series data</p>"},{"location":"Time%20Series/91_analysis/#trailing-moving-averagestma","title":"Trailing Moving Averages(TMA)","text":"<p>In trailing moving average, instead of averaging over a window that is centered around a time period of interest, it simply takes the average of the last w values. For example, a trailing moving average with a window of 3 would be calculated as:</p> \\[TMA(t) = mean(t-2, t-1, t)\\] <p>TMA are useful for forecasting.</p>"},{"location":"Time%20Series/91_analysis/#correlation","title":"Correlation","text":"<ul> <li>Most important point about values in time series is its dependence on the previous values.</li> <li>We can calculate the correlation for time series observations with previous time steps, called as lags.</li> <li>Because the correlation of the time series observations is calculated with values of the same series at previous times, this is called an autocorrelation or serial correlation.</li> <li>To understand it better lets consider the example of fish prices. We will use below notation to represent the fish prices. <ul> <li>\\(P(t)\\)= Fish price of today</li> <li>\\(P(t-1)\\) = Fish price of last month</li> <li>\\(P(t-2)\\) =Fish price of last to last month</li> </ul> </li> <li>Time series of fish prices can be represented as \\(P(t-n),..... P(t-3), P(t-2),P(t-1), P(t)\\)</li> <li>So if we have fish prices for last few months then it will be easy for us to predict the fish price for today (Here we are ignoring all other external factors that may affect the fish prices)</li> </ul> <p>All the past and future data points are related in time series and ACF and PACF functions help us to determine correlation in it.</p>"},{"location":"Time%20Series/91_analysis/#auto-correlation-function-acf","title":"Auto Correlation Function (ACF)","text":"<ul> <li>ACF tells you how correlated points are with each other, based on how many time steps they are separated by.</li> <li>Now to understand it better lets consider above example of fish prices. Let's try to find the correlation between fish price for current month P(t) and two months ago P(t-2). Important thing to note that, fish price of two months ago can directly affect the today's fish price or it can indirectly affect the fish price through last months price P(t-1)</li> <li>So ACF consider the direct as well indirect effect between the points while determining the correlation</li> </ul>"},{"location":"Time%20Series/91_analysis/#partial-auto-correlation-function-pacf","title":"Partial Auto Correlation Function (PACF)","text":"<ul> <li>Unlike ACF, PACF only consider the direct effect between the points while determining the correlation</li> <li>In case of above fish price example PACF will determine the correlation between fish price for current month P(t) and two months ago P(t-2) by considering only P(t) and P(t-2) and ignoring P(t-1)</li> </ul> What's on your mind? Put it in the comments!"},{"location":"Time%20Series/92_forecasting/","title":"Time Series Forecasting","text":"<p>Forecasting refers to the future predictions based on the time series data analysis. Below are the steps performed during time series forecasting</p> <ul> <li>Step 1: Understand the time series characteristics like trend, seasonality etc</li> <li>Step 2: Do the analysis and identify the best method to make the time series stationary</li> <li>Step 3: Note down the transformation steps performed to make the time series stationary and make sure that the reverse transformation of data is possible to get the original scale back</li> <li>Step 4: Based on data analysis choose the appropriate model for time series forecasting</li> <li>Step 5: We can assess the performance of a model by applying simple metrics such as residual sum of squares(RSS). Make sure to use whole data for prediction.</li> <li>Step 6: Now we will have an array of predictions which are in transformed scale. We just need to apply the reverse transformation to get the prediction values in original scale.</li> <li>Step 7: At the end we can do the future forecasting and get the future forecasted values in original scale.</li> </ul>"},{"location":"Time%20Series/92_forecasting/#models-used-for-time-series-forecasting","title":"Models Used For Time Series Forecasting","text":"<ul> <li>Autoregression (AR)</li> <li>Moving Average (MA)</li> <li>Autoregressive Moving Average (ARMA)</li> <li>Autoregressive Integrated Moving Average (ARIMA)</li> <li>Seasonal Autoregressive Integrated Moving-Average (SARIMA)</li> <li>Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX)</li> <li>Vector Autoregression (VAR)</li> <li>Vector Autoregression Moving-Average (VARMA)</li> <li>Vector Autoregression Moving-Average with Exogenous Regressors (VARMAX)</li> <li>Simple Exponential Smoothing (SES)</li> <li>Holt Winter\u2019s Exponential Smoothing (HWES)</li> </ul> <p>Next part of this article we are going to analyze and forecast air passengers time series data using ARIMA model. Brief introduction of ARIMA model is as below</p>"},{"location":"Time%20Series/92_forecasting/#arima","title":"ARIMA","text":"<ul> <li>ARIMA stands for Auto-Regressive Integrated Moving Averages. It is actually a combination of AR and MA model. </li> <li>ARIMA has three parameters 'p' for the order of Auto-Regressive (AR) part, 'q' for the order of Moving Average (MA) part and 'd' for the order of integrated part. </li> </ul>"},{"location":"Time%20Series/92_forecasting/#auto-regressive-ar-model","title":"Auto-Regressive (AR) Model:","text":"<ul> <li>As the name indicates, its the regression of the variables against itself. In this model linear combination of the past values are used to forecast the future values. </li> <li>To figure out the order of AR model we will use PACF function</li> </ul>"},{"location":"Time%20Series/92_forecasting/#integrationi","title":"Integration(I):","text":"<ul> <li>Uses differencing of observations (subtracting an observation from observation at the previous time step) in order to make the time series stationary. Differencing involves the subtraction of the current values of a series with its previous values \\(d\\) number of times.</li> <li>Most of the time value of \\(d = 1\\), means first order of difference.</li> </ul>"},{"location":"Time%20Series/92_forecasting/#moving-average-ma-model","title":"Moving Average (MA) Model:","text":"<ul> <li>Rather than using past values of the forecast variable in a regression, a moving average model uses linear combination of past forecast errors</li> <li>To figure out the order of MA model we will use ACF function</li> </ul> What's on your mind? Put it in the comments!"},{"location":"Time%20Series/93_python_example/","title":"TSA Python Example","text":"<p>We have a monthly time series data of the air passengers from 1 Jan 1949 to 1 Dec 1960. Each row contains the air passenger number for a month of that particular year. Objective is to build a model to forecast the air passenger traffic for future months.</p>  Don't Miss Any Updates! <p> Before we continue, I have a humble request, to be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel. </p>"},{"location":"Time%20Series/93_python_example/#import-the-library","title":"Import The Library","text":"<ul> <li>dataidea: allows us to quickly load most common packages and datasets for the course</li> <li>statsmodels: Using statsmodels module classes and functions for time series analysis and forecasting </li> <li>adfuller: Augmented Dickey-Fuller</li> <li>ACF: Auto Correlation Function</li> <li>PACF: Partial Auto Correlation Function</li> <li>ARIMA: Autoregressive Integrated Moving Average ARIMA(p,d,q) Model</li> <li>sm.tsa.seasonal.seasonal_decompose: For decomposition of time series</li> <li>rcParams: To change the matplotlib properties like figure size</li> </ul> <pre><code>from dataidea.packages import np, pd, plt, sns, sm\nfrom dataidea.datasets import loadDataset # allows us to load datasets\nfrom statsmodels.tsa.stattools import adfuller,acf, pacf\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom pylab import rcParams\n</code></pre> <pre><code># Set plot size \nrcParams['figure.figsize'] = 10, 6\n</code></pre> <pre><code># load the dataset\ndata = loadDataset('air_passengers')\n\n# Print the first 5 rows\ndata.head()\n</code></pre> Month Passengers 0 1949-01 112 1 1949-02 118 2 1949-03 132 3 1949-04 129 4 1949-05 121"},{"location":"Time%20Series/93_python_example/#understanding-the-data","title":"Understanding The Data","text":"<ul> <li>Dataframe <code>data</code> contains the time series data. There are two columns <code>Month</code> and <code>Passengers</code>. <code>Month</code> column contains the value of month in that year and passenger column contains the number of air passengers for that particular month.</li> <li>As you may have noticed <code>Month</code> column datatype is <code>Object</code>, so we are going to convert it to <code>datetime</code></li> <li>To make plotting easier, we set the index of pandas dataframe <code>data</code> to the <code>Month</code> column so that it will act as x-axis &amp; Passenger column as y-axis</li> </ul> <pre><code># convert month to datetime\ndata['Month'] = pd.to_datetime(data.Month)\n\n# set month as index\ndata = data.set_index(data.Month)\ndata.drop('Month', axis = 1, inplace = True)\n\n# look at the first 5 rows\ndata.head()\n</code></pre> Passengers Month 1949-01-01 112 1949-02-01 118 1949-03-01 132 1949-04-01 129 1949-05-01 121"},{"location":"Time%20Series/93_python_example/#time-series-characteristics","title":"Time Series Characteristics","text":""},{"location":"Time%20Series/93_python_example/#trend","title":"Trend","text":"<pre><code># plt.figure(figsize= (10,6))\nplt.plot(data)\nplt.xlabel('Years')\nplt.ylabel('No of Air Passengers')\nplt.title('Trend of the Time Series')\nplt.show()\n</code></pre> <p>As you can see from above plot there is upward trend of number of passenger for every year. </p>"},{"location":"Time%20Series/93_python_example/#variance","title":"Variance","text":"<p>In above graph you can clearly see that the variation is also increasing with the level of the series. You will see in the later part of this exercise how we handle the variance to increase the stationarity of the series.</p>"},{"location":"Time%20Series/93_python_example/#seasonality","title":"Seasonality","text":"<p>We can also see the graph going up and down at regular interval, that is the sign of seasonality. Let's plot the graph for few months to visualize for seasonality.</p> <pre><code># To plot the seasonality we are going to create a temp dataframe \n# and add columns for Month and Year values\ndata_temp = data.copy()\ndata_temp['Year'] = pd.DatetimeIndex(data_temp.index).year\ndata_temp['Month'] = pd.DatetimeIndex(data_temp.index).month\n\n# Stacked line plot\nplt.figure(figsize=(10,10))\nplt.title('Seasonality of the Time Series')\nsns.pointplot(x='Month',y='Passengers',hue='Year',data=data_temp)\n</code></pre> <pre><code>&lt;Axes: title={'center': 'Seasonality of the Time Series'}, xlabel='Month', ylabel='Passengers'&gt;\n</code></pre> <p></p> <p>From above graph we can say that every year in month of July we observe maximum number of passengers and similarly minimum number of passenger in the month of November.</p>"},{"location":"Time%20Series/93_python_example/#decomposition-of-time-series","title":"Decomposition of Time Series","text":"<p>Let's now use the decomposition technique to deconstruct the time series data into several component like trend and seasonality for visualization of time series characteristics.</p> <p>Here we are going to use 'additive' model because it is quick to develop, fast to train, and provide interpretable patterns.</p> <pre><code>from statsmodels.tsa.seasonal import seasonal_decompose\n\ndecomposition = seasonal_decompose(\n    data, \n    model='additive')\n\nfig = decomposition.plot()\n</code></pre> <p></p> <p>[ad]</p> <p></p>"},{"location":"Time%20Series/93_python_example/#time-series-analysis","title":"Time Series Analysis","text":"<p>So our time series has variance, trend and seasonality characteristics. During our analysis we are going to try multiple techniques to make time series stationary and record the stationarity scores for each method. Finally, we will select the method, which is easy for inverse transformation easy and give best stationarity score.</p>"},{"location":"Time%20Series/93_python_example/#check-for-stationarity","title":"Check for Stationarity","text":"<p>We are going to use rolling statistics and Dickey-Fuller test to check the stationarity of the time series</p> <pre><code>def stationarity_test(timeseries):\n    # Get rolling statistics for window = 12 i.e. yearly statistics\n    rolling_mean = timeseries.rolling(window=12).mean()\n    rolling_std = timeseries.rolling(window=12).std()\n\n    # Plot rolling statistic\n    plt.figure(figsize= (10,6))\n    plt.xlabel('Years')\n    plt.ylabel('No of Air Passengers')    \n    plt.title('Stationary Test: Rolling Mean and Standard Deviation')\n    plt.plot(timeseries, color= 'blue', label= 'Original')\n    plt.plot(rolling_mean, color= 'green', label= 'Rolling Mean')\n    plt.plot(rolling_std, color= 'red', label= 'Rolling Std')   \n    plt.legend()\n    plt.show()\n\n    # Dickey-Fuller test\n    print('Results of Dickey-Fuller Test')\n    df_test = adfuller(timeseries)\n    df_output = pd.Series(df_test[0:4], index = ['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n    for key, value in df_test[4].items():\n        df_output['Critical Value (%s)' %key] = value\n    print(df_output)\n</code></pre> <pre><code># Lets test the stationarity score with original series data\nstationarity_test(data)\n</code></pre> <p></p> <pre><code>Results of Dickey-Fuller Test\nTest Statistic                   0.815369\np-value                          0.991880\n#Lags Used                      13.000000\nNumber of Observations Used    130.000000\nCritical Value (1%)             -3.481682\nCritical Value (5%)             -2.884042\nCritical Value (10%)            -2.578770\ndtype: float64\n</code></pre> <p>Though it's clear from visual observation that it's not a stationary series, but still lets have look at the rolling statistics and Duckey Fuller test results</p> <ul> <li>Rolling statistics: Standard deviation has very less variation but mean is increasing continuously.</li> <li>Duckey Fuller Test: Test statistic is way more than the critical values.</li> </ul>"},{"location":"Time%20Series/93_python_example/#convert-non-stationary-data-to-stationary-data","title":"Convert Non-Stationary Data to Stationary Data","text":"<p>Let's first use the differencing technique to obtain the stationarity.</p>"},{"location":"Time%20Series/93_python_example/#differencing","title":"Differencing","text":"<p>To transform the series using Differencing we will use the <code>diff()</code> method of pandas. A benefit of using the Pandas function, in addition to requiring less code, is that it maintains the date-time information for the differenced series</p> <p>\\(\\(Y_t' = Y_t - Y_{t-1}\\)\\)</p> <pre><code>data_diff = data.diff(periods = 1) # First order differencing\nplt.xlabel('Years')\nplt.ylabel('No of Air Passengers')    \nplt.title('Convert Non Stationary Data to Stationary Data using Differencing ')\nplt.plot(data_diff)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D&gt;]\n</code></pre> <p></p> <p>So from above graph its clear that differencing technique removed the trend from the time series, but variance is still there Now lets run the <code>stationarity_test()</code> to check the effectiveness of the 'Differencing' technique</p> <pre><code>data_diff.dropna(inplace = True)# Data transformation may add na values\nstationarity_test(data_diff)\n</code></pre> <p></p> <pre><code>Results of Dickey-Fuller Test\nTest Statistic                  -2.829267\np-value                          0.054213\n#Lags Used                      12.000000\nNumber of Observations Used    130.000000\nCritical Value (1%)             -3.481682\nCritical Value (5%)             -2.884042\nCritical Value (10%)            -2.578770\ndtype: float64\n</code></pre> <p>The rolling values appear to be varying slightly, and we can see there is slight upward trend in standard deviation. Also, the test statistic is smaller than the 10% critical but since p-value is greater than 0.05 it is not a stationary series.</p> <p>Note that variance in the series is also affecting above results, which can be removed using transformation technique.</p> <p>Let's also check with transformation technique</p>"},{"location":"Time%20Series/93_python_example/#transformation","title":"Transformation","text":"<p>Since variance is proportional to the levels, we are going to use the log transformation.</p> <pre><code># apply the log transformation\ndata_log = np.log(data)\n\n# let's plot the results\nplt.subplot(211)\nplt.plot(data, label= 'Time Series with Variance')\nplt.legend()\nplt.subplot(212)\nplt.plot(data_log, label='Time Series without Variance (Log Transformation)')\nplt.legend()  \nplt.show()\n</code></pre> <p></p> <p>Since log transformation has removed the variance from series, lets use this transformed data hence forward.  Note that, Since we are using log transformation, we can use the exponential of the series to get the original scale back. <pre><code>    df = exp(df_log)\n</code></pre></p> <p>Let cross-check the differencing method scores with this log transformed data again.</p> <pre><code># First order differencing\ndata_log_diff = data_log.diff(periods = 1) \n\n# Data transformation may add na values\ndata_log_diff.dropna(inplace = True)\nstationarity_test(data_log_diff)\n</code></pre> <p></p> <pre><code>Results of Dickey-Fuller Test\nTest Statistic                  -2.717131\np-value                          0.071121\n#Lags Used                      14.000000\nNumber of Observations Used    128.000000\nCritical Value (1%)             -3.482501\nCritical Value (5%)             -2.884398\nCritical Value (10%)            -2.578960\ndtype: float64\n</code></pre> <p>The rolling mean and standard deviation values are okay now. The test statistic is smaller than the 10% critical values but since p-value is greater than 0.05 it is not a stationary series.</p> <p>Let's also check with Moving Average technique\u2026</p>"},{"location":"Time%20Series/93_python_example/#moving-average","title":"Moving Average","text":"<p>Since we have time series data from 1 Jan 1949 to 1 Dec 1960, we will define a yearly window for moving average. Window size = 12. Note that we are going to use Log transformed data.</p> <pre><code>data_log_moving_avg = data_log.rolling(window = 12).mean()\n\n# let's plot the results\nplt.xlabel('Years')\nplt.ylabel('No of Air Passengers')    \nplt.title('Convert Non Stationary Data to Stationary Data using Moving Average')\nplt.plot(data_log, color= 'blue', label='Orignal')\nplt.plot(data_log_moving_avg, color= 'red', label='Moving Average')\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>As you can see from above graph that data is more smooth without any variance. If we use the differencing technique with log transformed data and mean average data then we should get better stationarity scores</p> <pre><code>data_log_moving_avg_diff = data_log - data_log_moving_avg\ndata_log_moving_avg_diff.dropna(inplace = True)\nstationarity_test(data_log_moving_avg_diff)\n</code></pre> <p></p> <pre><code>Results of Dickey-Fuller Test\nTest Statistic                  -3.162908\np-value                          0.022235\n#Lags Used                      13.000000\nNumber of Observations Used    119.000000\nCritical Value (1%)             -3.486535\nCritical Value (5%)             -2.886151\nCritical Value (10%)            -2.579896\ndtype: float64\n</code></pre> <p>As expected now we are able to see some real improvements. p-value is less than 0.05 that means our series is stationary, but we can only say this with 95% of confidence, as test statistics is less than 5% critical value.</p> <p>In order to increase the stationarity of the series lets try to use 'Weighted Moving Average' technique</p>"},{"location":"Time%20Series/93_python_example/#weighted-moving-average-wma","title":"Weighted Moving Average (WMA)","text":"<p>Here we are going to use exponentially weighted moving average with parameter \u2018halflife = 12\u2019. This parameter defines the amount of exponential decay. This is just an assumption here and would depend largely on the business domain.</p> <pre><code>data_log_weighted_avg = data_log.ewm(halflife = 12).mean()\nplt.plot(data_log)\nplt.plot(data_log_weighted_avg, color = 'red')\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D&gt;]\n</code></pre> <p></p> <p>Notice that WMA follow's no of passenger values more closely than a corresponding Simple Moving Average which also results in more accurate trend direction. Now lets check, the effect of this on stationarity scores!</p> <pre><code>data_log_weighted_avg_diff = data_log - data_log_weighted_avg\nstationarity_test(data_log_weighted_avg_diff)\n</code></pre> <p></p> <pre><code>Results of Dickey-Fuller Test\nTest Statistic                  -3.601262\np-value                          0.005737\n#Lags Used                      13.000000\nNumber of Observations Used    130.000000\nCritical Value (1%)             -3.481682\nCritical Value (5%)             -2.884042\nCritical Value (10%)            -2.578770\ndtype: float64\n</code></pre> <p>Test statistic is smaller than the 1% critical value, which is better than the previous case. Note that in this case there will be no missing values as all values from starting are given weights. So it\u2019ll work even with no previous values.</p> <p>There is one more way to obtain better stationarity is by using the residual data from time series decomposition.</p>"},{"location":"Time%20Series/93_python_example/#decomposition-of-time-series_1","title":"Decomposition of Time Series","text":"<p>Let's now use the decomposition technique to deconstruct the log transformed time series data, so that we can check the stationarity using residual data.</p> <pre><code>import statsmodels.api as smapi\n\ndecomposition = smapi.tsa.seasonal_decompose(data_log,period =12)\nfig = decomposition.plot()\n</code></pre> <p></p> <p>Here we can see that the trend and seasonality are separated out from log transformed data, and we can now check the stationarity of the residuals</p> <pre><code>data_log_residual = decomposition.resid\ndata_log_residual.dropna(inplace = True)\nstationarity_test(data_log_residual)\n</code></pre> <p></p> <pre><code>Results of Dickey-Fuller Test\nTest Statistic                -6.332387e+00\np-value                        2.885059e-08\n#Lags Used                     9.000000e+00\nNumber of Observations Used    1.220000e+02\nCritical Value (1%)           -3.485122e+00\nCritical Value (5%)           -2.885538e+00\nCritical Value (10%)          -2.579569e+00\ndtype: float64\n</code></pre> <p>The Dickey-Fuller test statistic is significantly lower than the 1% critical value and p-value is almost 0. So this time series is very close to stationary. This concludes our time series analysis and data transformation to get the stationary series. Now we can start modeling it for forecast.</p>"},{"location":"Time%20Series/93_python_example/#forecasting","title":"Forecasting","text":"<ul> <li>Though using residual values gives us very good results, but it's relatively difficult to add noise and seasonality back into predicted residuals in this case. </li> <li>So we are going to make model on the time series(<code>df_log_diff</code>), where we have used log transformation and differencing technique. This is one of the most popular and beginner-friendly technique. As per our time series analysis <code>df_log_diff</code> is not a perfectly stationary series, that's why we are going to use statistical models like ARIMA to forecast the data.</li> <li>Remember that ARIMA model uses three parameters, <code>p</code> for the order of Auto-Regressive (AR) part, <code>q</code> for the order of Moving Average (MA) part and <code>d</code> for the order of integrated part. We are going to use <code>d=1</code> but to find the value for <code>p</code> and <code>q</code> lets plot ACF and PACF.</li> <li>Note that since we are using <code>d=1</code>, first order of differencing will be performed on given series. Since first value of time series don't have any value to subtract from resulting series will have one less value from original series</li> </ul>"},{"location":"Time%20Series/93_python_example/#acf-and-pacf-plots","title":"ACF and PACF Plots","text":"<ul> <li>To figure out the order of AR model(p) we will use PACF function. <code>p</code> = the lag value where the PACF chart crosses the upper confidence interval for the first time</li> <li>To figure out the order of MA model(<code>q</code>) we will use ACF function. <code>q</code> = the lag value where the ACF chart crosses the upper confidence interval for the first time</li> </ul> <pre><code>lag_acf = acf(data_log_diff, nlags=20)\nlag_pacf = pacf(data_log_diff, nlags=20, method='ols')\n\n# Plot ACF: \nplt.subplot(121) \nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\n# Draw 95% confidence interval line\nplt.axhline(y=-1.96/np.sqrt(len(data_log_diff)),linestyle='--',color='red')\nplt.axhline(y=1.96/np.sqrt(len(data_log_diff)),linestyle='--',color='red')\nplt.xlabel('Lags')\nplt.title('Autocorrelation Function')\n\n#Plot PACF:\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\n# Draw 95% confidence interval line\nplt.axhline(y=-1.96/np.sqrt(len(data_log_diff)),linestyle='--',color='red')\nplt.axhline(y=1.96/np.sqrt(len(data_log_diff)),linestyle='--',color='red')\nplt.xlabel('Lags')\nplt.title('Partial Autocorrelation Function')\nplt.tight_layout()\n</code></pre> <p>From above graph its clear that p=2 and q=2. Now we have the ARIMA parameters values, lets make 3 different ARIMA models considering individual as well as combined effects. We will also print the RSS(Residual Sum of Square) metric for each. Please note that here RSS is for the values of residuals and not actual series.</p>"},{"location":"Time%20Series/93_python_example/#ar-model","title":"AR Model","text":"<p>Since <code>q</code> is MA model parameter we will keep its value as <code>0</code>.</p> <pre><code># # freq = 'MS' &gt; The frequency of the time-series MS = calendar month begin\n# # The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters to use\n# model = ARIMA(data_log, order=(2, 1, 0), freq = 'MS')  \n# results_AR = model.fit()\n# plt.plot(data_log_diff)\n# plt.plot(results_AR.fittedvalues, color='red')\n# plt.title('AR Model, RSS: %.4f'% sum((results_AR.fittedvalues - data_log_diff['Passengers'])**2))\n# plt.show()\n</code></pre>"},{"location":"Time%20Series/93_python_example/#ma-model","title":"MA Model","text":"<p>Since 'p' is AR model parameter we will keep its value as '0'.</p> <pre><code># model = ARIMA(data_log, order=(0, 1, 2), freq = 'MS')  \n# results_MA = model.fit()  \n# plt.plot(data_log_diff)\n# plt.plot(results_MA.fittedvalues, color='red')\n# plt.title('MA Model, RSS: %.4f'% sum((results_MA.fittedvalues-data_log_diff['Passengers'])**2))\n</code></pre>"},{"location":"Time%20Series/93_python_example/#combined-model","title":"Combined Model","text":"<pre><code># model = ARIMA(data_log, order=(2, 1, 2), freq = 'MS')  \n# results_ARIMA = model.fit(disp=-1)  \n# plt.plot(data_log_diff)\n# plt.plot(results_ARIMA.fittedvalues, color='red')\n# plt.title('Combined Model, RSS: %.4f'% sum((results_ARIMA.fittedvalues-data_log_diff['Passengers'])**2))\n</code></pre> <p>Here we can see that the AR and MA models have almost the same RSS score but combined is significantly better. So we will go ahead with combined ARIMA model and use it for predictions.</p>"},{"location":"Time%20Series/93_python_example/#prediction-and-reverse-transformation","title":"Prediction and Reverse Transformation","text":"<ul> <li>We will create a separate series of predicted values using ARIMA model</li> <li>Reverse transform the predicted values to get the original scale back</li> <li>Compare the predicted values with original values and plot them</li> </ul> <pre><code># # Create a separate series of predicted values\n# predictions_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\n\n# print('Total no of predictions: ', len(predictions_diff))\n# predictions_diff.head()\n</code></pre> <p>Since we are using first order of differencing(d =1), there is no prediction available for first value (1949-02-01) of original series. In order to remove 'differencing transformation' from the prediction values we are going to add these differences consecutively to the base number. An easy way to do it is to first determine the cumulative sum at index and then add it to the base number. We are going to use pandas cumsum() function for it.</p> <pre><code># predictions_diff_cumsum = predictions_diff.cumsum()\n# predictions_diff_cumsum.head()\n</code></pre> <p>Above values once added to the base number will completely remove the differencing transformation. For this, lets create a series with all values as base number and add the 'predictions_diff_cumsum' to it.</p> <pre><code># predictions_log = pd.Series(df_log['Passengers'].iloc[0], index=df_log.index) # Series of base number\n# predictions_log = predictions_log.add(predictions_diff_cumsum,fill_value=0)\n# predictions_log.head()\n</code></pre> <p>So as of now we have removed the differencing transformation, now lets remove the log transformation to get the original scale back.</p> <pre><code># predictions = np.exp(predictions_log)\n# plt.plot(df)\n# plt.plot(predictions)\n</code></pre> <pre><code># df_predictions =pd.DataFrame(predictions, columns=['Predicted Values'])\n# pd.concat([df,df_predictions],axis =1).T\n</code></pre>"},{"location":"Time%20Series/93_python_example/#future-forecasting","title":"Future Forecasting","text":"<ul> <li>We have data from 1 Jan 1949 to 1 Dec 1960. 12 years of data with passenger number observation for each month i.e. 144 total observations.</li> <li>If we want to forecast for next 5 years or 60 months then, \u2018end\u2019 count will be &gt;  144 + 60 = 204.</li> <li>We are going to use statsmodels plot_predict() method for it</li> </ul> <pre><code># results_ARIMA.plot_predict(start = 1, end= 204) \n</code></pre> <pre><code># # Forecasted values in original scale will be\n# forecast_values_log_scale = results_ARIMA.forecast(steps = 60)\n# forecast_values_original_scale = np.exp(forecast_values_log_scale[0])\n\n# forecast_date_range= pd.date_range(\"1961-01-01\", \"1965-12-01\", freq=\"MS\")\n\n# df_forecast =pd.DataFrame(forecast_values_original_scale, columns=['Forecast'])\n# df_forecast['Month'] = forecast_date_range\n\n# df_forecast[['Month', 'Forecast']]\n</code></pre> <p> To be among the first to hear about future updates, simply enter your email below, follow us on   (formally Twitter), or subscribe to our  YouTube channel. </p> <p></p> What's on your mind? Put it in the comments!"}]}